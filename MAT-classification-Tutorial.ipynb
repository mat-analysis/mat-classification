{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MAT-classification: Analysis and Classification methods for Multiple Aspect Trajectory Data Mining \\[MAT-Tools Framework\\]\n",
    "\n",
    "Sample Code in python notebook to use mat-classification as a python library.\n",
    "\n",
    "The present package offers a tool, to support the user in the task of data analysis of multiple aspect trajectories. It integrates into a unique framework for multiple aspects trajectories and in general for multidimensional sequence data mining methods.\n",
    "\n",
    "Created on Dec, 2023\n",
    "Copyright (C) 2023, License GPL Version 3 or superior (see LICENSE file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mat-classification in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (0.1b3)\n",
      "Requirement already satisfied: glob2 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-classification) (0.7)\n",
      "Requirement already satisfied: numpy in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-classification) (1.23.5)\n",
      "Requirement already satisfied: pandas in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-classification) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-classification) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-classification) (2.8.2)\n",
      "Requirement already satisfied: mat-data in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-classification) (0.1rc6)\n",
      "Requirement already satisfied: mat-model in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-data->mat-classification) (0.1b7)\n",
      "Requirement already satisfied: py7zr in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-data->mat-classification) (0.21.0)\n",
      "Requirement already satisfied: geohash in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-data->mat-classification) (1.0)\n",
      "Requirement already satisfied: pyarrow in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-data->mat-classification) (16.1.0)\n",
      "Requirement already satisfied: fastparquet in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from mat-data->mat-classification) (2024.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from pandas->mat-classification) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from pandas->mat-classification) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from python-dateutil->mat-classification) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from scikit-learn->mat-classification) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from scikit-learn->mat-classification) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from scikit-learn->mat-classification) (3.1.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from fastparquet->mat-data->mat-classification) (2.8.3)\n",
      "Requirement already satisfied: fsspec in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from fastparquet->mat-data->mat-classification) (2022.5.0)\n",
      "Requirement already satisfied: packaging in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from fastparquet->mat-data->mat-classification) (21.3)\n",
      "Requirement already satisfied: docutils>=0.3 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from geohash->mat-data->mat-classification) (0.18.1)\n",
      "Requirement already satisfied: texttable in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from py7zr->mat-data->mat-classification) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from py7zr->mat-data->mat-classification) (3.20.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from py7zr->mat-data->mat-classification) (0.15.10)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from py7zr->mat-data->mat-classification) (1.1.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from py7zr->mat-data->mat-classification) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from py7zr->mat-data->mat-classification) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from py7zr->mat-data->mat-classification) (1.0.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from py7zr->mat-data->mat-classification) (1.1.0)\n",
      "Requirement already satisfied: psutil in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from py7zr->mat-data->mat-classification) (5.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/tarlisportela/miniforge3/lib/python3.9/site-packages (from packaging->fastparquet->mat-data->mat-classification) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install mat-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Loading data\n",
    "To use helpers for data pre-processing, import from package `matdata` (dependency: [mat-data](https://github.com/ttportela/mat-data)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1. Loading a sample data\n",
    "    a) Lets start by loading FoursquareNYC data:\n",
    "(For other preprocessing functions, check the docs: https://mat-analysis.github.io/mat-tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset file: https://github.com/mat-analysis/datasets/tree/main/mat/FoursquareNYC/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1055k  100 1055k    0     0   487k      0  0:00:02  0:00:02 --:--:--  488k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>space</th>\n",
       "      <th>time</th>\n",
       "      <th>day</th>\n",
       "      <th>poi</th>\n",
       "      <th>type</th>\n",
       "      <th>root_type</th>\n",
       "      <th>rating</th>\n",
       "      <th>weather</th>\n",
       "      <th>tid</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.8331652006224 -73.9418603427692</td>\n",
       "      <td>317</td>\n",
       "      <td>Monday</td>\n",
       "      <td>The Lair Of Modern Strange Cowboy</td>\n",
       "      <td>Home (private)</td>\n",
       "      <td>Residence</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>126</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.8340978041072 -73.9452672225881</td>\n",
       "      <td>1404</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Galaxy Gourmet Deli</td>\n",
       "      <td>Deli / Bodega</td>\n",
       "      <td>Food</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>126</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.8331652006224 -73.9418603427692</td>\n",
       "      <td>0</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>The Lair Of Modern Strange Cowboy</td>\n",
       "      <td>Home (private)</td>\n",
       "      <td>Residence</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>126</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.7646959283254 -73.8851974964414</td>\n",
       "      <td>1069</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Popeyes Louisiana Kitchen</td>\n",
       "      <td>Fried Chicken Joint</td>\n",
       "      <td>Food</td>\n",
       "      <td>6.6</td>\n",
       "      <td>Clear</td>\n",
       "      <td>126</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.7660790376824 -73.8835287094116</td>\n",
       "      <td>1120</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>MTA Bus Operations Depot - LaGuardia</td>\n",
       "      <td>Bus Station</td>\n",
       "      <td>Travel &amp; Transport</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>126</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66941</th>\n",
       "      <td>40.7047332789043 -73.9877378940582</td>\n",
       "      <td>1037</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Miami Ad School Brooklyn</td>\n",
       "      <td>General College &amp; University</td>\n",
       "      <td>College &amp; University</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>29563</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66942</th>\n",
       "      <td>40.6951627360199 -73.9954478691072</td>\n",
       "      <td>1210</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Lantern Thai Kitchen</td>\n",
       "      <td>Thai Restaurant</td>\n",
       "      <td>Food</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>29563</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66943</th>\n",
       "      <td>40.6978026652822 -73.9941451630314</td>\n",
       "      <td>481</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Eastern Athletic Club</td>\n",
       "      <td>Gym</td>\n",
       "      <td>Outdoors &amp; Recreation</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>29563</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66944</th>\n",
       "      <td>40.6946728967503 -73.9940820360805</td>\n",
       "      <td>819</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Coffee Shop</td>\n",
       "      <td>Food</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>29563</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66945</th>\n",
       "      <td>40.6978026652822 -73.9941451630314</td>\n",
       "      <td>476</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Eastern Athletic Club</td>\n",
       "      <td>Gym</td>\n",
       "      <td>Outdoors &amp; Recreation</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>29563</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66946 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    space  time       day  \\\n",
       "0      40.8331652006224 -73.9418603427692   317    Monday   \n",
       "1      40.8340978041072 -73.9452672225881  1404    Monday   \n",
       "2      40.8331652006224 -73.9418603427692     0   Tuesday   \n",
       "3      40.7646959283254 -73.8851974964414  1069  Thursday   \n",
       "4      40.7660790376824 -73.8835287094116  1120  Thursday   \n",
       "...                                   ...   ...       ...   \n",
       "66941  40.7047332789043 -73.9877378940582  1037    Friday   \n",
       "66942  40.6951627360199 -73.9954478691072  1210    Friday   \n",
       "66943  40.6978026652822 -73.9941451630314   481  Saturday   \n",
       "66944  40.6946728967503 -73.9940820360805   819  Saturday   \n",
       "66945  40.6978026652822 -73.9941451630314   476    Sunday   \n",
       "\n",
       "                                        poi                          type  \\\n",
       "0         The Lair Of Modern Strange Cowboy                Home (private)   \n",
       "1                       Galaxy Gourmet Deli                 Deli / Bodega   \n",
       "2         The Lair Of Modern Strange Cowboy                Home (private)   \n",
       "3                 Popeyes Louisiana Kitchen           Fried Chicken Joint   \n",
       "4      MTA Bus Operations Depot - LaGuardia                   Bus Station   \n",
       "...                                     ...                           ...   \n",
       "66941              Miami Ad School Brooklyn  General College & University   \n",
       "66942                  Lantern Thai Kitchen               Thai Restaurant   \n",
       "66943                 Eastern Athletic Club                           Gym   \n",
       "66944                             Starbucks                   Coffee Shop   \n",
       "66945                 Eastern Athletic Club                           Gym   \n",
       "\n",
       "                   root_type  rating weather    tid  label  \n",
       "0                  Residence    -1.0   Clear    126      6  \n",
       "1                       Food     8.2  Clouds    126      6  \n",
       "2                  Residence    -1.0  Clouds    126      6  \n",
       "3                       Food     6.6   Clear    126      6  \n",
       "4         Travel & Transport    -1.0   Clear    126      6  \n",
       "...                      ...     ...     ...    ...    ...  \n",
       "66941   College & University    -1.0  Clouds  29563   1070  \n",
       "66942                   Food     8.0  Clouds  29563   1070  \n",
       "66943  Outdoors & Recreation     6.9  Clouds  29563   1070  \n",
       "66944                   Food     7.0  Clouds  29563   1070  \n",
       "66945  Outdoors & Recreation     6.9  Clouds  29563   1070  \n",
       "\n",
       "[66946 rows x 10 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matdata.dataset import load_ds\n",
    "\n",
    "dataset='mat.FoursquareNYC'\n",
    "\n",
    "data = load_ds(dataset, missing='-999')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4171e8bb481c4e97adbae98a2a8838b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Spliting Data (class-balanced):   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19cf6994a7c4750aab4782e91b9bd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sorting data:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b1d2bc358546f0836562d51655e54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sorting data:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([ 84, 164, 181, 390, 553, 646, 662, 702, 768, 901]), '--', 112, 56)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matdata.preprocess import klabels_stratify\n",
    "train, test = klabels_stratify(data, kl=10) #, random_num=42)\n",
    "\n",
    "train.label.unique(), '--', len(train.tid.unique()), len(test.tid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.2. Saving and Conversions\n",
    "    b) Saving trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train.to_parquet(f'sample/data/{dataset.split(\".\")[1]}/train.parquet', index=False)\n",
    "test.to_parquet(f'sample/data/{dataset.split(\".\")[1]}/test.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can save using the converter functions (which provide more options), in other formats for other input types of methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CSV: sample/data/FoursquareNYC/train.csv\n",
      "Done.\n",
      " --------------------------------------------------------------------------------\n",
      "Writing CSV: sample/data/FoursquareNYC/test.csv\n",
      "Done.\n",
      " --------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0963c4764cf461da5e3672cc7dd6d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing ZIP:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7439bdcf4e6743c3af21f019d20c6448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing ZIP:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Parquet: sample/data/FoursquareNYC/train.parquet\n",
      "Done.\n",
      " --------------------------------------------------------------------------------\n",
      "Writing Parquet: sample/data/FoursquareNYC/test.parquet\n",
      "Done.\n",
      " --------------------------------------------------------------------------------\n",
      "Help on function df2parquet in module matdata.converter:\n",
      "\n",
      "df2parquet(df, data_path, file='train', tid_col='tid', class_col='label', select_cols=None, opLabel='Writing Parquet')\n",
      "    Writes a pandas DataFrame to a Parquet file.\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    df : pandas.DataFrame\n",
      "        The DataFrame to be written to the Parquet file.\n",
      "    data_path : str\n",
      "        The directory path where the Parquet file will be saved.\n",
      "    file : str, optional (default='train')\n",
      "        The base name of the Parquet file (without extension).\n",
      "    tid_col : str, optional (default='tid')\n",
      "        The name of the column to be used as the trajectory identifier.\n",
      "    class_col : str, optional (default='label')\n",
      "        The name of the column to be treated as the class/label column.\n",
      "    select_cols : list of str, optional\n",
      "        A list of column names to be included in the Parquet file. If None, all columns are included.\n",
      "    opLabel : str, optional (default='Writing PARQUET')\n",
      "        A label describing the operation, useful for logging or display purposes.\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    pandas.DataFrame\n",
      "        The input DataFrame\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from matdata.converter import df2csv, df2zip, df2parquet\n",
    "\n",
    "# Saving as csv:\n",
    "df2csv(train, f'sample/data/{dataset.split(\".\")[1]}', 'train')\n",
    "df2csv(test, f'sample/data/{dataset.split(\".\")[1]}', 'test')\n",
    "\n",
    "# Saving as zip (containing trajectory type of files):\n",
    "df2zip(train, f'sample/data/{dataset.split(\".\")[1]}', 'train')\n",
    "df2zip(test, f'sample/data/{dataset.split(\".\")[1]}', 'test')\n",
    "\n",
    "# Saving as parquet (override):\n",
    "df2parquet(train, f'sample/data/{dataset.split(\".\")[1]}', 'train')\n",
    "df2parquet(test, f'sample/data/{dataset.split(\".\")[1]}', 'test')\n",
    "\n",
    "# Check docs:\n",
    "help(df2parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Classification Methods\n",
    "\n",
    "All classification methods follow similar classifier model structure, you can use in two ways:\n",
    "\n",
    "Option 1 (the class default data preparation):\n",
    "- `prepare_input(train, test)` => `model.train()` => `model.test()`\n",
    "\n",
    "Option 2 (your own data preparation):\n",
    "- `model.fit(X_train, y_train, X_val, y_val)` => `model.predict(X_test, y_test)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.1. Trajectory Based Methods\n",
    "\n",
    "Methods based on inputing trajectory data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.1. MARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-30 18:59:54.691689\n",
      "\n",
      "[MARC:] Building model\n",
      "\n",
      "###########    DATA PREPARATION      ###########\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Attribute 'space': 189 unique values\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Attribute 'time': 580 unique values\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Attribute 'day': 7 unique values\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Attribute 'poi': 172 unique values\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Attribute 'type': 97 unique values\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Attribute 'root_type': 10 unique values\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Attribute 'rating': 44 unique values\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Attribute 'weather': 6 unique values\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Total of attribute/value pairs: 1105\n",
      "\u001b[[[    INFO    ] 2024-06-30 18:59:54 :: Processing trajectory 46/46. \n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Loading data from files ... DONE!\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Trajectories:  46\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Labels:        2\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Train size:    0.6739130434782609\n",
      "[    INFO    ] 2024-06-30 18:59:54 :: Test size:     0.32608695652173914\n",
      "[MARC:] Training hiperparameter model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976df75787ea460b94720e1319678d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[MARC:] Model Training:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 18:59:55.228426: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 191ms/steps: 28.4335 - acc: 0.41\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.870833\tprec_macro: 0.888889\trec_macro: 0.882353\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.660633\tprec_macro: 0.700000\trec_macro: 0.678571\n",
      "1/1 [==============================] - 2s 2s/step - loss: 28.4335 - acc: 0.4194 - val_loss: 27.8171 - val_acc: 0.6667\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 27.8030 - acc: 0.67\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.732143\tprec_macro: 0.732143\trec_macro: 0.732143\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 27.8030 - acc: 0.6774 - val_loss: 27.2089 - val_acc: 0.7333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.1870 - acc: 0.80\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.722222\tprec_macro: 0.750000\trec_macro: 0.723214\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 27.1870 - acc: 0.8065 - val_loss: 26.6108 - val_acc: 0.7333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 26.5841 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 26.5841 - acc: 0.9677 - val_loss: 26.0230 - val_acc: 0.8000\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 25.9863 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 25.9863 - acc: 0.9677 - val_loss: 25.4441 - val_acc: 0.8000\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 25.4086 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 25.4086 - acc: 0.9677 - val_loss: 24.8737 - val_acc: 0.8000\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 24.8172 - acc: 0.93\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 24.8172 - acc: 0.9355 - val_loss: 24.3117 - val_acc: 0.8000\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.2492 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 24.2492 - acc: 1.0000 - val_loss: 23.7576 - val_acc: 0.8000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 23.6828 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 23.6828 - acc: 1.0000 - val_loss: 23.2116 - val_acc: 0.8000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 23.1358 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 23.1358 - acc: 1.0000 - val_loss: 22.6731 - val_acc: 0.8000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 22.6036 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 22.6036 - acc: 1.0000 - val_loss: 22.1428 - val_acc: 0.9333\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 22.0379 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 22.0379 - acc: 1.0000 - val_loss: 21.6193 - val_acc: 0.9333\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 21.5097 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 21.5097 - acc: 1.0000 - val_loss: 21.1043 - val_acc: 0.9333\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 20.9825 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 20.9825 - acc: 1.0000 - val_loss: 20.5978 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 20.4710 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 20.4710 - acc: 1.0000 - val_loss: 20.0993 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 19.9696 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 19.9696 - acc: 1.0000 - val_loss: 19.6096 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.4870 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 19.4870 - acc: 1.0000 - val_loss: 19.1250 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 18.9921 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 18.9921 - acc: 1.0000 - val_loss: 18.6511 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.5229 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 18.5229 - acc: 1.0000 - val_loss: 18.1870 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 18.0686 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 18.0686 - acc: 1.0000 - val_loss: 17.7326 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.6099 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 17.6099 - acc: 1.0000 - val_loss: 17.2873 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.1704 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 17.1704 - acc: 1.0000 - val_loss: 16.8488 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.7374 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 16.7374 - acc: 1.0000 - val_loss: 16.4202 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.3166 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 16.3166 - acc: 1.0000 - val_loss: 16.0007 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 15.9028 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 15.9028 - acc: 1.0000 - val_loss: 15.5906 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.4956 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 15.4956 - acc: 1.0000 - val_loss: 15.1902 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.1034 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 15.1034 - acc: 1.0000 - val_loss: 14.7972 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.7129 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 14.7129 - acc: 1.0000 - val_loss: 14.4123 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.3319 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 14.3319 - acc: 1.0000 - val_loss: 14.0360 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.9636 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 13.9636 - acc: 1.0000 - val_loss: 13.6689 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.5999 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 13.5999 - acc: 1.0000 - val_loss: 13.3078 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.2410 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 13.2410 - acc: 1.0000 - val_loss: 12.9552 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.8943 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 12.8943 - acc: 1.0000 - val_loss: 12.6102 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.5542 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 12.5542 - acc: 1.0000 - val_loss: 12.2714 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 12.2158 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 12.2158 - acc: 1.0000 - val_loss: 11.9402 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.8881 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 11.8881 - acc: 1.0000 - val_loss: 11.6164 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 11.5668 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 11.5668 - acc: 1.0000 - val_loss: 11.2978 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.2505 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 11.2505 - acc: 1.0000 - val_loss: 10.9840 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.9382 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 10.9382 - acc: 1.0000 - val_loss: 10.6780 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 10.6355 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 10.6355 - acc: 1.0000 - val_loss: 10.3791 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.3367 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 10.3367 - acc: 1.0000 - val_loss: 10.0865 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.0462 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 10.0462 - acc: 1.0000 - val_loss: 9.8015 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 9.7615 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 9.7615 - acc: 1.0000 - val_loss: 9.5231 - val_acc: 1.0000\n",
      "===== Training Epoch 44 =====\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 9.4846 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 9.4846 - acc: 1.0000 - val_loss: 9.2511 - val_acc: 1.0000\n",
      "Epoch 44: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 205ms/steps: 32.5951 - acc: 0.48\n",
      "TRAIN\t\tacc: 0.838710\tacc_top5: 1.000000\tf1_macro: 0.827202\tprec_macro: 0.886364\trec_macro: 0.821429\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 2s 2s/step - loss: 32.5951 - acc: 0.4839 - val_loss: 31.7824 - val_acc: 0.7333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 31.7691 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 31.7691 - acc: 0.7419 - val_loss: 30.9864 - val_acc: 0.8000\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 30.9891 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 30.9891 - acc: 0.7419 - val_loss: 30.2040 - val_acc: 0.8000\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 30.2023 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 30.2023 - acc: 0.8710 - val_loss: 29.4382 - val_acc: 0.8667\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 29.4140 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 29.4140 - acc: 0.9677 - val_loss: 28.6851 - val_acc: 0.9333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.6617 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 28.6617 - acc: 1.0000 - val_loss: 27.9438 - val_acc: 0.9333\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 27.9075 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 27.9075 - acc: 1.0000 - val_loss: 27.2148 - val_acc: 0.9333\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 27.1646 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 27.1646 - acc: 1.0000 - val_loss: 26.4975 - val_acc: 0.9333\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 26.4478 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 26.4478 - acc: 1.0000 - val_loss: 25.7928 - val_acc: 0.9333\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 25.7522 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 25.7522 - acc: 1.0000 - val_loss: 25.1002 - val_acc: 0.9333\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 25.0527 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 25.0527 - acc: 1.0000 - val_loss: 24.4196 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 24.3713 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 24.3713 - acc: 1.0000 - val_loss: 23.7502 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.6912 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 23.6912 - acc: 1.0000 - val_loss: 23.0940 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.0353 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 23.0353 - acc: 1.0000 - val_loss: 22.4501 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 22.3863 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 22.3863 - acc: 1.0000 - val_loss: 21.8203 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 21.7600 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 21.7600 - acc: 1.0000 - val_loss: 21.2023 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 21.1484 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 21.1484 - acc: 1.0000 - val_loss: 20.5938 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 20.5232 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 20.5232 - acc: 1.0000 - val_loss: 20.0034 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.9433 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 19.9433 - acc: 1.0000 - val_loss: 19.4279 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 19.3648 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 19.3648 - acc: 1.0000 - val_loss: 18.8659 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 18.8162 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 18.8162 - acc: 1.0000 - val_loss: 18.3196 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 18.2715 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 18.2715 - acc: 1.0000 - val_loss: 17.7857 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.7383 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 17.7383 - acc: 1.0000 - val_loss: 17.2664 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 17.2212 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 17.2212 - acc: 1.0000 - val_loss: 16.7579 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.7237 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 16.7237 - acc: 1.0000 - val_loss: 16.2605 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 16.2258 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 16.2258 - acc: 1.0000 - val_loss: 15.7753 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 15.7425 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 15.7425 - acc: 1.0000 - val_loss: 15.3007 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 33ms/stepss: 15.2740 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 15.2740 - acc: 1.0000 - val_loss: 14.8365 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.8118 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 14.8118 - acc: 1.0000 - val_loss: 14.3835 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.3588 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 14.3588 - acc: 1.0000 - val_loss: 13.9413 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.9199 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 13.9199 - acc: 1.0000 - val_loss: 13.5085 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 13.4870 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 13.4870 - acc: 1.0000 - val_loss: 13.0880 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.0694 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 13.0694 - acc: 1.0000 - val_loss: 12.6782 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.6618 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 12.6618 - acc: 1.0000 - val_loss: 12.2787 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.2622 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 12.2622 - acc: 1.0000 - val_loss: 11.8906 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.8754 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 11.8754 - acc: 1.0000 - val_loss: 11.5136 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.4993 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 11.4993 - acc: 1.0000 - val_loss: 11.1456 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.1323 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 11.1323 - acc: 1.0000 - val_loss: 10.7841 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.7716 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 10.7716 - acc: 1.0000 - val_loss: 10.4310 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 10.4193 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 10.4193 - acc: 1.0000 - val_loss: 10.0867 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 10.0756 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 10.0756 - acc: 1.0000 - val_loss: 9.7512 - val_acc: 1.0000\n",
      "Epoch 41: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 203ms/steps: 28.3693 - acc: 0.29\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.577273\tprec_macro: 0.616848\trec_macro: 0.590336\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.550000\tprec_macro: 0.625000\trec_macro: 0.580357\n",
      "1/1 [==============================] - 2s 2s/step - loss: 28.3693 - acc: 0.2903 - val_loss: 28.3543 - val_acc: 0.6000\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 28.3531 - acc: 0.58\n",
      "TRAIN\t\tacc: 0.645161\tacc_top5: 1.000000\tf1_macro: 0.582619\tprec_macro: 0.707692\trec_macro: 0.613445\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 28.3531 - acc: 0.5806 - val_loss: 28.3533 - val_acc: 0.6667\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.3542 - acc: 0.58\n",
      "TRAIN\t\tacc: 0.677419\tacc_top5: 1.000000\tf1_macro: 0.608586\tprec_macro: 0.814815\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 28.3542 - acc: 0.5806 - val_loss: 28.3598 - val_acc: 0.6000\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 28.3585 - acc: 0.64\n",
      "TRAIN\t\tacc: 0.645161\tacc_top5: 1.000000\tf1_macro: 0.554248\tprec_macro: 0.803571\trec_macro: 0.607143\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 28.3585 - acc: 0.6452 - val_loss: 28.3727 - val_acc: 0.5333\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 28.3671 - acc: 0.64\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.494565\tprec_macro: 0.793103\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 28.3671 - acc: 0.6452 - val_loss: 28.3916 - val_acc: 0.5333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 28.3860 - acc: 0.61\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.494565\tprec_macro: 0.793103\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 28.3860 - acc: 0.6129 - val_loss: 28.4163 - val_acc: 0.5333\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 28.4077 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.494565\tprec_macro: 0.793103\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 28.4077 - acc: 0.7419 - val_loss: 28.4476 - val_acc: 0.5333\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 28.4400 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.494565\tprec_macro: 0.793103\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 28.4400 - acc: 0.6774 - val_loss: 28.4852 - val_acc: 0.5333\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 28.4747 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.677419\tacc_top5: 1.000000\tf1_macro: 0.608586\tprec_macro: 0.814815\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 28.4747 - acc: 0.7097 - val_loss: 28.5296 - val_acc: 0.5333\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.5135 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.709677\tacc_top5: 1.000000\tf1_macro: 0.658507\tprec_macro: 0.826923\trec_macro: 0.678571\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 28.5135 - acc: 0.6774 - val_loss: 28.5797 - val_acc: 0.5333\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 28.5656 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.709677\tacc_top5: 1.000000\tf1_macro: 0.658507\tprec_macro: 0.826923\trec_macro: 0.678571\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 28.5656 - acc: 0.6774 - val_loss: 28.6356 - val_acc: 0.5333\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 28.6176 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.709677\tacc_top5: 1.000000\tf1_macro: 0.658507\tprec_macro: 0.826923\trec_macro: 0.678571\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 28.6176 - acc: 0.7419 - val_loss: 28.6971 - val_acc: 0.5333\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.6770 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.741935\tacc_top5: 1.000000\tf1_macro: 0.704762\tprec_macro: 0.840000\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 28.6770 - acc: 0.7419 - val_loss: 28.7643 - val_acc: 0.6000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.7439 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.741935\tacc_top5: 1.000000\tf1_macro: 0.704762\tprec_macro: 0.840000\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 28.7439 - acc: 0.7097 - val_loss: 28.8374 - val_acc: 0.6000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.8117 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.838710\tacc_top5: 1.000000\tf1_macro: 0.827202\tprec_macro: 0.886364\trec_macro: 0.821429\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 28.8117 - acc: 0.8065 - val_loss: 28.9170 - val_acc: 0.6667\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.8875 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 28.8875 - acc: 0.8710 - val_loss: 29.0032 - val_acc: 0.6667\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.9721 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 28.9721 - acc: 0.8065 - val_loss: 29.0948 - val_acc: 0.6667\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 29.0600 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 29.0600 - acc: 0.8387 - val_loss: 29.1918 - val_acc: 0.6667\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 29.1536 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 29.1536 - acc: 0.8065 - val_loss: 29.2936 - val_acc: 0.6667\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.2480 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 29.2480 - acc: 0.9677 - val_loss: 29.4002 - val_acc: 0.6667\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.3524 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 29.3524 - acc: 0.9355 - val_loss: 29.5105 - val_acc: 0.7333\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 29.4556 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 29.4556 - acc: 0.9677 - val_loss: 29.6251 - val_acc: 0.7333\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 29.5674 - acc: 1.00\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 29.5674 - acc: 1.0000 - val_loss: 29.7433 - val_acc: 0.7333\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.6843 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 29.6843 - acc: 0.9677 - val_loss: 29.8645 - val_acc: 0.8000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 29.7950 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 29.7950 - acc: 0.9677 - val_loss: 29.9889 - val_acc: 0.8000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.9182 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 29.9182 - acc: 1.0000 - val_loss: 30.1154 - val_acc: 0.8667\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.0390 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 30.0390 - acc: 1.0000 - val_loss: 30.2438 - val_acc: 0.8667\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 30.1577 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 30.1577 - acc: 1.0000 - val_loss: 30.3729 - val_acc: 0.8667\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.2821 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 30.2821 - acc: 1.0000 - val_loss: 30.5021 - val_acc: 0.9333\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.4070 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 30.4070 - acc: 1.0000 - val_loss: 30.6313 - val_acc: 0.9333\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.5357 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 30.5357 - acc: 1.0000 - val_loss: 30.7607 - val_acc: 0.9333\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 30.6577 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 30.6577 - acc: 1.0000 - val_loss: 30.8904 - val_acc: 0.9333\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 30.7837 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 30.7837 - acc: 1.0000 - val_loss: 31.0199 - val_acc: 0.9333\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 30.9066 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 30.9066 - acc: 1.0000 - val_loss: 31.1510 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 31.0401 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 31.0401 - acc: 1.0000 - val_loss: 31.2854 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 31.1779 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 31.1779 - acc: 1.0000 - val_loss: 31.4229 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 31.3095 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 31.3095 - acc: 1.0000 - val_loss: 31.5641 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 31.4459 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 31.4459 - acc: 1.0000 - val_loss: 31.7112 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 31.5953 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 31.5953 - acc: 1.0000 - val_loss: 31.8650 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 31.7513 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 31.7513 - acc: 1.0000 - val_loss: 32.0276 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 31.9217 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 31.9217 - acc: 1.0000 - val_loss: 32.2002 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 32.0909 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 32.0909 - acc: 1.0000 - val_loss: 32.3844 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 32.2809 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 32.2809 - acc: 1.0000 - val_loss: 32.5797 - val_acc: 1.0000\n",
      "===== Training Epoch 44 =====\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 32.4720 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 32.4720 - acc: 1.0000 - val_loss: 32.7860 - val_acc: 1.0000\n",
      "===== Training Epoch 45 =====\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 32.6904 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 32.6904 - acc: 1.0000 - val_loss: 33.0046 - val_acc: 1.0000\n",
      "===== Training Epoch 46 =====\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 32.9010 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 32.9010 - acc: 1.0000 - val_loss: 33.2380 - val_acc: 1.0000\n",
      "===== Training Epoch 47 =====\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 33.1387 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 33.1387 - acc: 1.0000 - val_loss: 33.4877 - val_acc: 1.0000\n",
      "===== Training Epoch 48 =====\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 33.3780 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 33.3780 - acc: 1.0000 - val_loss: 33.7482 - val_acc: 1.0000\n",
      "===== Training Epoch 49 =====\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 33.6369 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 33.6369 - acc: 1.0000 - val_loss: 34.0185 - val_acc: 1.0000\n",
      "===== Training Epoch 50 =====\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 33.9170 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 33.9170 - acc: 1.0000 - val_loss: 34.3001 - val_acc: 1.0000\n",
      "===== Training Epoch 51 =====\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 34.1968 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 34.1968 - acc: 1.0000 - val_loss: 34.5934 - val_acc: 1.0000\n",
      "===== Training Epoch 52 =====\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 34.5048 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 34.5048 - acc: 1.0000 - val_loss: 34.8992 - val_acc: 1.0000\n",
      "===== Training Epoch 53 =====\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 34.7843 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 34.7843 - acc: 1.0000 - val_loss: 35.2176 - val_acc: 1.0000\n",
      "===== Training Epoch 54 =====\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 35.1332 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 35.1332 - acc: 1.0000 - val_loss: 35.5485 - val_acc: 1.0000\n",
      "===== Training Epoch 55 =====\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 35.4507 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 35.4507 - acc: 1.0000 - val_loss: 35.8937 - val_acc: 1.0000\n",
      "===== Training Epoch 56 =====\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 35.7937 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 35.7937 - acc: 1.0000 - val_loss: 36.2527 - val_acc: 1.0000\n",
      "===== Training Epoch 57 =====\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 36.1544 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 36.1544 - acc: 1.0000 - val_loss: 36.6278 - val_acc: 1.0000\n",
      "===== Training Epoch 58 =====\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 36.5219 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 36.5219 - acc: 1.0000 - val_loss: 37.0176 - val_acc: 1.0000\n",
      "===== Training Epoch 59 =====\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 36.9192 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 36.9192 - acc: 1.0000 - val_loss: 37.4222 - val_acc: 1.0000\n",
      "===== Training Epoch 60 =====\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 37.3131 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 37.3131 - acc: 1.0000 - val_loss: 37.8425 - val_acc: 1.0000\n",
      "===== Training Epoch 61 =====\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 37.7326 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 37.7326 - acc: 1.0000 - val_loss: 38.2774 - val_acc: 1.0000\n",
      "===== Training Epoch 62 =====\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 38.1647 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 38.1647 - acc: 1.0000 - val_loss: 38.7264 - val_acc: 1.0000\n",
      "===== Training Epoch 63 =====\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 38.6077 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 38.6077 - acc: 1.0000 - val_loss: 39.1896 - val_acc: 1.0000\n",
      "===== Training Epoch 64 =====\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 39.0896 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 39.0896 - acc: 1.0000 - val_loss: 39.6689 - val_acc: 0.9333\n",
      "Epoch 64: early stopping\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 231ms/steps: 32.6407 - acc: 0.61\n",
      "TRAIN\t\tacc: 0.774194\tacc_top5: 1.000000\tf1_macro: 0.747967\tprec_macro: 0.854167\trec_macro: 0.750000\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 2s 2s/step - loss: 32.6407 - acc: 0.6129 - val_loss: 31.8484 - val_acc: 0.7333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 31.8477 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.645161\tacc_top5: 1.000000\tf1_macro: 0.554248\tprec_macro: 0.803571\trec_macro: 0.607143\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 31.8477 - acc: 0.8387 - val_loss: 31.0693 - val_acc: 0.6667\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 31.0684 - acc: 0.77\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.494565\tprec_macro: 0.793103\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 31.0684 - acc: 0.7742 - val_loss: 30.3048 - val_acc: 0.6000\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 30.3031 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.580645\tacc_top5: 1.000000\tf1_macro: 0.428369\tprec_macro: 0.783333\trec_macro: 0.535714\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 30.3031 - acc: 0.6774 - val_loss: 29.5567 - val_acc: 0.6000\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 28ms/stepss: 29.5555 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.580645\tacc_top5: 1.000000\tf1_macro: 0.428369\tprec_macro: 0.783333\trec_macro: 0.535714\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 29.5555 - acc: 0.6774 - val_loss: 28.8220 - val_acc: 0.5333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 28.8159 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.580645\tacc_top5: 1.000000\tf1_macro: 0.428369\tprec_macro: 0.783333\trec_macro: 0.535714\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 28.8159 - acc: 0.7097 - val_loss: 28.1010 - val_acc: 0.5333\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.0971 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.494565\tprec_macro: 0.793103\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 28.0971 - acc: 0.7097 - val_loss: 27.3939 - val_acc: 0.5333\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 27.3882 - acc: 0.64\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.494565\tprec_macro: 0.793103\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 27.3882 - acc: 0.6452 - val_loss: 26.6993 - val_acc: 0.5333\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 26.6922 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.494565\tprec_macro: 0.793103\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 26.6922 - acc: 0.6774 - val_loss: 26.0187 - val_acc: 0.5333\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 26.0075 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.645161\tacc_top5: 1.000000\tf1_macro: 0.554248\tprec_macro: 0.803571\trec_macro: 0.607143\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 26.0075 - acc: 0.7419 - val_loss: 25.3494 - val_acc: 0.5333\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 25.3391 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.677419\tacc_top5: 1.000000\tf1_macro: 0.608586\tprec_macro: 0.814815\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 25.3391 - acc: 0.7097 - val_loss: 24.6924 - val_acc: 0.5333\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 24.6773 - acc: 0.77\n",
      "TRAIN\t\tacc: 0.677419\tacc_top5: 1.000000\tf1_macro: 0.608586\tprec_macro: 0.814815\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 24.6773 - acc: 0.7742 - val_loss: 24.0462 - val_acc: 0.5333\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 24.0305 - acc: 0.77\n",
      "TRAIN\t\tacc: 0.774194\tacc_top5: 1.000000\tf1_macro: 0.747967\tprec_macro: 0.854167\trec_macro: 0.750000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 24.0305 - acc: 0.7742 - val_loss: 23.4126 - val_acc: 0.6000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.3972 - acc: 0.77\n",
      "TRAIN\t\tacc: 0.806452\tacc_top5: 1.000000\tf1_macro: 0.788636\tprec_macro: 0.869565\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 23.3972 - acc: 0.7742 - val_loss: 22.7923 - val_acc: 0.6000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 22.7719 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.838710\tacc_top5: 1.000000\tf1_macro: 0.827202\tprec_macro: 0.886364\trec_macro: 0.821429\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 22.7719 - acc: 0.8387 - val_loss: 22.1829 - val_acc: 0.6000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 22.1599 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.899459\tprec_macro: 0.925000\trec_macro: 0.892857\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 22.1599 - acc: 0.9032 - val_loss: 21.5838 - val_acc: 0.6667\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 21.5585 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 21.5585 - acc: 0.9032 - val_loss: 20.9916 - val_acc: 0.6667\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 20.9647 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 20.9647 - acc: 0.9032 - val_loss: 20.4119 - val_acc: 0.7333\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 20.3809 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 20.3809 - acc: 0.9677 - val_loss: 19.8420 - val_acc: 0.8000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 19.8081 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 19.8081 - acc: 0.9677 - val_loss: 19.2825 - val_acc: 0.8667\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.2447 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 19.2447 - acc: 0.9677 - val_loss: 18.7347 - val_acc: 0.9333\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 18.6928 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 18.6928 - acc: 0.9677 - val_loss: 18.1940 - val_acc: 0.9333\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 18.1522 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 18.1522 - acc: 0.9677 - val_loss: 17.6650 - val_acc: 0.9333\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 17.6191 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 17.6191 - acc: 0.9677 - val_loss: 17.1457 - val_acc: 0.9333\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 17.0907 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 17.0907 - acc: 0.9677 - val_loss: 16.6375 - val_acc: 0.9333\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 16.5835 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 16.5835 - acc: 1.0000 - val_loss: 16.1417 - val_acc: 0.9333\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.0875 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 16.0875 - acc: 1.0000 - val_loss: 15.6558 - val_acc: 0.9333\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.5975 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 15.5975 - acc: 1.0000 - val_loss: 15.1783 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 15.1125 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 15.1125 - acc: 1.0000 - val_loss: 14.7108 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.6438 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 14.6438 - acc: 1.0000 - val_loss: 14.2531 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.1858 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 14.1858 - acc: 1.0000 - val_loss: 13.8031 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.7342 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 13.7342 - acc: 1.0000 - val_loss: 13.3644 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.2888 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 13.2888 - acc: 1.0000 - val_loss: 12.9359 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.8616 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 12.8616 - acc: 1.0000 - val_loss: 12.5179 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.4447 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 12.4447 - acc: 1.0000 - val_loss: 12.1096 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 12.0368 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 12.0368 - acc: 1.0000 - val_loss: 11.7130 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.6453 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 11.6453 - acc: 1.0000 - val_loss: 11.3253 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.2542 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 11.2542 - acc: 1.0000 - val_loss: 10.9459 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.8775 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 10.8775 - acc: 1.0000 - val_loss: 10.5777 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.5128 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 10.5128 - acc: 1.0000 - val_loss: 10.2216 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.1622 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 10.1622 - acc: 1.0000 - val_loss: 9.8757 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 9.8136 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 9.8136 - acc: 1.0000 - val_loss: 9.5390 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 9.4795 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 9.4795 - acc: 1.0000 - val_loss: 9.2108 - val_acc: 1.0000\n",
      "===== Training Epoch 44 =====\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 9.1555 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 9.1555 - acc: 1.0000 - val_loss: 8.8911 - val_acc: 1.0000\n",
      "===== Training Epoch 45 =====\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 8.8373 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 8.8373 - acc: 1.0000 - val_loss: 8.5801 - val_acc: 1.0000\n",
      "===== Training Epoch 46 =====\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 8.5281 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 8.5281 - acc: 1.0000 - val_loss: 8.2799 - val_acc: 1.0000\n",
      "===== Training Epoch 47 =====\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 8.2288 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 8.2288 - acc: 1.0000 - val_loss: 7.9860 - val_acc: 1.0000\n",
      "===== Training Epoch 48 =====\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 7.9405 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 7.9405 - acc: 1.0000 - val_loss: 7.7013 - val_acc: 1.0000\n",
      "===== Training Epoch 49 =====\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 7.6571 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 7.6571 - acc: 1.0000 - val_loss: 7.4271 - val_acc: 1.0000\n",
      "===== Training Epoch 50 =====\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 7.3851 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 7.3851 - acc: 1.0000 - val_loss: 7.1648 - val_acc: 1.0000\n",
      "===== Training Epoch 51 =====\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 7.1243 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 7.1243 - acc: 1.0000 - val_loss: 6.9101 - val_acc: 1.0000\n",
      "===== Training Epoch 52 =====\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 6.8709 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 6.8709 - acc: 1.0000 - val_loss: 6.6619 - val_acc: 1.0000\n",
      "===== Training Epoch 53 =====\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 6.6264 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 6.6264 - acc: 1.0000 - val_loss: 6.4202 - val_acc: 1.0000\n",
      "===== Training Epoch 54 =====\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 6.3844 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 6.3844 - acc: 1.0000 - val_loss: 6.1838 - val_acc: 1.0000\n",
      "===== Training Epoch 55 =====\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 6.1502 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 6.1502 - acc: 1.0000 - val_loss: 5.9540 - val_acc: 1.0000\n",
      "===== Training Epoch 56 =====\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 5.9226 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 5.9226 - acc: 1.0000 - val_loss: 5.7329 - val_acc: 1.0000\n",
      "===== Training Epoch 57 =====\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 5.7021 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 5.7021 - acc: 1.0000 - val_loss: 5.5207 - val_acc: 1.0000\n",
      "===== Training Epoch 58 =====\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 5.4886 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 5.4886 - acc: 1.0000 - val_loss: 5.3161 - val_acc: 1.0000\n",
      "Epoch 58: early stopping\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 203ms/steps: 28.3459 - acc: 0.54\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 2s 2s/step - loss: 28.3459 - acc: 0.5484 - val_loss: 28.3291 - val_acc: 0.7333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 28.3132 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.899459\tprec_macro: 0.925000\trec_macro: 0.892857\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 28.3132 - acc: 0.6774 - val_loss: 28.3184 - val_acc: 0.7333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 28.3034 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 28.3034 - acc: 0.8065 - val_loss: 28.3133 - val_acc: 0.7333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 28.2964 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 28.2964 - acc: 0.8710 - val_loss: 28.3097 - val_acc: 0.8000\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 28.2887 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 28.2887 - acc: 0.9032 - val_loss: 28.3057 - val_acc: 0.8000\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 28.2628 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 28.2628 - acc: 0.9355 - val_loss: 28.3007 - val_acc: 0.8000\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 28.2540 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 28.2540 - acc: 0.9355 - val_loss: 28.2941 - val_acc: 0.8000\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.2378 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 28.2378 - acc: 0.9355 - val_loss: 28.2855 - val_acc: 0.8667\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.2093 - acc: 1.00\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 28.2093 - acc: 1.0000 - val_loss: 28.2749 - val_acc: 0.8667\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.2142 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 28.2142 - acc: 1.0000 - val_loss: 28.2685 - val_acc: 0.8667\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.1985 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 28.1985 - acc: 1.0000 - val_loss: 28.2606 - val_acc: 0.8667\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.1705 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 28.1705 - acc: 1.0000 - val_loss: 28.2511 - val_acc: 0.9333\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.1541 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 28.1541 - acc: 1.0000 - val_loss: 28.2403 - val_acc: 0.9333\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.1456 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 28.1456 - acc: 1.0000 - val_loss: 28.2352 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.1283 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 28.1283 - acc: 1.0000 - val_loss: 28.2294 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.1217 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 28.1217 - acc: 1.0000 - val_loss: 28.2230 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.1231 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 28.1231 - acc: 1.0000 - val_loss: 28.2238 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.1160 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 28.1160 - acc: 1.0000 - val_loss: 28.2248 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.1151 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 28.1151 - acc: 1.0000 - val_loss: 28.2261 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.1194 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 28.1194 - acc: 1.0000 - val_loss: 28.2278 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.1266 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 28.1266 - acc: 1.0000 - val_loss: 28.2298 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.1344 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 28.1344 - acc: 1.0000 - val_loss: 28.2321 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.1373 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 28.1373 - acc: 1.0000 - val_loss: 28.2347 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.1398 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 28.1398 - acc: 1.0000 - val_loss: 28.2379 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.1551 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 28.1551 - acc: 1.0000 - val_loss: 28.2411 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.1574 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 28.1574 - acc: 1.0000 - val_loss: 28.2447 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.1648 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 28.1648 - acc: 1.0000 - val_loss: 28.2488 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.1773 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 28.1773 - acc: 1.0000 - val_loss: 28.2530 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.1835 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 28.1835 - acc: 1.0000 - val_loss: 28.2572 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 28.1903 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 28.1903 - acc: 1.0000 - val_loss: 28.2616 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.2025 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 28.2025 - acc: 1.0000 - val_loss: 28.2656 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.2083 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 28.2083 - acc: 1.0000 - val_loss: 28.2922 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.2385 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 28.2385 - acc: 1.0000 - val_loss: 28.3185 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.2671 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 28.2671 - acc: 1.0000 - val_loss: 28.3438 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.2946 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 28.2946 - acc: 1.0000 - val_loss: 28.3880 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.3414 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 28.3414 - acc: 1.0000 - val_loss: 28.4315 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.3873 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 28.3873 - acc: 1.0000 - val_loss: 28.4739 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.4312 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 28.4312 - acc: 1.0000 - val_loss: 28.5144 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.4731 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 28.4731 - acc: 1.0000 - val_loss: 28.5529 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.5137 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 28.5137 - acc: 1.0000 - val_loss: 28.5894 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.5510 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 28.5510 - acc: 1.0000 - val_loss: 28.6237 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.5872 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 28.5872 - acc: 1.0000 - val_loss: 28.6560 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.6199 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 28.6199 - acc: 1.0000 - val_loss: 28.6860 - val_acc: 1.0000\n",
      "===== Training Epoch 44 =====\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.6507 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 28.6507 - acc: 1.0000 - val_loss: 28.7142 - val_acc: 1.0000\n",
      "Epoch 44: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 220ms/steps: 32.5981 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 2s 2s/step - loss: 32.5981 - acc: 0.7097 - val_loss: 31.7973 - val_acc: 0.8000\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 31.7922 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 31.7922 - acc: 0.8710 - val_loss: 30.9978 - val_acc: 0.9333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 30.9900 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 30.9900 - acc: 0.9032 - val_loss: 30.2128 - val_acc: 0.9333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 30.1997 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 30.1997 - acc: 0.9677 - val_loss: 29.4429 - val_acc: 0.9333\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 29.4291 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 29.4291 - acc: 0.9677 - val_loss: 28.6854 - val_acc: 0.9333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 28.6622 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 28.6622 - acc: 0.9677 - val_loss: 27.9418 - val_acc: 0.9333\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 27.9189 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 27.9189 - acc: 0.9677 - val_loss: 27.2110 - val_acc: 0.9333\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 27.1840 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 27.1840 - acc: 0.9677 - val_loss: 26.4920 - val_acc: 0.9333\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 26.4562 - acc: 1.00\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 26.4562 - acc: 1.0000 - val_loss: 25.7857 - val_acc: 0.9333\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 25.7434 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 25.7434 - acc: 1.0000 - val_loss: 25.0909 - val_acc: 0.9333\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 25.0595 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 25.0595 - acc: 1.0000 - val_loss: 24.4061 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 24.3468 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 24.3468 - acc: 1.0000 - val_loss: 23.7313 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.6824 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 23.6824 - acc: 1.0000 - val_loss: 23.0703 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.0170 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 23.0170 - acc: 1.0000 - val_loss: 22.4234 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 22.3708 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 22.3708 - acc: 1.0000 - val_loss: 21.7892 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 21.7328 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 21.7328 - acc: 1.0000 - val_loss: 21.1700 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 21.1217 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 21.1217 - acc: 1.0000 - val_loss: 20.5618 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 20.5086 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 20.5086 - acc: 1.0000 - val_loss: 19.9702 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 19.9173 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 19.9173 - acc: 1.0000 - val_loss: 19.3921 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.3488 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 19.3488 - acc: 1.0000 - val_loss: 18.8285 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.7854 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 18.7854 - acc: 1.0000 - val_loss: 18.2808 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.2445 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 18.2445 - acc: 1.0000 - val_loss: 17.7434 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.7104 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 17.7104 - acc: 1.0000 - val_loss: 17.2209 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.1911 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 17.1911 - acc: 1.0000 - val_loss: 16.7090 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.6839 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 16.6839 - acc: 1.0000 - val_loss: 16.2111 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 16.1890 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 16.1890 - acc: 1.0000 - val_loss: 15.7282 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.7072 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 15.7072 - acc: 1.0000 - val_loss: 15.2565 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.2397 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 15.2397 - acc: 1.0000 - val_loss: 14.7947 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.7787 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 14.7787 - acc: 1.0000 - val_loss: 14.3433 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.3290 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 14.3290 - acc: 1.0000 - val_loss: 13.9032 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.8916 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 13.8916 - acc: 1.0000 - val_loss: 13.4734 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 13.4616 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 13.4616 - acc: 1.0000 - val_loss: 13.0545 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.0440 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 13.0440 - acc: 1.0000 - val_loss: 12.6437 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 12.6339 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 12.6339 - acc: 1.0000 - val_loss: 12.2425 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.2341 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 12.2341 - acc: 1.0000 - val_loss: 11.8512 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.8435 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 11.8435 - acc: 1.0000 - val_loss: 11.4710 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.4636 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 11.4636 - acc: 1.0000 - val_loss: 11.0995 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.0927 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 11.0927 - acc: 1.0000 - val_loss: 10.7383 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.7318 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 10.7318 - acc: 1.0000 - val_loss: 10.3874 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.3814 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 10.3814 - acc: 1.0000 - val_loss: 10.0469 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.0414 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 10.0414 - acc: 1.0000 - val_loss: 9.7157 - val_acc: 1.0000\n",
      "Epoch 41: early stopping\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 217ms/steps: 28.3865 - acc: 0.45\n",
      "TRAIN\t\tacc: 0.838710\tacc_top5: 1.000000\tf1_macro: 0.832432\tprec_macro: 0.854545\trec_macro: 0.827731\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 2s 2s/step - loss: 28.3865 - acc: 0.4516 - val_loss: 28.3118 - val_acc: 0.8000\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 28.3172 - acc: 0.77\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 28.3172 - acc: 0.7742 - val_loss: 28.2850 - val_acc: 0.8000\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 28.2801 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 28.2801 - acc: 0.8065 - val_loss: 28.2642 - val_acc: 0.8000\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 28.2314 - acc: 0.87\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 28.2314 - acc: 0.8710 - val_loss: 28.2480 - val_acc: 0.8000\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.2069 - acc: 0.87\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 28.2069 - acc: 0.8710 - val_loss: 28.2348 - val_acc: 0.8667\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 28.1698 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 28.1698 - acc: 0.9677 - val_loss: 28.2246 - val_acc: 0.8667\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 28.1411 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 28.1411 - acc: 0.9677 - val_loss: 28.2173 - val_acc: 0.8667\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 29ms/stepss: 28.1467 - acc: 0.93\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 28.1467 - acc: 0.9355 - val_loss: 28.2129 - val_acc: 0.9333\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 28ms/stepss: 28.1270 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 28.1270 - acc: 1.0000 - val_loss: 28.2113 - val_acc: 1.0000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 28.1173 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 28.1173 - acc: 1.0000 - val_loss: 28.2133 - val_acc: 1.0000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 28.1245 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 28.1245 - acc: 1.0000 - val_loss: 28.2186 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.1109 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 28.1109 - acc: 1.0000 - val_loss: 28.2278 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.1181 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 28.1181 - acc: 1.0000 - val_loss: 28.2414 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.1395 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 28.1395 - acc: 1.0000 - val_loss: 28.2592 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 28.1513 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 28.1513 - acc: 1.0000 - val_loss: 28.2818 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.1748 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 28.1748 - acc: 1.0000 - val_loss: 28.3094 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.2016 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 28.2016 - acc: 1.0000 - val_loss: 28.3415 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.2405 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 28.2405 - acc: 1.0000 - val_loss: 28.3787 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.2732 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 28.2732 - acc: 1.0000 - val_loss: 28.4207 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.3261 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 28.3261 - acc: 1.0000 - val_loss: 28.4675 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.3852 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 28.3852 - acc: 1.0000 - val_loss: 28.5189 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.4445 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 28.4445 - acc: 1.0000 - val_loss: 28.5745 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.4951 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 28.4951 - acc: 1.0000 - val_loss: 28.6348 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 28.5606 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 28.5606 - acc: 1.0000 - val_loss: 28.6995 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.6332 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 28.6332 - acc: 1.0000 - val_loss: 28.7686 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.7045 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 28.7045 - acc: 1.0000 - val_loss: 28.8424 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.7817 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 28.7817 - acc: 1.0000 - val_loss: 28.9207 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 28.8633 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 28.8633 - acc: 1.0000 - val_loss: 29.0038 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.9490 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 28.9490 - acc: 1.0000 - val_loss: 29.0923 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.0393 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 29.0393 - acc: 1.0000 - val_loss: 29.1862 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.1341 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 29.1341 - acc: 1.0000 - val_loss: 29.2854 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.2361 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 29.2361 - acc: 1.0000 - val_loss: 29.3899 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.3411 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 29.3411 - acc: 1.0000 - val_loss: 29.5001 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 29.4527 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 29.4527 - acc: 1.0000 - val_loss: 29.6162 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.5695 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 29.5695 - acc: 1.0000 - val_loss: 29.7385 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.6918 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 29.6918 - acc: 1.0000 - val_loss: 29.8673 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 29.8217 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 29.8217 - acc: 1.0000 - val_loss: 30.0029 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 29.9563 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 29.9563 - acc: 1.0000 - val_loss: 30.1441 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.0978 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 30.0978 - acc: 1.0000 - val_loss: 30.2908 - val_acc: 1.0000\n",
      "Epoch 39: early stopping\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 259ms/steps: 32.5812 - acc: 0.25\n",
      "TRAIN\t\tacc: 0.774194\tacc_top5: 1.000000\tf1_macro: 0.747967\tprec_macro: 0.854167\trec_macro: 0.750000\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 2s 2s/step - loss: 32.5812 - acc: 0.2581 - val_loss: 31.7559 - val_acc: 0.6667\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 33ms/stepss: 31.7414 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 31.7414 - acc: 0.7419 - val_loss: 30.9478 - val_acc: 0.7333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 29ms/stepss: 30.9272 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 30.9272 - acc: 0.8710 - val_loss: 30.1541 - val_acc: 0.9333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 30.1339 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 30.1339 - acc: 0.9032 - val_loss: 29.3777 - val_acc: 1.0000\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 28ms/stepss: 29.3623 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 29.3623 - acc: 0.9355 - val_loss: 28.6145 - val_acc: 1.0000\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 28.5868 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 28.5868 - acc: 0.9677 - val_loss: 27.8627 - val_acc: 1.0000\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 25ms/stepss: 27.7980 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 27.7980 - acc: 0.9677 - val_loss: 27.1225 - val_acc: 1.0000\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 27.0710 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 27.0710 - acc: 0.9677 - val_loss: 26.3936 - val_acc: 1.0000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 26.3264 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 26.3264 - acc: 1.0000 - val_loss: 25.6775 - val_acc: 1.0000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 25.6280 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 25.6280 - acc: 1.0000 - val_loss: 24.9741 - val_acc: 1.0000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 24.9255 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 24.9255 - acc: 1.0000 - val_loss: 24.2852 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 24.2131 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 24.2131 - acc: 1.0000 - val_loss: 23.6116 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 23.5471 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 23.5471 - acc: 1.0000 - val_loss: 22.9525 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 22.8935 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 22.8935 - acc: 1.0000 - val_loss: 22.3088 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 22.2489 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 22.2489 - acc: 1.0000 - val_loss: 21.6803 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 21.6246 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 21.6246 - acc: 1.0000 - val_loss: 21.0687 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 21.0176 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 21.0176 - acc: 1.0000 - val_loss: 20.4698 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 20.4192 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 20.4192 - acc: 1.0000 - val_loss: 19.8887 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 19.8480 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 19.8480 - acc: 1.0000 - val_loss: 19.3223 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.2860 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 19.2860 - acc: 1.0000 - val_loss: 18.7692 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.7411 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 18.7411 - acc: 1.0000 - val_loss: 18.2301 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 18.2051 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 18.2051 - acc: 1.0000 - val_loss: 17.7015 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.6766 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 17.6766 - acc: 1.0000 - val_loss: 17.1879 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 17.1675 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 17.1675 - acc: 1.0000 - val_loss: 16.6873 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 16.6714 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 16.6714 - acc: 1.0000 - val_loss: 16.1972 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.1817 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 16.1817 - acc: 1.0000 - val_loss: 15.7182 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.7038 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 15.7038 - acc: 1.0000 - val_loss: 15.2496 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.2382 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 15.2382 - acc: 1.0000 - val_loss: 14.7904 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.7803 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 14.7803 - acc: 1.0000 - val_loss: 14.3424 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.3320 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 14.3320 - acc: 1.0000 - val_loss: 13.9048 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.8962 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 13.8962 - acc: 1.0000 - val_loss: 13.4768 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.4686 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 13.4686 - acc: 1.0000 - val_loss: 13.0600 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.0521 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 13.0521 - acc: 1.0000 - val_loss: 12.6529 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.6460 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 12.6460 - acc: 1.0000 - val_loss: 12.2554 - val_acc: 1.0000\n",
      "Epoch 34: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 231ms/steps: 28.3917 - acc: 0.48\n",
      "TRAIN\t\tacc: 0.741935\tacc_top5: 1.000000\tf1_macro: 0.704762\tprec_macro: 0.840000\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 2s 2s/step - loss: 28.3917 - acc: 0.4839 - val_loss: 28.3860 - val_acc: 0.6667\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 33ms/stepss: 28.3796 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.709677\tacc_top5: 1.000000\tf1_macro: 0.658507\tprec_macro: 0.826923\trec_macro: 0.678571\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 28.3796 - acc: 0.7419 - val_loss: 28.3857 - val_acc: 0.6667\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 37ms/stepss: 28.3841 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.677419\tacc_top5: 1.000000\tf1_macro: 0.608586\tprec_macro: 0.814815\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 28.3841 - acc: 0.7097 - val_loss: 28.3920 - val_acc: 0.5333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 36ms/stepss: 28.3853 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.774194\tacc_top5: 1.000000\tf1_macro: 0.747967\tprec_macro: 0.854167\trec_macro: 0.750000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 28.3853 - acc: 0.7419 - val_loss: 28.4050 - val_acc: 0.6000\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 28.3952 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.774194\tacc_top5: 1.000000\tf1_macro: 0.747967\tprec_macro: 0.854167\trec_macro: 0.750000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 28.3952 - acc: 0.8387 - val_loss: 28.4242 - val_acc: 0.6000\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 28.4171 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.806452\tacc_top5: 1.000000\tf1_macro: 0.788636\tprec_macro: 0.869565\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 28.4171 - acc: 0.6774 - val_loss: 28.4502 - val_acc: 0.6000\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 28.4347 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.838710\tacc_top5: 1.000000\tf1_macro: 0.827202\tprec_macro: 0.886364\trec_macro: 0.821429\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 28.4347 - acc: 0.8387 - val_loss: 28.4826 - val_acc: 0.6000\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 28.4686 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 28.4686 - acc: 0.8065 - val_loss: 28.5217 - val_acc: 0.6000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 29ms/stepss: 28.4998 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 28.4998 - acc: 0.9032 - val_loss: 28.5660 - val_acc: 0.6000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 28.5450 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.899459\tprec_macro: 0.925000\trec_macro: 0.892857\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 28.5450 - acc: 0.8387 - val_loss: 28.6154 - val_acc: 0.6667\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.5929 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.899459\tprec_macro: 0.925000\trec_macro: 0.892857\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 28.5929 - acc: 0.8387 - val_loss: 28.6699 - val_acc: 0.6667\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 28.6377 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 28.6377 - acc: 0.9355 - val_loss: 28.7289 - val_acc: 0.6667\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 28.6917 - acc: 1.00\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 28.6917 - acc: 1.0000 - val_loss: 28.7915 - val_acc: 0.6667\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 28.7491 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 28.7491 - acc: 0.9677 - val_loss: 28.8574 - val_acc: 0.6667\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.8111 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 28.8111 - acc: 0.9677 - val_loss: 28.9258 - val_acc: 0.6667\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.8724 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 28.8724 - acc: 0.9677 - val_loss: 28.9971 - val_acc: 0.6667\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.9414 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 28.9414 - acc: 1.0000 - val_loss: 29.0705 - val_acc: 0.6667\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 29.0112 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 29.0112 - acc: 1.0000 - val_loss: 29.1457 - val_acc: 0.8000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 29.0789 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 29.0789 - acc: 1.0000 - val_loss: 29.2207 - val_acc: 0.8000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.1440 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 29.1440 - acc: 1.0000 - val_loss: 29.2960 - val_acc: 0.8667\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.2113 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 29.2113 - acc: 1.0000 - val_loss: 29.3710 - val_acc: 0.8667\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.2829 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 29.2829 - acc: 1.0000 - val_loss: 29.4450 - val_acc: 0.9333\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 29.3494 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 29.3494 - acc: 1.0000 - val_loss: 29.5181 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.4142 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 29.4142 - acc: 1.0000 - val_loss: 29.5903 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.4902 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 29.4902 - acc: 1.0000 - val_loss: 29.6615 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.5521 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 29.5521 - acc: 1.0000 - val_loss: 29.7325 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.6181 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 29.6181 - acc: 1.0000 - val_loss: 29.8032 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 29.6916 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 29.6916 - acc: 1.0000 - val_loss: 29.8751 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 29.7560 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 29.7560 - acc: 1.0000 - val_loss: 29.9486 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.8248 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 29.8248 - acc: 1.0000 - val_loss: 30.0256 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 29.9028 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 29.9028 - acc: 1.0000 - val_loss: 30.1062 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 29.9807 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 29.9807 - acc: 1.0000 - val_loss: 30.1928 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.0648 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 30.0648 - acc: 1.0000 - val_loss: 30.2870 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 30.1577 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 30.1577 - acc: 1.0000 - val_loss: 30.3890 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.2602 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 30.2602 - acc: 1.0000 - val_loss: 30.4998 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.3711 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 30.3711 - acc: 1.0000 - val_loss: 30.6203 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.4944 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 30.4944 - acc: 1.0000 - val_loss: 30.7520 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.6271 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 30.6271 - acc: 1.0000 - val_loss: 30.8955 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 30.7689 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 30.7689 - acc: 1.0000 - val_loss: 31.0514 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 30.9275 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 30.9275 - acc: 1.0000 - val_loss: 31.2186 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 31.0985 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 31.0985 - acc: 1.0000 - val_loss: 31.3990 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 31.2753 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 31.2753 - acc: 1.0000 - val_loss: 31.5920 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 31.4651 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 31.4651 - acc: 1.0000 - val_loss: 31.7966 - val_acc: 1.0000\n",
      "===== Training Epoch 44 =====\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 31.6728 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 31.6728 - acc: 1.0000 - val_loss: 32.0126 - val_acc: 1.0000\n",
      "===== Training Epoch 45 =====\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 31.8894 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 31.8894 - acc: 1.0000 - val_loss: 32.2389 - val_acc: 1.0000\n",
      "===== Training Epoch 46 =====\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 32.1198 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 32.1198 - acc: 1.0000 - val_loss: 32.4746 - val_acc: 1.0000\n",
      "===== Training Epoch 47 =====\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 32.3508 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 32.3508 - acc: 1.0000 - val_loss: 32.7201 - val_acc: 1.0000\n",
      "===== Training Epoch 48 =====\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 32.6058 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 32.6058 - acc: 1.0000 - val_loss: 32.9737 - val_acc: 1.0000\n",
      "===== Training Epoch 49 =====\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 32.8503 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 32.8503 - acc: 1.0000 - val_loss: 33.2340 - val_acc: 1.0000\n",
      "===== Training Epoch 50 =====\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 33.1083 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 33.1083 - acc: 1.0000 - val_loss: 33.5001 - val_acc: 1.0000\n",
      "===== Training Epoch 51 =====\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 33.3710 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 33.3710 - acc: 1.0000 - val_loss: 33.7719 - val_acc: 1.0000\n",
      "===== Training Epoch 52 =====\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 33.6481 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 33.6481 - acc: 1.0000 - val_loss: 34.0491 - val_acc: 1.0000\n",
      "===== Training Epoch 53 =====\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 33.9201 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 33.9201 - acc: 1.0000 - val_loss: 34.3325 - val_acc: 1.0000\n",
      "Epoch 53: early stopping\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 292ms/steps: 32.6953 - acc: 0.45\n",
      "TRAIN\t\tacc: 0.580645\tacc_top5: 1.000000\tf1_macro: 0.428369\tprec_macro: 0.783333\trec_macro: 0.535714\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 32.6953 - acc: 0.4516 - val_loss: 31.9036 - val_acc: 0.5333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 39ms/stepss: 31.9028 - acc: 0.61\n",
      "TRAIN\t\tacc: 0.580645\tacc_top5: 1.000000\tf1_macro: 0.428369\tprec_macro: 0.783333\trec_macro: 0.535714\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 31.9028 - acc: 0.6129 - val_loss: 31.1230 - val_acc: 0.5333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 38ms/stepss: 31.1206 - acc: 0.64\n",
      "TRAIN\t\tacc: 0.580645\tacc_top5: 1.000000\tf1_macro: 0.428369\tprec_macro: 0.783333\trec_macro: 0.535714\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 31.1206 - acc: 0.6452 - val_loss: 30.3559 - val_acc: 0.5333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 29ms/stepss: 30.3523 - acc: 0.61\n",
      "TRAIN\t\tacc: 0.580645\tacc_top5: 1.000000\tf1_macro: 0.428369\tprec_macro: 0.783333\trec_macro: 0.535714\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 30.3523 - acc: 0.6129 - val_loss: 29.6044 - val_acc: 0.5333\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 29.6003 - acc: 0.58\n",
      "TRAIN\t\tacc: 0.580645\tacc_top5: 1.000000\tf1_macro: 0.428369\tprec_macro: 0.783333\trec_macro: 0.535714\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 29.6003 - acc: 0.5806 - val_loss: 28.8662 - val_acc: 0.5333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 28.8622 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.494565\tprec_macro: 0.793103\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 28.8622 - acc: 0.6774 - val_loss: 28.1416 - val_acc: 0.5333\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 33ms/stepss: 28.1303 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.645161\tacc_top5: 1.000000\tf1_macro: 0.554248\tprec_macro: 0.803571\trec_macro: 0.607143\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 28.1303 - acc: 0.7097 - val_loss: 27.4299 - val_acc: 0.5333\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 27.4164 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.741935\tacc_top5: 1.000000\tf1_macro: 0.704762\tprec_macro: 0.840000\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 27.4164 - acc: 0.8065 - val_loss: 26.7300 - val_acc: 0.6000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 31ms/stepss: 26.7164 - acc: 0.77\n",
      "TRAIN\t\tacc: 0.806452\tacc_top5: 1.000000\tf1_macro: 0.788636\tprec_macro: 0.869565\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 26.7164 - acc: 0.7742 - val_loss: 26.0437 - val_acc: 0.6000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 26.0224 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.899459\tprec_macro: 0.925000\trec_macro: 0.892857\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 26.0224 - acc: 0.8710 - val_loss: 25.3680 - val_acc: 0.6000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 25ms/stepss: 25.3532 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 25.3532 - acc: 0.8065 - val_loss: 24.7041 - val_acc: 0.6000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 24.6823 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 24.6823 - acc: 0.9032 - val_loss: 24.0507 - val_acc: 0.6667\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.0277 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 24.0277 - acc: 0.9355 - val_loss: 23.4099 - val_acc: 0.7333\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.3836 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 23.3836 - acc: 0.9677 - val_loss: 22.7811 - val_acc: 0.8667\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 22.7514 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 22.7514 - acc: 0.9677 - val_loss: 22.1633 - val_acc: 0.8667\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 22.1260 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 22.1260 - acc: 0.9677 - val_loss: 21.5590 - val_acc: 0.9333\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 21.5233 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 21.5233 - acc: 0.9677 - val_loss: 20.9608 - val_acc: 0.9333\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 20.9156 - acc: 1.00\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 20.9156 - acc: 1.0000 - val_loss: 20.3738 - val_acc: 0.9333\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 20.3267 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 20.3267 - acc: 1.0000 - val_loss: 19.7969 - val_acc: 0.9333\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 19.7467 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 19.7467 - acc: 1.0000 - val_loss: 19.2312 - val_acc: 0.9333\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 19.1736 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 19.1736 - acc: 1.0000 - val_loss: 18.6764 - val_acc: 0.9333\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.6161 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 18.6161 - acc: 1.0000 - val_loss: 18.1284 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.0645 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 18.0645 - acc: 1.0000 - val_loss: 17.5914 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 17.5268 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 17.5268 - acc: 1.0000 - val_loss: 17.0661 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.9986 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 16.9986 - acc: 1.0000 - val_loss: 16.5519 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.4821 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 16.4821 - acc: 1.0000 - val_loss: 16.0492 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.9786 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 15.9786 - acc: 1.0000 - val_loss: 15.5558 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 15.4860 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 15.4860 - acc: 1.0000 - val_loss: 15.0731 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.0020 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 15.0020 - acc: 1.0000 - val_loss: 14.6032 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.5318 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 14.5318 - acc: 1.0000 - val_loss: 14.1429 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.0765 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 14.0765 - acc: 1.0000 - val_loss: 13.6909 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.6228 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 13.6228 - acc: 1.0000 - val_loss: 13.2514 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 13.1882 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 13.1882 - acc: 1.0000 - val_loss: 12.8243 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.7639 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 12.7639 - acc: 1.0000 - val_loss: 12.4091 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.3521 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 12.3521 - acc: 1.0000 - val_loss: 12.0048 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.9522 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 11.9522 - acc: 1.0000 - val_loss: 11.6121 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 11.5604 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 11.5604 - acc: 1.0000 - val_loss: 11.2296 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.1830 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 11.1830 - acc: 1.0000 - val_loss: 10.8560 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.8127 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 10.8127 - acc: 1.0000 - val_loss: 10.4931 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.4527 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 10.4527 - acc: 1.0000 - val_loss: 10.1403 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.1021 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 10.1021 - acc: 1.0000 - val_loss: 9.7973 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 9.7633 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 9.7633 - acc: 1.0000 - val_loss: 9.4650 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 9.4313 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 9.4313 - acc: 1.0000 - val_loss: 9.1410 - val_acc: 1.0000\n",
      "===== Training Epoch 44 =====\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 9.1086 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 9.1086 - acc: 1.0000 - val_loss: 8.8252 - val_acc: 1.0000\n",
      "===== Training Epoch 45 =====\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 8.7952 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 8.7952 - acc: 1.0000 - val_loss: 8.5180 - val_acc: 1.0000\n",
      "===== Training Epoch 46 =====\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 8.4910 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 8.4910 - acc: 1.0000 - val_loss: 8.2200 - val_acc: 1.0000\n",
      "===== Training Epoch 47 =====\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 8.1938 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 8.1938 - acc: 1.0000 - val_loss: 7.9281 - val_acc: 1.0000\n",
      "===== Training Epoch 48 =====\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 7.9047 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 7.9047 - acc: 1.0000 - val_loss: 7.6439 - val_acc: 1.0000\n",
      "===== Training Epoch 49 =====\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 7.6200 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 7.6200 - acc: 1.0000 - val_loss: 7.3696 - val_acc: 1.0000\n",
      "===== Training Epoch 50 =====\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 7.3474 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 7.3474 - acc: 1.0000 - val_loss: 7.1080 - val_acc: 1.0000\n",
      "===== Training Epoch 51 =====\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 7.0869 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 7.0869 - acc: 1.0000 - val_loss: 6.8537 - val_acc: 1.0000\n",
      "===== Training Epoch 52 =====\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 6.8338 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 6.8338 - acc: 1.0000 - val_loss: 6.6049 - val_acc: 1.0000\n",
      "Epoch 52: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 244ms/steps: 28.3471 - acc: 0.61\n",
      "TRAIN\t\tacc: 0.741935\tacc_top5: 1.000000\tf1_macro: 0.704762\tprec_macro: 0.840000\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 2s 2s/step - loss: 28.3471 - acc: 0.6129 - val_loss: 27.7177 - val_acc: 0.6667\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 34ms/stepss: 27.7149 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 27.7149 - acc: 0.7097 - val_loss: 27.0967 - val_acc: 0.7333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 26ms/stepss: 27.0866 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.899459\tprec_macro: 0.925000\trec_macro: 0.892857\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 27.0866 - acc: 0.7097 - val_loss: 26.4868 - val_acc: 0.7333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 26.4607 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 26.4607 - acc: 0.8387 - val_loss: 25.8883 - val_acc: 0.7333\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 25.8470 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 25.8470 - acc: 0.9677 - val_loss: 25.2972 - val_acc: 0.8667\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 25.2484 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 25.2484 - acc: 0.9355 - val_loss: 24.8284 - val_acc: 0.9333\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 24.7680 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 24.7680 - acc: 0.9677 - val_loss: 24.4204 - val_acc: 0.9333\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 24.3451 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 24.3451 - acc: 0.9677 - val_loss: 24.0026 - val_acc: 1.0000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.9048 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 23.9048 - acc: 1.0000 - val_loss: 23.5775 - val_acc: 1.0000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 23.4932 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 23.4932 - acc: 1.0000 - val_loss: 23.1487 - val_acc: 1.0000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 23.0800 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 23.0800 - acc: 1.0000 - val_loss: 22.7799 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 22.6918 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 22.6918 - acc: 1.0000 - val_loss: 22.4562 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 22.3576 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 22.3576 - acc: 1.0000 - val_loss: 22.1721 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 22.0840 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 22.0840 - acc: 1.0000 - val_loss: 21.9250 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 21.8303 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 21.8303 - acc: 1.0000 - val_loss: 21.6671 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 21.5841 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 21.5841 - acc: 1.0000 - val_loss: 21.3991 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 21.3178 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 21.3178 - acc: 1.0000 - val_loss: 21.1735 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 21.0927 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 21.0927 - acc: 1.0000 - val_loss: 20.9387 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 20.8638 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 20.8638 - acc: 1.0000 - val_loss: 20.6946 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 20.6257 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 20.6257 - acc: 1.0000 - val_loss: 20.4423 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 20.3760 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 20.3760 - acc: 1.0000 - val_loss: 20.1824 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 20.1269 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 20.1269 - acc: 1.0000 - val_loss: 19.9173 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 19.8576 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 19.8576 - acc: 1.0000 - val_loss: 19.6471 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 19.5924 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 19.5924 - acc: 1.0000 - val_loss: 19.3723 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 19.3237 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 19.3237 - acc: 1.0000 - val_loss: 19.0943 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 19.0485 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 19.0485 - acc: 1.0000 - val_loss: 18.8147 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 18.7713 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 18.7713 - acc: 1.0000 - val_loss: 18.6108 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.5735 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 18.5735 - acc: 1.0000 - val_loss: 18.4029 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.3661 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 18.3661 - acc: 1.0000 - val_loss: 18.1897 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.1554 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 18.1554 - acc: 1.0000 - val_loss: 17.9713 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 17.9404 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 17.9404 - acc: 1.0000 - val_loss: 17.8172 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.7882 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 17.7882 - acc: 1.0000 - val_loss: 17.6562 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.6288 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 17.6288 - acc: 1.0000 - val_loss: 17.4873 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 17.4616 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 17.4616 - acc: 1.0000 - val_loss: 17.3775 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 17.3536 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 17.3536 - acc: 1.0000 - val_loss: 17.3185 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.2959 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 17.2959 - acc: 1.0000 - val_loss: 17.2475 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.2265 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 17.2265 - acc: 1.0000 - val_loss: 17.2227 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.2032 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 17.2032 - acc: 1.0000 - val_loss: 17.2381 - val_acc: 1.0000\n",
      "Epoch 38: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 268ms/steps: 32.6117 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 2s 2s/step - loss: 32.6117 - acc: 0.6774 - val_loss: 31.7888 - val_acc: 0.7333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 36ms/stepss: 31.7892 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 31.7892 - acc: 0.8710 - val_loss: 30.9658 - val_acc: 0.9333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 30.9561 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 30.9561 - acc: 0.9032 - val_loss: 30.1577 - val_acc: 1.0000\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 34ms/stepss: 30.1443 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 30.1443 - acc: 0.9677 - val_loss: 29.3638 - val_acc: 1.0000\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 34ms/stepss: 29.3335 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 29.3335 - acc: 1.0000 - val_loss: 28.5827 - val_acc: 1.0000\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 28ms/stepss: 28.5457 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 28.5457 - acc: 1.0000 - val_loss: 27.8151 - val_acc: 1.0000\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 27.7763 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 27.7763 - acc: 1.0000 - val_loss: 27.0614 - val_acc: 1.0000\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 26ms/stepss: 27.0238 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 27.0238 - acc: 0.9677 - val_loss: 26.3224 - val_acc: 1.0000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 26.2763 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 26.2763 - acc: 1.0000 - val_loss: 25.6005 - val_acc: 1.0000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 25.5663 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 25.5663 - acc: 1.0000 - val_loss: 24.8947 - val_acc: 1.0000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 24.8674 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 24.8674 - acc: 1.0000 - val_loss: 24.2068 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.1594 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 24.1594 - acc: 1.0000 - val_loss: 23.5363 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 23.5020 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 23.5020 - acc: 1.0000 - val_loss: 22.8839 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 22.8491 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 22.8491 - acc: 1.0000 - val_loss: 22.2475 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 22.2166 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 22.2166 - acc: 1.0000 - val_loss: 21.6260 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 21.6037 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 21.6037 - acc: 1.0000 - val_loss: 21.0208 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 21.0008 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 21.0008 - acc: 1.0000 - val_loss: 20.4279 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 20.4077 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 20.4077 - acc: 1.0000 - val_loss: 19.8512 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 19.8331 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 19.8331 - acc: 1.0000 - val_loss: 19.2869 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 19.2723 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 19.2723 - acc: 1.0000 - val_loss: 18.7349 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 18.7227 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 18.7227 - acc: 1.0000 - val_loss: 18.1968 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.1874 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 18.1874 - acc: 1.0000 - val_loss: 17.6684 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.6590 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 17.6590 - acc: 1.0000 - val_loss: 17.1534 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.1463 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 17.1463 - acc: 1.0000 - val_loss: 16.6505 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.6448 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 16.6448 - acc: 1.0000 - val_loss: 16.1588 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 16.1528 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 16.1528 - acc: 1.0000 - val_loss: 15.6804 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 15.6751 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 15.6751 - acc: 1.0000 - val_loss: 15.2111 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.2070 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 15.2070 - acc: 1.0000 - val_loss: 14.7508 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 14.7469 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 14.7469 - acc: 1.0000 - val_loss: 14.3014 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 14.2976 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 14.2976 - acc: 1.0000 - val_loss: 13.8640 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.8611 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 13.8611 - acc: 1.0000 - val_loss: 13.4373 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 13.4341 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 13.4341 - acc: 1.0000 - val_loss: 13.0203 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.0171 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 13.0171 - acc: 1.0000 - val_loss: 12.6122 - val_acc: 1.0000\n",
      "Epoch 33: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 266ms/steps: 28.4142 - acc: 0.61\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.901587\tprec_macro: 0.905983\trec_macro: 0.899160\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 3s 3s/step - loss: 28.4142 - acc: 0.6129 - val_loss: 28.3442 - val_acc: 0.8667\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 39ms/stepss: 28.3269 - acc: 0.83\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 28.3269 - acc: 0.8387 - val_loss: 28.2702 - val_acc: 0.8000\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 25ms/stepss: 28.2580 - acc: 0.83\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 28.2580 - acc: 0.8387 - val_loss: 28.2016 - val_acc: 0.8667\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 25ms/stepss: 28.1492 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 28.1492 - acc: 0.9677 - val_loss: 28.1393 - val_acc: 0.9333\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 28.0928 - acc: 0.93\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 28.0928 - acc: 0.9355 - val_loss: 28.0828 - val_acc: 0.9333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 34ms/stepss: 28.0072 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 28.0072 - acc: 1.0000 - val_loss: 28.0291 - val_acc: 1.0000\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 33ms/stepss: 27.9682 - acc: 0.93\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 27.9682 - acc: 0.9355 - val_loss: 27.9811 - val_acc: 1.0000\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 27.8948 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 27.8948 - acc: 0.9677 - val_loss: 27.9386 - val_acc: 1.0000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 31ms/stepss: 27.8347 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 27.8347 - acc: 1.0000 - val_loss: 27.9023 - val_acc: 1.0000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 27.8076 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 27.8076 - acc: 1.0000 - val_loss: 27.8706 - val_acc: 1.0000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 27.7909 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 27.7909 - acc: 1.0000 - val_loss: 27.8445 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 27.7328 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 27.7328 - acc: 1.0000 - val_loss: 27.8241 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 27.7290 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 27.7290 - acc: 1.0000 - val_loss: 27.8095 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 27.7169 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 27.7169 - acc: 1.0000 - val_loss: 27.7999 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 27.7094 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 27.7094 - acc: 1.0000 - val_loss: 27.7966 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 27.7240 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 27.7240 - acc: 1.0000 - val_loss: 27.7985 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.7268 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 27.7268 - acc: 1.0000 - val_loss: 27.8056 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.7362 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 27.7362 - acc: 1.0000 - val_loss: 27.8167 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.7501 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 27.7501 - acc: 1.0000 - val_loss: 27.8321 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.7732 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 27.7732 - acc: 1.0000 - val_loss: 27.8502 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.7982 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 27.7982 - acc: 1.0000 - val_loss: 27.8716 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 27.8223 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 27.8223 - acc: 1.0000 - val_loss: 27.8967 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.8489 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 27.8489 - acc: 1.0000 - val_loss: 27.9256 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.8822 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 27.8822 - acc: 1.0000 - val_loss: 27.9573 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.9193 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 27.9193 - acc: 1.0000 - val_loss: 27.9914 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.9545 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 27.9545 - acc: 1.0000 - val_loss: 28.0289 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 27.9934 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 27.9934 - acc: 1.0000 - val_loss: 28.0693 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 28.0367 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 28.0367 - acc: 1.0000 - val_loss: 28.1125 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.0809 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 28.0809 - acc: 1.0000 - val_loss: 28.1591 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.1290 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 28.1290 - acc: 1.0000 - val_loss: 28.2081 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.1789 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 28.1789 - acc: 1.0000 - val_loss: 28.2602 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.2321 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 28.2321 - acc: 1.0000 - val_loss: 28.3138 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.2869 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 28.2869 - acc: 1.0000 - val_loss: 28.3709 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.3444 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 28.3444 - acc: 1.0000 - val_loss: 28.4309 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.4050 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 28.4050 - acc: 1.0000 - val_loss: 28.4927 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 28.4671 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 28.4671 - acc: 1.0000 - val_loss: 28.5566 - val_acc: 1.0000\n",
      "Epoch 36: early stopping\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 292ms/steps: 32.5712 - acc: 0.48\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.899459\tprec_macro: 0.925000\trec_macro: 0.892857\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 2s 2s/step - loss: 32.5712 - acc: 0.4839 - val_loss: 31.7096 - val_acc: 0.8000\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 29ms/stepss: 31.7105 - acc: 0.77\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 31.7105 - acc: 0.7742 - val_loss: 30.8780 - val_acc: 0.9333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 32ms/stepss: 30.8566 - acc: 0.83\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 30.8566 - acc: 0.8387 - val_loss: 30.0641 - val_acc: 1.0000\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 36ms/stepss: 30.0260 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 30.0260 - acc: 1.0000 - val_loss: 29.2684 - val_acc: 1.0000\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 29.2259 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 29.2259 - acc: 1.0000 - val_loss: 28.4876 - val_acc: 1.0000\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 28.4647 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 28.4647 - acc: 1.0000 - val_loss: 27.7209 - val_acc: 1.0000\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 35ms/stepss: 27.6812 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 27.6812 - acc: 1.0000 - val_loss: 26.9682 - val_acc: 1.0000\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 34ms/stepss: 26.9199 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 26.9199 - acc: 1.0000 - val_loss: 26.2315 - val_acc: 1.0000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 26.1698 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 26.1698 - acc: 1.0000 - val_loss: 25.5120 - val_acc: 1.0000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 25.4622 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 25.4622 - acc: 1.0000 - val_loss: 24.8089 - val_acc: 1.0000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 24.7674 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 24.7674 - acc: 1.0000 - val_loss: 24.1248 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 24.0780 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 24.0780 - acc: 1.0000 - val_loss: 23.4583 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 23.4131 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 23.4131 - acc: 1.0000 - val_loss: 22.8106 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 22.7692 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 22.7692 - acc: 1.0000 - val_loss: 22.1812 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 22.1451 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 22.1451 - acc: 1.0000 - val_loss: 21.5683 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 21.5376 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 21.5376 - acc: 1.0000 - val_loss: 20.9714 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 20.9472 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 20.9472 - acc: 1.0000 - val_loss: 20.3857 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 20.3608 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 20.3608 - acc: 1.0000 - val_loss: 19.8150 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 19.7933 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 19.7933 - acc: 1.0000 - val_loss: 19.2553 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 19.2382 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 19.2382 - acc: 1.0000 - val_loss: 18.7071 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.6924 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 18.6924 - acc: 1.0000 - val_loss: 18.1723 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 18.1607 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 18.1607 - acc: 1.0000 - val_loss: 17.6478 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.6362 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 17.6362 - acc: 1.0000 - val_loss: 17.1374 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 17.1276 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 17.1276 - acc: 1.0000 - val_loss: 16.6389 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.6300 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 16.6300 - acc: 1.0000 - val_loss: 16.1520 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.1444 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 16.1444 - acc: 1.0000 - val_loss: 15.6765 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 15.6694 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 15.6694 - acc: 1.0000 - val_loss: 15.2103 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.2045 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 15.2045 - acc: 1.0000 - val_loss: 14.7528 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.7468 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 14.7468 - acc: 1.0000 - val_loss: 14.3043 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.2988 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 14.2988 - acc: 1.0000 - val_loss: 13.8671 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.8625 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 13.8625 - acc: 1.0000 - val_loss: 13.4389 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.4342 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 13.4342 - acc: 1.0000 - val_loss: 13.0216 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.0171 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 13.0171 - acc: 1.0000 - val_loss: 12.6156 - val_acc: 1.0000\n",
      "Epoch 33: early stopping\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 275ms/steps: 28.3872 - acc: 0.32\n",
      "TRAIN\t\tacc: 0.677419\tacc_top5: 1.000000\tf1_macro: 0.630952\tprec_macro: 0.736667\trec_macro: 0.649160\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 2s 2s/step - loss: 28.3872 - acc: 0.3226 - val_loss: 28.1478 - val_acc: 0.8000\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 41ms/stepss: 28.1472 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.709677\tacc_top5: 1.000000\tf1_macro: 0.658507\tprec_macro: 0.826923\trec_macro: 0.678571\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 28.1472 - acc: 0.7419 - val_loss: 27.9191 - val_acc: 0.7333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 27.9155 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.741935\tacc_top5: 1.000000\tf1_macro: 0.704762\tprec_macro: 0.840000\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 27.9155 - acc: 0.8065 - val_loss: 27.7018 - val_acc: 0.6667\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 27.6989 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.774194\tacc_top5: 1.000000\tf1_macro: 0.747967\tprec_macro: 0.854167\trec_macro: 0.750000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 27.6989 - acc: 0.7097 - val_loss: 27.4990 - val_acc: 0.6667\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 36ms/stepss: 27.4891 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 27.4891 - acc: 0.8065 - val_loss: 27.3063 - val_acc: 0.6667\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 28ms/stepss: 27.2981 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.899459\tprec_macro: 0.925000\trec_macro: 0.892857\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 27.2981 - acc: 0.8065 - val_loss: 27.1229 - val_acc: 0.6667\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 34ms/stepss: 27.1066 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 27.1066 - acc: 0.8710 - val_loss: 26.9478 - val_acc: 0.6667\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 26.9289 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 26.9289 - acc: 0.8710 - val_loss: 26.7819 - val_acc: 0.6667\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 26.7608 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 26.7608 - acc: 0.8710 - val_loss: 26.6228 - val_acc: 0.6667\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 26.5957 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 26.5957 - acc: 0.9677 - val_loss: 26.4707 - val_acc: 0.6667\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 26.4415 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 26.4415 - acc: 0.9355 - val_loss: 26.3248 - val_acc: 0.6667\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 26.2916 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 26.2916 - acc: 0.9355 - val_loss: 26.1854 - val_acc: 0.8000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 26.1445 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 26.1445 - acc: 0.9355 - val_loss: 26.0529 - val_acc: 0.8000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 26.0037 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 26.0037 - acc: 0.9677 - val_loss: 25.9284 - val_acc: 0.8000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 25.8689 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 25.8689 - acc: 0.9677 - val_loss: 25.8104 - val_acc: 0.8000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 25.7497 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 25.7497 - acc: 0.9677 - val_loss: 25.7030 - val_acc: 0.8667\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 25.6366 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 25.6366 - acc: 1.0000 - val_loss: 25.6010 - val_acc: 0.8667\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 25.5285 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 25.5285 - acc: 1.0000 - val_loss: 25.5061 - val_acc: 0.8667\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 25.4270 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 25.4270 - acc: 1.0000 - val_loss: 25.4193 - val_acc: 0.8667\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 25.3307 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 25.3307 - acc: 1.0000 - val_loss: 25.3374 - val_acc: 0.9333\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 25.2413 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 25.2413 - acc: 1.0000 - val_loss: 25.2603 - val_acc: 0.9333\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 25.1637 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 25.1637 - acc: 1.0000 - val_loss: 25.1897 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 25.0877 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 25.0877 - acc: 1.0000 - val_loss: 25.1260 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 25.0169 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 25.0169 - acc: 1.0000 - val_loss: 25.0647 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.9529 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 24.9529 - acc: 1.0000 - val_loss: 25.0078 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 24.8949 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 24.8949 - acc: 1.0000 - val_loss: 24.9560 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.8362 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 24.8362 - acc: 1.0000 - val_loss: 24.9101 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.7967 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 24.7967 - acc: 1.0000 - val_loss: 24.8699 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 24.7566 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 24.7566 - acc: 1.0000 - val_loss: 24.8361 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.7192 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 24.7192 - acc: 1.0000 - val_loss: 24.8061 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.6982 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 24.6982 - acc: 1.0000 - val_loss: 24.7805 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.6719 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 24.6719 - acc: 1.0000 - val_loss: 24.7565 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.6531 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 24.6531 - acc: 1.0000 - val_loss: 24.7409 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.6387 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 24.6387 - acc: 1.0000 - val_loss: 24.7328 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.6375 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 24.6375 - acc: 1.0000 - val_loss: 24.7310 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.6395 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 24.6395 - acc: 1.0000 - val_loss: 24.7336 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 24.6453 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 24.6453 - acc: 1.0000 - val_loss: 24.7418 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.6574 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 24.6574 - acc: 1.0000 - val_loss: 24.7573 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.6768 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 24.6768 - acc: 1.0000 - val_loss: 24.7799 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.7034 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 24.7034 - acc: 1.0000 - val_loss: 24.8130 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.7382 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 24.7382 - acc: 1.0000 - val_loss: 24.8495 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.7780 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 24.7780 - acc: 1.0000 - val_loss: 24.8890 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.8183 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 24.8183 - acc: 1.0000 - val_loss: 24.9362 - val_acc: 1.0000\n",
      "===== Training Epoch 44 =====\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.8671 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 24.8671 - acc: 1.0000 - val_loss: 24.9923 - val_acc: 1.0000\n",
      "===== Training Epoch 45 =====\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 24.9265 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 24.9265 - acc: 1.0000 - val_loss: 25.0552 - val_acc: 1.0000\n",
      "===== Training Epoch 46 =====\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 24.9910 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 24.9910 - acc: 1.0000 - val_loss: 25.1210 - val_acc: 1.0000\n",
      "===== Training Epoch 47 =====\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 25.0578 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 25.0578 - acc: 1.0000 - val_loss: 25.1883 - val_acc: 1.0000\n",
      "===== Training Epoch 48 =====\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 25.1266 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 25.1266 - acc: 1.0000 - val_loss: 25.2592 - val_acc: 1.0000\n",
      "===== Training Epoch 49 =====\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 25.1987 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 25.1987 - acc: 1.0000 - val_loss: 25.3367 - val_acc: 1.0000\n",
      "===== Training Epoch 50 =====\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 25.2772 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 25.2772 - acc: 1.0000 - val_loss: 25.4179 - val_acc: 1.0000\n",
      "===== Training Epoch 51 =====\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 25.3594 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 25.3594 - acc: 1.0000 - val_loss: 25.5015 - val_acc: 1.0000\n",
      "===== Training Epoch 52 =====\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 25.4442 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 25.4442 - acc: 1.0000 - val_loss: 25.5886 - val_acc: 1.0000\n",
      "Epoch 52: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 290ms/steps: 32.6258 - acc: 0.45\n",
      "TRAIN\t\tacc: 0.645161\tacc_top5: 1.000000\tf1_macro: 0.554248\tprec_macro: 0.803571\trec_macro: 0.607143\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 32.6258 - acc: 0.4516 - val_loss: 31.8323 - val_acc: 0.5333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 38ms/stepss: 31.8306 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.677419\tacc_top5: 1.000000\tf1_macro: 0.608586\tprec_macro: 0.814815\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 31.8306 - acc: 0.7419 - val_loss: 31.0486 - val_acc: 0.5333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 36ms/stepss: 31.0453 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.709677\tacc_top5: 1.000000\tf1_macro: 0.658507\tprec_macro: 0.826923\trec_macro: 0.678571\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 31.0453 - acc: 0.7419 - val_loss: 30.2781 - val_acc: 0.5333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 30.2732 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.806452\tacc_top5: 1.000000\tf1_macro: 0.788636\tprec_macro: 0.869565\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 30.2732 - acc: 0.7097 - val_loss: 29.5243 - val_acc: 0.6000\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 29.5177 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 29.5177 - acc: 0.7419 - val_loss: 28.7822 - val_acc: 0.6667\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 28.7723 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 28.7723 - acc: 0.8710 - val_loss: 28.0525 - val_acc: 0.6667\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 36ms/stepss: 28.0364 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.899459\tprec_macro: 0.925000\trec_macro: 0.892857\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 28.0364 - acc: 0.9032 - val_loss: 27.3364 - val_acc: 0.6667\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 27.3175 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 27.3175 - acc: 0.9355 - val_loss: 26.6350 - val_acc: 0.6667\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 26.6173 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 26.6173 - acc: 0.9677 - val_loss: 25.9470 - val_acc: 0.8667\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 25.9252 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 25.9252 - acc: 0.9355 - val_loss: 25.2696 - val_acc: 0.9333\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 25.2478 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 25.2478 - acc: 0.9677 - val_loss: 24.6048 - val_acc: 0.9333\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 24.5723 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 24.5723 - acc: 0.9677 - val_loss: 23.9504 - val_acc: 0.9333\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 23.9166 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 23.9166 - acc: 1.0000 - val_loss: 23.3076 - val_acc: 0.9333\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 23.2735 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 23.2735 - acc: 1.0000 - val_loss: 22.6741 - val_acc: 0.9333\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 22.6296 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 22.6296 - acc: 1.0000 - val_loss: 22.0505 - val_acc: 0.9333\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 22.0033 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 22.0033 - acc: 1.0000 - val_loss: 21.4394 - val_acc: 0.9333\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 21.3886 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 21.3886 - acc: 1.0000 - val_loss: 20.8370 - val_acc: 0.9333\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 20.7847 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 20.7847 - acc: 1.0000 - val_loss: 20.2469 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 20.1887 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 20.1887 - acc: 1.0000 - val_loss: 19.6670 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.6098 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 19.6098 - acc: 1.0000 - val_loss: 19.0969 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.0352 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 19.0352 - acc: 1.0000 - val_loss: 18.5399 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 18.4738 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 18.4738 - acc: 1.0000 - val_loss: 17.9921 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 17.9257 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 17.9257 - acc: 1.0000 - val_loss: 17.4574 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 17.3906 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 17.3906 - acc: 1.0000 - val_loss: 16.9336 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.8683 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 16.8683 - acc: 1.0000 - val_loss: 16.4212 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.3527 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 16.3527 - acc: 1.0000 - val_loss: 15.9219 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.8581 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 15.8581 - acc: 1.0000 - val_loss: 15.4321 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.3744 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 15.3744 - acc: 1.0000 - val_loss: 14.9541 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 14.8945 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 14.8945 - acc: 1.0000 - val_loss: 14.4898 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 14.4354 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 14.4354 - acc: 1.0000 - val_loss: 14.0374 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.9866 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 13.9866 - acc: 1.0000 - val_loss: 13.5954 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.5488 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 13.5488 - acc: 1.0000 - val_loss: 13.1663 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 13.1199 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 13.1199 - acc: 1.0000 - val_loss: 12.7478 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.7070 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 12.7070 - acc: 1.0000 - val_loss: 12.3391 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.3006 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 12.3006 - acc: 1.0000 - val_loss: 11.9422 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 11.9074 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 11.9074 - acc: 1.0000 - val_loss: 11.5581 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.5256 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 11.5256 - acc: 1.0000 - val_loss: 11.1832 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 11.1520 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 11.1520 - acc: 1.0000 - val_loss: 10.8168 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.7888 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 10.7888 - acc: 1.0000 - val_loss: 10.4616 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.4356 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 10.4356 - acc: 1.0000 - val_loss: 10.1170 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.0932 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 10.0932 - acc: 1.0000 - val_loss: 9.7815 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 9.7590 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 9.7590 - acc: 1.0000 - val_loss: 9.4537 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 9.4310 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 9.4310 - acc: 1.0000 - val_loss: 9.1345 - val_acc: 1.0000\n",
      "===== Training Epoch 44 =====\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 9.1143 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 9.1143 - acc: 1.0000 - val_loss: 8.8242 - val_acc: 1.0000\n",
      "===== Training Epoch 45 =====\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 8.8046 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 8.8046 - acc: 1.0000 - val_loss: 8.5213 - val_acc: 1.0000\n",
      "===== Training Epoch 46 =====\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 8.5040 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 8.5040 - acc: 1.0000 - val_loss: 8.2286 - val_acc: 1.0000\n",
      "===== Training Epoch 47 =====\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 8.2112 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 8.2112 - acc: 1.0000 - val_loss: 7.9425 - val_acc: 1.0000\n",
      "===== Training Epoch 48 =====\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 7.9273 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 7.9273 - acc: 1.0000 - val_loss: 7.6637 - val_acc: 1.0000\n",
      "Epoch 48: early stopping\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 248ms/steps: 28.4069 - acc: 0.25\n",
      "TRAIN\t\tacc: 0.645161\tacc_top5: 1.000000\tf1_macro: 0.554248\tprec_macro: 0.803571\trec_macro: 0.607143\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 2s 2s/step - loss: 28.4069 - acc: 0.2581 - val_loss: 27.7426 - val_acc: 0.6667\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 33ms/stepss: 27.7397 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 27.7397 - acc: 0.7419 - val_loss: 27.3023 - val_acc: 0.6667\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 35ms/stepss: 27.2809 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 27.2809 - acc: 0.9355 - val_loss: 26.9372 - val_acc: 0.7333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 30ms/stepss: 26.9024 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 26.9024 - acc: 0.9032 - val_loss: 26.6176 - val_acc: 0.8000\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 26ms/stepss: 26.5737 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 26.5737 - acc: 0.9677 - val_loss: 26.3296 - val_acc: 0.9333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 26.2671 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 26.2671 - acc: 0.9677 - val_loss: 26.0050 - val_acc: 0.9333\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 25.9387 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 25.9387 - acc: 0.9677 - val_loss: 25.6550 - val_acc: 1.0000\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 25.5826 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 25.5826 - acc: 1.0000 - val_loss: 25.3517 - val_acc: 1.0000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 25.2606 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 25.2606 - acc: 1.0000 - val_loss: 25.0850 - val_acc: 1.0000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 25.0101 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 25.0101 - acc: 1.0000 - val_loss: 24.8501 - val_acc: 1.0000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 24.7732 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 24.7732 - acc: 1.0000 - val_loss: 24.6427 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 24.5452 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 24.5452 - acc: 1.0000 - val_loss: 24.4583 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 24.3641 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 24.3641 - acc: 1.0000 - val_loss: 24.2974 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 24.2123 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 24.2123 - acc: 1.0000 - val_loss: 24.1560 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 24.0754 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 24.0754 - acc: 1.0000 - val_loss: 24.0323 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.9574 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 23.9574 - acc: 1.0000 - val_loss: 23.9245 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 23.8530 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 23.8530 - acc: 1.0000 - val_loss: 23.8325 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.7660 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 23.7660 - acc: 1.0000 - val_loss: 23.7546 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.6918 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 23.6918 - acc: 1.0000 - val_loss: 23.6887 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.6329 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 23.6329 - acc: 1.0000 - val_loss: 23.6341 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.5824 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 23.5824 - acc: 1.0000 - val_loss: 23.5916 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.5460 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 23.5460 - acc: 1.0000 - val_loss: 23.5579 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.5134 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 23.5134 - acc: 1.0000 - val_loss: 23.5291 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.4880 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 23.4880 - acc: 1.0000 - val_loss: 23.5081 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 23.4720 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 23.4720 - acc: 1.0000 - val_loss: 23.4951 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.4613 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 23.4613 - acc: 1.0000 - val_loss: 23.4892 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 23.4576 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 23.4576 - acc: 1.0000 - val_loss: 23.4923 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.4641 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 23.4641 - acc: 1.0000 - val_loss: 23.5026 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 23.4757 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 23.4757 - acc: 1.0000 - val_loss: 23.5180 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.4928 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 23.4928 - acc: 1.0000 - val_loss: 23.5406 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.5171 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 23.5171 - acc: 1.0000 - val_loss: 23.5692 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.5468 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 23.5468 - acc: 1.0000 - val_loss: 23.6037 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.5826 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 23.5826 - acc: 1.0000 - val_loss: 23.6445 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.6250 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 23.6250 - acc: 1.0000 - val_loss: 23.6890 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.6704 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 23.6704 - acc: 1.0000 - val_loss: 23.7395 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.7218 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 23.7218 - acc: 1.0000 - val_loss: 23.7963 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 23.7796 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 23.7796 - acc: 1.0000 - val_loss: 23.8598 - val_acc: 1.0000\n",
      "Epoch 37: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 272ms/steps: 32.6683 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.709677\tacc_top5: 1.000000\tf1_macro: 0.658507\tprec_macro: 0.826923\trec_macro: 0.678571\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 32.6683 - acc: 0.7097 - val_loss: 31.8243 - val_acc: 0.5333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 31.8294 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 31.8294 - acc: 0.7419 - val_loss: 30.9888 - val_acc: 0.8000\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 35ms/stepss: 30.9829 - acc: 0.80\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 30.9829 - acc: 0.8065 - val_loss: 30.1669 - val_acc: 0.9333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 33ms/stepss: 30.1477 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 30.1477 - acc: 0.9355 - val_loss: 29.3602 - val_acc: 0.9333\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 29.3368 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 29.3368 - acc: 0.9677 - val_loss: 28.5673 - val_acc: 1.0000\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 46ms/stepss: 28.5474 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 28.5474 - acc: 0.9677 - val_loss: 27.7905 - val_acc: 1.0000\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 33ms/stepss: 27.7626 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 27.7626 - acc: 0.9677 - val_loss: 27.0304 - val_acc: 1.0000\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 26.9986 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 26.9986 - acc: 0.9677 - val_loss: 26.2885 - val_acc: 1.0000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 26.2534 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 26.2534 - acc: 1.0000 - val_loss: 25.5671 - val_acc: 1.0000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 25.5418 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 25.5418 - acc: 1.0000 - val_loss: 24.8647 - val_acc: 1.0000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 25ms/stepss: 24.8452 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 24.8452 - acc: 1.0000 - val_loss: 24.1836 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 24.1613 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 24.1613 - acc: 1.0000 - val_loss: 23.5196 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 23.5037 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 23.5037 - acc: 1.0000 - val_loss: 22.8760 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 22.8611 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 22.8611 - acc: 1.0000 - val_loss: 22.2490 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 22.2336 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 22.2336 - acc: 1.0000 - val_loss: 21.6374 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 21.6288 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 21.6288 - acc: 1.0000 - val_loss: 21.0409 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 21.0293 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 21.0293 - acc: 1.0000 - val_loss: 20.4556 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 20.4464 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 20.4464 - acc: 1.0000 - val_loss: 19.8838 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 19.8763 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 19.8763 - acc: 1.0000 - val_loss: 19.3226 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.3170 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 19.3170 - acc: 1.0000 - val_loss: 18.7729 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 18.7681 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 18.7681 - acc: 1.0000 - val_loss: 18.2370 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 18.2332 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 18.2332 - acc: 1.0000 - val_loss: 17.7105 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.7063 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 17.7063 - acc: 1.0000 - val_loss: 17.1965 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.1932 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 17.1932 - acc: 1.0000 - val_loss: 16.6935 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 16.6910 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 16.6910 - acc: 1.0000 - val_loss: 16.2007 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.1983 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 16.1983 - acc: 1.0000 - val_loss: 15.7208 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 15.7183 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 15.7183 - acc: 1.0000 - val_loss: 15.2510 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 15.2489 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 15.2489 - acc: 1.0000 - val_loss: 14.7905 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 14.7887 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 14.7887 - acc: 1.0000 - val_loss: 14.3416 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.3398 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 14.3398 - acc: 1.0000 - val_loss: 13.9025 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.9010 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 13.9010 - acc: 1.0000 - val_loss: 13.4733 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.4718 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 13.4718 - acc: 1.0000 - val_loss: 13.0540 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.0525 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 13.0525 - acc: 1.0000 - val_loss: 12.6440 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.6427 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 12.6427 - acc: 1.0000 - val_loss: 12.2431 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.2418 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 12.2418 - acc: 1.0000 - val_loss: 11.8518 - val_acc: 1.0000\n",
      "Epoch 35: early stopping\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "[MARC:] Creating a model to test set\n",
      "[MARC:] Evaluation Config - 100-add-gru\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2931acbb7b8641de8e1f6cb8a0864f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Testing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 263ms/steps: 28.3711 - acc: 0.45\n",
      "TRAIN\t\tacc: 0.612903\tacc_top5: 1.000000\tf1_macro: 0.577273\tprec_macro: 0.616848\trec_macro: 0.590336\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.497608\tprec_macro: 0.522727\trec_macro: 0.517857\n",
      "1/1 [==============================] - 2s 2s/step - loss: 28.3711 - acc: 0.4516 - val_loss: 27.7501 - val_acc: 0.5333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 38ms/stepss: 27.7455 - acc: 0.61\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.550000\tprec_macro: 0.625000\trec_macro: 0.580357\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 27.7455 - acc: 0.6129 - val_loss: 27.1424 - val_acc: 0.6000\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 40ms/stepss: 27.1186 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.903226\tacc_top5: 1.000000\tf1_macro: 0.899459\tprec_macro: 0.925000\trec_macro: 0.892857\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "TEST\t\tacc: 0.666667\tacc_top5: 1.000000\tf1_macro: 0.603175\tprec_macro: 0.807692\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 27.1186 - acc: 0.8387 - val_loss: 26.5439 - val_acc: 0.6667\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 29ms/stepss: 26.5202 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 26.5202 - acc: 0.8710 - val_loss: 25.9572 - val_acc: 0.7333\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 25ms/stepss: 25.9226 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 25.9226 - acc: 0.8710 - val_loss: 25.3776 - val_acc: 0.7333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 25.3283 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 25.3283 - acc: 0.9032 - val_loss: 24.8064 - val_acc: 0.8000\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 24.7549 - acc: 0.90\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 24.7549 - acc: 0.9032 - val_loss: 24.2441 - val_acc: 0.8667\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 24.1883 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 24.1883 - acc: 0.9677 - val_loss: 23.6897 - val_acc: 0.9333\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 23ms/stepss: 23.6296 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 23.6296 - acc: 0.9677 - val_loss: 23.1447 - val_acc: 0.9333\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 23.0541 - acc: 0.93\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 23.0541 - acc: 0.9355 - val_loss: 22.6077 - val_acc: 0.9333\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 22.5174 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 22.5174 - acc: 0.9677 - val_loss: 22.0783 - val_acc: 0.9333\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 21.9953 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 21.9953 - acc: 0.9677 - val_loss: 21.5566 - val_acc: 0.9333\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 21.4574 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 21.4574 - acc: 1.0000 - val_loss: 21.0446 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 20.9347 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 20.9347 - acc: 1.0000 - val_loss: 20.5388 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 20.4223 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 20.4223 - acc: 1.0000 - val_loss: 20.0421 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.9454 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 19.9454 - acc: 1.0000 - val_loss: 19.5545 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 19.4402 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 19.4402 - acc: 1.0000 - val_loss: 19.0743 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.9671 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 18.9671 - acc: 1.0000 - val_loss: 18.6046 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 18.4860 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 18.4860 - acc: 1.0000 - val_loss: 18.1432 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 18.0307 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 18.0307 - acc: 1.0000 - val_loss: 17.6921 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 17.5761 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 17.5761 - acc: 1.0000 - val_loss: 17.2512 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.1460 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 17.1460 - acc: 1.0000 - val_loss: 16.8170 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.7155 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 16.7155 - acc: 1.0000 - val_loss: 16.3930 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 16.2920 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 16.2920 - acc: 1.0000 - val_loss: 15.9777 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 15.8898 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 15.8898 - acc: 1.0000 - val_loss: 15.5712 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 15.4854 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 15.4854 - acc: 1.0000 - val_loss: 15.1752 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 15.0913 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 15.0913 - acc: 1.0000 - val_loss: 14.7860 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.7108 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 14.7108 - acc: 1.0000 - val_loss: 14.4027 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.3304 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 14.3304 - acc: 1.0000 - val_loss: 14.0270 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 13.9573 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 13.9573 - acc: 1.0000 - val_loss: 13.6588 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.5933 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 13.5933 - acc: 1.0000 - val_loss: 13.2977 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.2381 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 13.2381 - acc: 1.0000 - val_loss: 12.9456 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.8878 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 12.8878 - acc: 1.0000 - val_loss: 12.6024 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.5468 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 12.5468 - acc: 1.0000 - val_loss: 12.2660 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 12.2141 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 12.2141 - acc: 1.0000 - val_loss: 11.9368 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 11.8873 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 11.8873 - acc: 1.0000 - val_loss: 11.6144 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.5665 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 11.5665 - acc: 1.0000 - val_loss: 11.2966 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.2515 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 11.2515 - acc: 1.0000 - val_loss: 10.9850 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.9421 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 10.9421 - acc: 1.0000 - val_loss: 10.6814 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.6387 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 10.6387 - acc: 1.0000 - val_loss: 10.3848 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.3439 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 10.3439 - acc: 1.0000 - val_loss: 10.0949 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.0562 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 10.0562 - acc: 1.0000 - val_loss: 9.8107 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 9.7722 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 9.7722 - acc: 1.0000 - val_loss: 9.5312 - val_acc: 1.0000\n",
      "Epoch 43: early stopping\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[MARC:] Processing time: 191168.212 milliseconds. Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>add</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>add</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>average</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>average</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>add</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>add</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>average</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>average</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>add</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>add</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>average</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.932127</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>100</td>\n",
       "      <td>average</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "1   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "16  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "15  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "14  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "13  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "12  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "11  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "10  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "9   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "8   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "7   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "6   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "5   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "4   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "3   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "17  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "2   0.933333            1.0           0.928571         0.932127      0.944444   \n",
       "\n",
       "    f1_macro   p0           p1    p2  \n",
       "0   1.000000  100          add   gru  \n",
       "1   1.000000  100          add  lstm  \n",
       "16  1.000000  300  concatenate   gru  \n",
       "15  1.000000  300      average  lstm  \n",
       "14  1.000000  300      average   gru  \n",
       "13  1.000000  300          add  lstm  \n",
       "12  1.000000  300          add   gru  \n",
       "11  1.000000  200  concatenate  lstm  \n",
       "10  1.000000  200  concatenate   gru  \n",
       "9   1.000000  200      average  lstm  \n",
       "8   1.000000  200      average   gru  \n",
       "7   1.000000  200          add  lstm  \n",
       "6   1.000000  200          add   gru  \n",
       "5   1.000000  100  concatenate  lstm  \n",
       "4   1.000000  100  concatenate   gru  \n",
       "3   1.000000  100      average  lstm  \n",
       "17  1.000000  300  concatenate  lstm  \n",
       "2   0.928571  100      average   gru  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matclassification.methods import MARC\n",
    "\n",
    "model = MARC()\n",
    "model.prepare_input(train, test)\n",
    "model.train()\n",
    "model.test()\n",
    "## We can visualize the training report (the same on most models):\n",
    "model.report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you encounter problems with input shape for MARC,\n",
    "On testing in Mac M1, tensorflow works on numpy version 1.23.5, and tensorflow-macos: 2.12 | tensorflow-metal: 0.8.0\n",
    "    \n",
    "    pip install tensorflow-macos==2.12 tensorflow-metal==0.8.0 numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>add</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>add</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>average</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>average</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>add</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>add</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>average</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>average</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>add</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>add</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>average</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>concatenate</td>\n",
       "      <td>lstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.932127</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>100</td>\n",
       "      <td>average</td>\n",
       "      <td>gru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "1   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "16  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "15  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "14  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "13  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "12  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "11  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "10  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "9   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "8   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "7   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "6   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "5   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "4   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "3   1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "17  1.000000            1.0           1.000000         1.000000      1.000000   \n",
       "2   0.933333            1.0           0.928571         0.932127      0.944444   \n",
       "\n",
       "    f1_macro   p0           p1    p2  \n",
       "0   1.000000  100          add   gru  \n",
       "1   1.000000  100          add  lstm  \n",
       "16  1.000000  300  concatenate   gru  \n",
       "15  1.000000  300      average  lstm  \n",
       "14  1.000000  300      average   gru  \n",
       "13  1.000000  300          add  lstm  \n",
       "12  1.000000  300          add   gru  \n",
       "11  1.000000  200  concatenate  lstm  \n",
       "10  1.000000  200  concatenate   gru  \n",
       "9   1.000000  200      average  lstm  \n",
       "8   1.000000  200      average   gru  \n",
       "7   1.000000  200          add  lstm  \n",
       "6   1.000000  200          add   gru  \n",
       "5   1.000000  100  concatenate  lstm  \n",
       "4   1.000000  100  concatenate   gru  \n",
       "3   1.000000  100      average  lstm  \n",
       "17  1.000000  300  concatenate  lstm  \n",
       "2   0.928571  100      average   gru  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>clstime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>191167.075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0       1.0            1.0                1.0              1.0           1.0   \n",
       "\n",
       "   f1_macro     clstime  \n",
       "0       1.0  191167.075  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and this is the model available metrics:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x165af7f70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Confusion Matrix:\n",
    "model.cm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model by default test all hyperparameter configurations, to check the selected configuration (check teh report for higher accuracy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 'add', 'gru')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, you can repeat the classification in a number of rounds with increasing random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MARC:] Creating a model to test set\n",
      "[MARC:] Evaluation Config - 100-add-gru\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7e10c0965a4c418ed16202956f20e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Testing:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 261ms/steps: 28.3537 - acc: 0.58\n",
      "TRAIN\t\tacc: 0.741935\tacc_top5: 1.000000\tf1_macro: 0.704762\tprec_macro: 0.840000\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 2s 2s/step - loss: 28.3537 - acc: 0.5806 - val_loss: 27.7336 - val_acc: 0.7333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 40ms/stepss: 27.7506 - acc: 0.61\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 27.7506 - acc: 0.6129 - val_loss: 27.1311 - val_acc: 0.7333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 38ms/stepss: 27.1270 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 27.1270 - acc: 0.7419 - val_loss: 26.5381 - val_acc: 0.7333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 35ms/stepss: 26.5270 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 26.5270 - acc: 0.8710 - val_loss: 25.9566 - val_acc: 0.7333\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 25ms/stepss: 25.9478 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 25.9478 - acc: 0.8710 - val_loss: 25.3819 - val_acc: 0.7333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 39ms/stepss: 25.3709 - acc: 0.93\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 25.3709 - acc: 0.9355 - val_loss: 24.8158 - val_acc: 0.7333\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 28ms/stepss: 24.8026 - acc: 0.93\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 24.8026 - acc: 0.9355 - val_loss: 24.2578 - val_acc: 0.7333\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 24.2307 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 24.2307 - acc: 0.9677 - val_loss: 23.7076 - val_acc: 0.8000\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 23.6723 - acc: 0.93\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 23.6723 - acc: 0.9355 - val_loss: 23.1658 - val_acc: 0.8667\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 23.1244 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 23.1244 - acc: 1.0000 - val_loss: 22.6312 - val_acc: 0.9333\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 22.5878 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 22.5878 - acc: 1.0000 - val_loss: 22.1039 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 25ms/stepss: 22.0510 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 22.0510 - acc: 0.9677 - val_loss: 21.5839 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 21.5158 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 21.5158 - acc: 1.0000 - val_loss: 21.0731 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 20.9904 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 20.9904 - acc: 1.0000 - val_loss: 20.5703 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 20.5006 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 20.5006 - acc: 1.0000 - val_loss: 20.0757 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 19.9962 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 19.9962 - acc: 1.0000 - val_loss: 19.5900 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 19.5041 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 19.5041 - acc: 1.0000 - val_loss: 19.1109 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 19.0224 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 19.0224 - acc: 1.0000 - val_loss: 18.6411 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.5487 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 18.5487 - acc: 1.0000 - val_loss: 18.1798 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.0863 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 18.0863 - acc: 1.0000 - val_loss: 17.7277 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.6363 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 17.6363 - acc: 1.0000 - val_loss: 17.2844 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.1967 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 17.1967 - acc: 1.0000 - val_loss: 16.8472 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 16.7511 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 16.7511 - acc: 1.0000 - val_loss: 16.4201 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.3286 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 16.3286 - acc: 1.0000 - val_loss: 16.0006 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 15.9198 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 15.9198 - acc: 1.0000 - val_loss: 15.5894 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 15.5085 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 15.5085 - acc: 1.0000 - val_loss: 15.1877 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.1044 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 15.1044 - acc: 1.0000 - val_loss: 14.7954 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 14.7179 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 14.7179 - acc: 1.0000 - val_loss: 14.4103 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 14.3409 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 14.3409 - acc: 1.0000 - val_loss: 14.0341 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.9679 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 13.9679 - acc: 1.0000 - val_loss: 13.6658 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.6017 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 13.6017 - acc: 1.0000 - val_loss: 13.3034 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.2405 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 13.2405 - acc: 1.0000 - val_loss: 12.9496 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 12.8924 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 12.8924 - acc: 1.0000 - val_loss: 12.6030 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.5459 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 12.5459 - acc: 1.0000 - val_loss: 12.2635 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 12.2115 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 12.2115 - acc: 1.0000 - val_loss: 11.9315 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 11.8803 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 11.8803 - acc: 1.0000 - val_loss: 11.6073 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.5596 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 11.5596 - acc: 1.0000 - val_loss: 11.2891 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.2445 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 11.2445 - acc: 1.0000 - val_loss: 10.9770 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.9339 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 10.9339 - acc: 1.0000 - val_loss: 10.6729 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.6321 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 10.6321 - acc: 1.0000 - val_loss: 10.3762 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.3369 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 10.3369 - acc: 1.0000 - val_loss: 10.0870 - val_acc: 1.0000\n",
      "Epoch 41: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 290ms/steps: 28.3651 - acc: 0.48\n",
      "TRAIN\t\tacc: 0.677419\tacc_top5: 1.000000\tf1_macro: 0.608586\tprec_macro: 0.814815\trec_macro: 0.642857\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 28.3651 - acc: 0.4839 - val_loss: 27.7496 - val_acc: 0.5333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 42ms/stepss: 27.7558 - acc: 0.58\n",
      "TRAIN\t\tacc: 0.806452\tacc_top5: 1.000000\tf1_macro: 0.788636\tprec_macro: 0.869565\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "TEST\t\tacc: 0.533333\tacc_top5: 1.000000\tf1_macro: 0.347826\tprec_macro: 0.766667\trec_macro: 0.500000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 27.7558 - acc: 0.5806 - val_loss: 27.1431 - val_acc: 0.5333\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 33ms/stepss: 27.1394 - acc: 0.77\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.488636\tprec_macro: 0.785714\trec_macro: 0.571429\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 27.1394 - acc: 0.7742 - val_loss: 26.5466 - val_acc: 0.6000\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 37ms/stepss: 26.5429 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 26.5429 - acc: 0.8387 - val_loss: 25.9616 - val_acc: 0.7333\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 35ms/stepss: 25.9483 - acc: 0.83\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 25.9483 - acc: 0.8387 - val_loss: 25.3845 - val_acc: 0.7333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 25.3620 - acc: 0.93\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 25.3620 - acc: 0.9355 - val_loss: 24.8163 - val_acc: 0.7333\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 24.7666 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 24.7666 - acc: 0.9677 - val_loss: 24.2579 - val_acc: 0.8000\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 24.2016 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 24.2016 - acc: 1.0000 - val_loss: 23.7070 - val_acc: 0.8667\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 23.6444 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 23.6444 - acc: 0.9677 - val_loss: 23.1647 - val_acc: 0.9333\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.1110 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 23.1110 - acc: 1.0000 - val_loss: 22.6295 - val_acc: 0.9333\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 22.5771 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 22.5771 - acc: 0.9677 - val_loss: 22.1017 - val_acc: 0.9333\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 22.0228 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 22.0228 - acc: 1.0000 - val_loss: 21.5817 - val_acc: 0.9333\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 21.5043 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 21.5043 - acc: 1.0000 - val_loss: 21.0703 - val_acc: 0.9333\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 20.9851 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 20.9851 - acc: 1.0000 - val_loss: 20.5674 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 20.4816 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 20.4816 - acc: 1.0000 - val_loss: 20.0729 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 19.9616 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 19.9616 - acc: 1.0000 - val_loss: 19.5860 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 19.4917 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 19.4917 - acc: 1.0000 - val_loss: 19.1048 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 18.9961 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 18.9961 - acc: 1.0000 - val_loss: 18.6337 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 18.5190 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 18.5190 - acc: 1.0000 - val_loss: 18.1710 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 18.0581 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 18.0581 - acc: 1.0000 - val_loss: 17.7175 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.6108 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 17.6108 - acc: 1.0000 - val_loss: 17.2738 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 26ms/stepss: 17.1738 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 17.1738 - acc: 1.0000 - val_loss: 16.8371 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 16.7312 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 16.7312 - acc: 1.0000 - val_loss: 16.4099 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.3065 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 16.3065 - acc: 1.0000 - val_loss: 15.9901 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 15.8954 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 15.8954 - acc: 1.0000 - val_loss: 15.5792 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 15.4944 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 15.4944 - acc: 1.0000 - val_loss: 15.1778 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 15.0908 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 15.0908 - acc: 1.0000 - val_loss: 14.7853 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 14.7043 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 14.7043 - acc: 1.0000 - val_loss: 14.4008 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 14.3211 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 14.3211 - acc: 1.0000 - val_loss: 14.0259 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.9533 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 13.9533 - acc: 1.0000 - val_loss: 13.6588 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.5908 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 13.5908 - acc: 1.0000 - val_loss: 13.2988 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.2343 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 13.2343 - acc: 1.0000 - val_loss: 12.9473 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.8852 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 12.8852 - acc: 1.0000 - val_loss: 12.6031 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 12.5439 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 12.5439 - acc: 1.0000 - val_loss: 12.2664 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.2097 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 12.2097 - acc: 1.0000 - val_loss: 11.9370 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.8829 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 11.8829 - acc: 1.0000 - val_loss: 11.6143 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.5647 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 11.5647 - acc: 1.0000 - val_loss: 11.2973 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 11.2477 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 11.2477 - acc: 1.0000 - val_loss: 10.9847 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.9378 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 10.9378 - acc: 1.0000 - val_loss: 10.6793 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.6339 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 10.6339 - acc: 1.0000 - val_loss: 10.3798 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 10.3357 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 10.3357 - acc: 1.0000 - val_loss: 10.0884 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.0466 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 10.0466 - acc: 1.0000 - val_loss: 9.8038 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 9.7623 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 9.7623 - acc: 1.0000 - val_loss: 9.5259 - val_acc: 1.0000\n",
      "===== Training Epoch 44 =====\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 9.4859 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 9.4859 - acc: 1.0000 - val_loss: 9.2554 - val_acc: 1.0000\n",
      "Epoch 44: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 320ms/steps: 28.3495 - acc: 0.67\n",
      "TRAIN\t\tacc: 0.838710\tacc_top5: 1.000000\tf1_macro: 0.838036\tprec_macro: 0.837500\trec_macro: 0.840336\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.598214\tprec_macro: 0.598214\trec_macro: 0.598214\n",
      "1/1 [==============================] - 3s 3s/step - loss: 28.3495 - acc: 0.6774 - val_loss: 27.7728 - val_acc: 0.6000\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 37ms/stepss: 27.7304 - acc: 0.74\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967607\tprec_macro: 0.966667\trec_macro: 0.970588\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "TEST\t\tacc: 0.600000\tacc_top5: 1.000000\tf1_macro: 0.598214\tprec_macro: 0.598214\trec_macro: 0.598214\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 27.7304 - acc: 0.7419 - val_loss: 27.1690 - val_acc: 0.6000\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 52ms/stepss: 27.1603 - acc: 0.87\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967607\tprec_macro: 0.966667\trec_macro: 0.970588\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "TEST\t\tacc: 0.800000\tacc_top5: 1.000000\tf1_macro: 0.784689\tprec_macro: 0.863636\trec_macro: 0.785714\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 27.1603 - acc: 0.8710 - val_loss: 26.5748 - val_acc: 0.8000\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 31ms/stepss: 26.5388 - acc: 0.90\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 26.5388 - acc: 0.9032 - val_loss: 25.9909 - val_acc: 0.8667\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 41ms/stepss: 25.9576 - acc: 0.87\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 25.9576 - acc: 0.8710 - val_loss: 25.4145 - val_acc: 0.8667\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 25.3737 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 25.3737 - acc: 0.9677 - val_loss: 24.8469 - val_acc: 0.8667\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 28ms/stepss: 24.7810 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 24.7810 - acc: 0.9677 - val_loss: 24.2878 - val_acc: 0.9333\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 24.2075 - acc: 0.96\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 24.2075 - acc: 0.9677 - val_loss: 23.7371 - val_acc: 0.9333\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 23.6602 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 23.6602 - acc: 1.0000 - val_loss: 23.1945 - val_acc: 0.9333\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 46ms/stepss: 23.0936 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 23.0936 - acc: 1.0000 - val_loss: 22.6592 - val_acc: 0.9333\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 22.5621 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 22.5621 - acc: 1.0000 - val_loss: 22.1309 - val_acc: 0.9333\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 25ms/stepss: 22.0052 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 22.0052 - acc: 1.0000 - val_loss: 21.6088 - val_acc: 0.9333\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 21.4996 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 21.4996 - acc: 1.0000 - val_loss: 21.0952 - val_acc: 0.9333\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 20.9845 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 20.9845 - acc: 1.0000 - val_loss: 20.5895 - val_acc: 0.9333\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 20.4515 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 20.4515 - acc: 1.0000 - val_loss: 20.0919 - val_acc: 0.9333\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.9721 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 19.9721 - acc: 1.0000 - val_loss: 19.6033 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.4755 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 19.4755 - acc: 1.0000 - val_loss: 19.1209 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 26ms/stepss: 18.9756 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 18.9756 - acc: 1.0000 - val_loss: 18.6483 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 18.5126 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 18.5126 - acc: 1.0000 - val_loss: 18.1839 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 29ms/stepss: 18.0501 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 18.0501 - acc: 1.0000 - val_loss: 17.7271 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 17.5993 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 17.5993 - acc: 1.0000 - val_loss: 17.2796 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.1537 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 17.1537 - acc: 1.0000 - val_loss: 16.8380 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 16.7149 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 16.7149 - acc: 1.0000 - val_loss: 16.4066 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 16.2936 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 16.2936 - acc: 1.0000 - val_loss: 15.9845 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 15.8832 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 15.8832 - acc: 1.0000 - val_loss: 15.5706 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 15.4701 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 15.4701 - acc: 1.0000 - val_loss: 15.1672 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 15.0689 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 15.0689 - acc: 1.0000 - val_loss: 14.7717 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 14.6819 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 14.6819 - acc: 1.0000 - val_loss: 14.3846 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 17ms/stepss: 14.2983 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 14.2983 - acc: 1.0000 - val_loss: 14.0062 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.9280 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 13.9280 - acc: 1.0000 - val_loss: 13.6363 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 13.5625 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 13.5625 - acc: 1.0000 - val_loss: 13.2741 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.2031 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 13.2031 - acc: 1.0000 - val_loss: 12.9225 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.8565 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 12.8565 - acc: 1.0000 - val_loss: 12.5782 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.5154 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 12.5154 - acc: 1.0000 - val_loss: 12.2394 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.1804 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 12.1804 - acc: 1.0000 - val_loss: 11.9071 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.8506 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 11.8506 - acc: 1.0000 - val_loss: 11.5829 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.5292 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 11.5292 - acc: 1.0000 - val_loss: 11.2654 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.2141 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 11.2141 - acc: 1.0000 - val_loss: 10.9524 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.9034 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 10.9034 - acc: 1.0000 - val_loss: 10.6467 - val_acc: 1.0000\n",
      "===== Training Epoch 40 =====\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.5999 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 10.5999 - acc: 1.0000 - val_loss: 10.3485 - val_acc: 1.0000\n",
      "===== Training Epoch 41 =====\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 10.3038 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 10.3038 - acc: 1.0000 - val_loss: 10.0552 - val_acc: 1.0000\n",
      "===== Training Epoch 42 =====\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.0118 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 10.0118 - acc: 1.0000 - val_loss: 9.7689 - val_acc: 1.0000\n",
      "===== Training Epoch 43 =====\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 9.7263 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 9.7263 - acc: 1.0000 - val_loss: 9.4886 - val_acc: 1.0000\n",
      "===== Training Epoch 44 =====\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 9.4475 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 9.4475 - acc: 1.0000 - val_loss: 9.2149 - val_acc: 1.0000\n",
      "===== Training Epoch 45 =====\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 9.1753 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 9.1753 - acc: 1.0000 - val_loss: 8.9470 - val_acc: 1.0000\n",
      "===== Training Epoch 46 =====\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 8.9085 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 8.9085 - acc: 1.0000 - val_loss: 8.6865 - val_acc: 1.0000\n",
      "Epoch 46: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[MARC:] Processing time: 223254.75900000002 milliseconds. Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       " 0       1.0            1.0                1.0              1.0           1.0   \n",
       " 1       1.0            1.0                1.0              1.0           1.0   \n",
       " 2       1.0            1.0                1.0              1.0           1.0   \n",
       " \n",
       "    f1_macro     clstime  \n",
       " 0       1.0  201250.305  \n",
       " 1       1.0  211932.515  \n",
       " 2       1.0  223253.654  ,\n",
       " array([[5.6651433e-04, 9.9943352e-01],\n",
       "        [8.0264499e-03, 9.9197352e-01],\n",
       "        [1.8863132e-03, 9.9811375e-01],\n",
       "        [2.4714947e-02, 9.7528505e-01],\n",
       "        [9.1735232e-01, 8.2647711e-02],\n",
       "        [9.5499998e-01, 4.5000061e-02],\n",
       "        [9.1258031e-01, 8.7419711e-02],\n",
       "        [9.9697435e-01, 3.0256321e-03],\n",
       "        [8.2169616e-01, 1.7830388e-01],\n",
       "        [9.1273481e-01, 8.7265208e-02],\n",
       "        [9.9702388e-01, 2.9761621e-03],\n",
       "        [3.7747170e-03, 9.9622524e-01],\n",
       "        [4.0573914e-02, 9.5942611e-01],\n",
       "        [2.0550643e-03, 9.9794489e-01],\n",
       "        [4.7285422e-03, 9.9527150e-01]], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>clstime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>201250.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>211932.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>223253.654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0       1.0            1.0                1.0              1.0           1.0   \n",
       "1       1.0            1.0                1.0              1.0           1.0   \n",
       "2       1.0            1.0                1.0              1.0           1.0   \n",
       "\n",
       "   f1_macro     clstime  \n",
       "0       1.0  201250.305  \n",
       "1       1.0  211932.515  \n",
       "2       1.0  223253.654  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can visualize the test report (the same on most models):\n",
    "model.test_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>clstime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>212145.491333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0       1.0            1.0                1.0              1.0           1.0   \n",
       "\n",
       "   f1_macro        clstime  \n",
       "0       1.0  212145.491333  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And show the mean results\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to use the classifier in a traditional way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-30 19:03:38.013530\n",
      "\n",
      "[MARC:] Building model\n",
      "\n",
      "###########    DATA PREPARATION      ###########\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Attribute 'space': 189 unique values\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Attribute 'time': 580 unique values\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Attribute 'day': 7 unique values\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Attribute 'poi': 172 unique values\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Attribute 'type': 97 unique values\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Attribute 'root_type': 10 unique values\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Attribute 'rating': 44 unique values\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Attribute 'weather': 6 unique values\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Total of attribute/value pairs: 1105\n",
      "\u001b[[[    INFO    ] 2024-06-30 19:03:38 :: Processing trajectory 46/46. \n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Loading data from files ... DONE!\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Trajectories:  46\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Labels:        2\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Train size:    0.6739130434782609\n",
      "[    INFO    ] 2024-06-30 19:03:38 :: Test size:     0.32608695652173914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File 'metrics.csv' not found!\n",
      "===== Training Epoch 1 =====\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 289ms/steps: 32.5826 - acc: 0.58\n",
      "TRAIN\t\tacc: 0.838710\tacc_top5: 1.000000\tf1_macro: 0.827202\tprec_macro: 0.886364\trec_macro: 0.821429\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "TEST\t\tacc: 0.733333\tacc_top5: 1.000000\tf1_macro: 0.700000\tprec_macro: 0.833333\trec_macro: 0.714286\n",
      "1/1 [==============================] - 3s 3s/step - loss: 32.5826 - acc: 0.5806 - val_loss: 31.7694 - val_acc: 0.7333\n",
      "===== Training Epoch 2 =====\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 40ms/stepss: 31.7628 - acc: 0.70\n",
      "TRAIN\t\tacc: 0.838710\tacc_top5: 1.000000\tf1_macro: 0.827202\tprec_macro: 0.886364\trec_macro: 0.821429\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "TEST\t\tacc: 0.866667\tacc_top5: 1.000000\tf1_macro: 0.861111\tprec_macro: 0.900000\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 31.7628 - acc: 0.7097 - val_loss: 30.9685 - val_acc: 0.8667\n",
      "===== Training Epoch 3 =====\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 46ms/stepss: 30.9652 - acc: 0.77\n",
      "TRAIN\t\tacc: 0.870968\tacc_top5: 1.000000\tf1_macro: 0.864035\tprec_macro: 0.904762\trec_macro: 0.857143\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 30.9652 - acc: 0.7742 - val_loss: 30.1813 - val_acc: 0.9333\n",
      "===== Training Epoch 4 =====\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 29ms/stepss: 30.1686 - acc: 0.90\n",
      "TRAIN\t\tacc: 0.935484\tacc_top5: 1.000000\tf1_macro: 0.933761\tprec_macro: 0.947368\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 30.1686 - acc: 0.9032 - val_loss: 29.4096 - val_acc: 0.9333\n",
      "===== Training Epoch 5 =====\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 45ms/stepss: 29.3999 - acc: 0.93\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 29.3999 - acc: 0.9355 - val_loss: 28.6506 - val_acc: 0.9333\n",
      "===== Training Epoch 6 =====\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 30ms/stepss: 28.6294 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 28.6294 - acc: 0.9677 - val_loss: 27.9037 - val_acc: 0.9333\n",
      "===== Training Epoch 7 =====\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 27.8772 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 27.8772 - acc: 0.9677 - val_loss: 27.1716 - val_acc: 0.9333\n",
      "===== Training Epoch 8 =====\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 22ms/stepss: 27.1381 - acc: 0.96\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 0.933333\tacc_top5: 1.000000\tf1_macro: 0.932127\tprec_macro: 0.944444\trec_macro: 0.928571\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 27.1381 - acc: 0.9677 - val_loss: 26.4522 - val_acc: 0.9333\n",
      "===== Training Epoch 9 =====\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 26.4086 - acc: 1.00\n",
      "TRAIN\t\tacc: 0.967742\tacc_top5: 1.000000\tf1_macro: 0.967196\tprec_macro: 0.972222\trec_macro: 0.964286\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 26.4086 - acc: 1.0000 - val_loss: 25.7455 - val_acc: 1.0000\n",
      "===== Training Epoch 10 =====\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 27ms/stepss: 25.7084 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 25.7084 - acc: 1.0000 - val_loss: 25.0497 - val_acc: 1.0000\n",
      "===== Training Epoch 11 =====\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 25.0157 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 25.0157 - acc: 1.0000 - val_loss: 24.3666 - val_acc: 1.0000\n",
      "===== Training Epoch 12 =====\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 21ms/stepss: 24.3136 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 24.3136 - acc: 1.0000 - val_loss: 23.6937 - val_acc: 1.0000\n",
      "===== Training Epoch 13 =====\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 16ms/stepss: 23.6449 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 23.6449 - acc: 1.0000 - val_loss: 23.0339 - val_acc: 1.0000\n",
      "===== Training Epoch 14 =====\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 20ms/stepss: 22.9830 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 22.9830 - acc: 1.0000 - val_loss: 22.3872 - val_acc: 1.0000\n",
      "===== Training Epoch 15 =====\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 32ms/stepss: 22.3399 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 22.3399 - acc: 1.0000 - val_loss: 21.7531 - val_acc: 1.0000\n",
      "===== Training Epoch 16 =====\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 24ms/stepss: 21.7032 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 21.7032 - acc: 1.0000 - val_loss: 21.1345 - val_acc: 1.0000\n",
      "===== Training Epoch 17 =====\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 21.0878 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 21.0878 - acc: 1.0000 - val_loss: 20.5303 - val_acc: 1.0000\n",
      "===== Training Epoch 18 =====\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 20.4816 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 20.4816 - acc: 1.0000 - val_loss: 19.9437 - val_acc: 1.0000\n",
      "===== Training Epoch 19 =====\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 19.9002 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 19.9002 - acc: 1.0000 - val_loss: 19.3702 - val_acc: 1.0000\n",
      "===== Training Epoch 20 =====\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 19.3284 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 19.3284 - acc: 1.0000 - val_loss: 18.8098 - val_acc: 1.0000\n",
      "===== Training Epoch 21 =====\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.7745 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 18.7745 - acc: 1.0000 - val_loss: 18.2643 - val_acc: 1.0000\n",
      "===== Training Epoch 22 =====\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 18.2328 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 18.2328 - acc: 1.0000 - val_loss: 17.7303 - val_acc: 1.0000\n",
      "===== Training Epoch 23 =====\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 17.6992 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 17.6992 - acc: 1.0000 - val_loss: 17.2110 - val_acc: 1.0000\n",
      "===== Training Epoch 24 =====\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 17.1876 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 17.1876 - acc: 1.0000 - val_loss: 16.7044 - val_acc: 1.0000\n",
      "===== Training Epoch 25 =====\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.6808 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 16.6808 - acc: 1.0000 - val_loss: 16.2091 - val_acc: 1.0000\n",
      "===== Training Epoch 26 =====\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 16.1881 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 16.1881 - acc: 1.0000 - val_loss: 15.7272 - val_acc: 1.0000\n",
      "===== Training Epoch 27 =====\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.7087 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 15.7087 - acc: 1.0000 - val_loss: 15.2559 - val_acc: 1.0000\n",
      "===== Training Epoch 28 =====\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 15.2399 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 15.2399 - acc: 1.0000 - val_loss: 14.7952 - val_acc: 1.0000\n",
      "===== Training Epoch 29 =====\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 14.7818 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 14.7818 - acc: 1.0000 - val_loss: 14.3458 - val_acc: 1.0000\n",
      "===== Training Epoch 30 =====\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 19ms/stepss: 14.3332 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 14.3332 - acc: 1.0000 - val_loss: 13.9083 - val_acc: 1.0000\n",
      "===== Training Epoch 31 =====\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 18ms/stepss: 13.8981 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 13.8981 - acc: 1.0000 - val_loss: 13.4811 - val_acc: 1.0000\n",
      "===== Training Epoch 32 =====\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.4706 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 13.4706 - acc: 1.0000 - val_loss: 13.0648 - val_acc: 1.0000\n",
      "===== Training Epoch 33 =====\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 13.0551 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 13.0551 - acc: 1.0000 - val_loss: 12.6571 - val_acc: 1.0000\n",
      "===== Training Epoch 34 =====\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 12.6487 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 12.6487 - acc: 1.0000 - val_loss: 12.2576 - val_acc: 1.0000\n",
      "===== Training Epoch 35 =====\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 12.2504 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 12.2504 - acc: 1.0000 - val_loss: 11.8685 - val_acc: 1.0000\n",
      "===== Training Epoch 36 =====\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.8621 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 11.8621 - acc: 1.0000 - val_loss: 11.4906 - val_acc: 1.0000\n",
      "===== Training Epoch 37 =====\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 14ms/stepss: 11.4839 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 11.4839 - acc: 1.0000 - val_loss: 11.1215 - val_acc: 1.0000\n",
      "===== Training Epoch 38 =====\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 11.1154 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 11.1154 - acc: 1.0000 - val_loss: 10.7601 - val_acc: 1.0000\n",
      "===== Training Epoch 39 =====\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/stepss: 10.7544 - acc: 1.00\n",
      "TRAIN\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "TEST\t\tacc: 1.000000\tacc_top5: 1.000000\tf1_macro: 1.000000\tprec_macro: 1.000000\trec_macro: 1.000000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 10.7544 - acc: 1.0000 - val_loss: 10.4092 - val_acc: 1.0000\n",
      "Epoch 39: early stopping\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "from matclassification.methods import MARC\n",
    "\n",
    "model = MARC()\n",
    "\n",
    "# Each classifier expect a ceirtain input format. If you want to change, check the 'xy' method:\n",
    "(keys, vocab_size, num_classes, max_length, le, x_train, x_test, y_train, y_test) = model.xy(train, test)\n",
    "\n",
    "# You can add method variables with this:\n",
    "model.add_config(keys=keys, \n",
    "                 vocab_size=vocab_size,\n",
    "                 num_classes=num_classes,\n",
    "                 max_length=max_length)\n",
    "model.le = le # The label encoder\n",
    "\n",
    "# Run the classifier:\n",
    "model.fit(x_train, y_train, x_test, y_test)\n",
    "\n",
    "summary, y_pred = model.predict(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0       1.0            1.0                1.0              1.0           1.0   \n",
       "\n",
       "   f1_macro  \n",
       "0       1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.2. DeepeST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-30 19:03:49.334432\n",
      "\n",
      "[DeepeST:] Building model\n",
      "\n",
      "###########    DATA PREPARATION      ###########\n",
      "Attribute 'space': 151 unique values\n",
      "Attribute 'time': 434 unique values\n",
      "Attribute 'day': 7 unique values\n",
      "Attribute 'poi': 137 unique values\n",
      "Attribute 'type': 85 unique values\n",
      "Attribute 'root_type': 10 unique values\n",
      "Attribute 'rating': 41 unique values\n",
      "Attribute 'weather': 5 unique values\n",
      "Attribute 'tid': 31 unique values\n",
      "Total of attribute/value pairs: 901\n",
      "\n",
      "\n",
      "###########      DATA ENCODING        ###########\n",
      "\n",
      "Input total: 2\n",
      "... tid_0: 31\n",
      "... tid_1: 15\n",
      "col_name: ['space', 'time', 'day', 'poi', 'type', 'root_type', 'rating', 'weather', 'tid', 'label']...\n",
      "... num_classes: 2\n",
      "... max_lenght: 38\n",
      "Removing column tid of attr\n",
      "Removing column label of attr\n",
      "\n",
      "\n",
      "#####   Encoding string data to integer   ######\n",
      "   Encoding: space\n",
      "   Encoding: time\n",
      "   Encoding: day\n",
      "   Encoding: poi\n",
      "   Encoding: type\n",
      "   Encoding: root_type\n",
      "   Encoding: rating\n",
      "   Encoding: weather\n",
      "\n",
      "\n",
      "###########      Generating y_train and y_test     ###########\n",
      "OneHot encoding on label y\n",
      "Input total: 2\n",
      "[DeepeST:] Training hiperparameter model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10451f2bfc504bcd9e46d532b0feadc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[DeepeST:] Model Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe551ea74c44f14850bd0006b2428a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.5561 - acc: 0.5161 - val_loss: 62.9719 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 62.9639 - acc: 0.5484 - val_loss: 61.4122 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 61.4005 - acc: 0.5484 - val_loss: 59.8795 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 59.8584 - acc: 0.5806 - val_loss: 58.3747 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 58.3573 - acc: 0.6129 - val_loss: 56.8936 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 56.8540 - acc: 0.6452 - val_loss: 55.4394 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 55.4279 - acc: 0.6774 - val_loss: 54.0139 - val_acc: 0.6000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 53.9801 - acc: 0.8710 - val_loss: 52.6137 - val_acc: 0.6000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 52.5983 - acc: 0.8065 - val_loss: 51.2414 - val_acc: 0.7333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 51.2290 - acc: 0.8387 - val_loss: 49.8900 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 49.8705 - acc: 0.9677 - val_loss: 48.5633 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 48.5452 - acc: 0.9677 - val_loss: 47.2579 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 47.2181 - acc: 1.0000 - val_loss: 45.9783 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 45.9397 - acc: 1.0000 - val_loss: 44.7226 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 44.6738 - acc: 1.0000 - val_loss: 43.4909 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 43.4216 - acc: 1.0000 - val_loss: 42.2847 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 42.2183 - acc: 1.0000 - val_loss: 41.0970 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 41.0322 - acc: 1.0000 - val_loss: 39.9387 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 39.8725 - acc: 1.0000 - val_loss: 38.8036 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 38.7255 - acc: 1.0000 - val_loss: 37.6922 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 37.6157 - acc: 1.0000 - val_loss: 36.6060 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 36.5255 - acc: 1.0000 - val_loss: 35.5359 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 35.4501 - acc: 1.0000 - val_loss: 34.4890 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 34.4068 - acc: 1.0000 - val_loss: 33.4633 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 33.3851 - acc: 1.0000 - val_loss: 32.4603 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 32.3878 - acc: 1.0000 - val_loss: 31.4846 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 31.4135 - acc: 1.0000 - val_loss: 30.5307 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 30.4613 - acc: 1.0000 - val_loss: 29.5979 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 29.5331 - acc: 1.0000 - val_loss: 28.6879 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 28.6291 - acc: 1.0000 - val_loss: 27.8041 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 27.7480 - acc: 1.0000 - val_loss: 26.9389 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 26.8899 - acc: 1.0000 - val_loss: 26.0956 - val_acc: 1.0000\n",
      "1/1 [==============================] - 0s 465ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a4764df4c54e3baf3c296a793957c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.5952 - acc: 0.6129 - val_loss: 63.0092 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 63.0057 - acc: 0.5806 - val_loss: 61.4453 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 61.4232 - acc: 0.6129 - val_loss: 59.9072 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 59.8903 - acc: 0.5806 - val_loss: 58.3979 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 58.3938 - acc: 0.6774 - val_loss: 56.9119 - val_acc: 0.8000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 56.8663 - acc: 0.8710 - val_loss: 55.4537 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 55.4453 - acc: 0.9032 - val_loss: 54.0230 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 54.0132 - acc: 0.9032 - val_loss: 52.6149 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 52.5866 - acc: 0.9677 - val_loss: 51.2334 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 51.2140 - acc: 1.0000 - val_loss: 49.8747 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 49.8324 - acc: 1.0000 - val_loss: 48.5403 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 48.5143 - acc: 1.0000 - val_loss: 47.2268 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 47.1790 - acc: 1.0000 - val_loss: 45.9400 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 45.8863 - acc: 1.0000 - val_loss: 44.6761 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 44.6227 - acc: 1.0000 - val_loss: 43.4367 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 43.3633 - acc: 1.0000 - val_loss: 42.2236 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 42.1625 - acc: 1.0000 - val_loss: 41.0301 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 40.9703 - acc: 1.0000 - val_loss: 39.8668 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 39.8136 - acc: 1.0000 - val_loss: 38.7277 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 38.6673 - acc: 1.0000 - val_loss: 37.6146 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 37.5629 - acc: 1.0000 - val_loss: 36.5295 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 36.4813 - acc: 1.0000 - val_loss: 35.4684 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 35.4222 - acc: 1.0000 - val_loss: 34.4344 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 34.3959 - acc: 1.0000 - val_loss: 33.4220 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 33.3866 - acc: 1.0000 - val_loss: 32.4325 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 32.4007 - acc: 1.0000 - val_loss: 31.4696 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 31.4408 - acc: 1.0000 - val_loss: 30.5267 - val_acc: 1.0000\n",
      "1/1 [==============================] - 0s 464ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8899b99dcf3481b9e444977fc0c47b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.6209 - acc: 0.5161 - val_loss: 63.0133 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 62.9875 - acc: 0.6129 - val_loss: 61.4340 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 61.4134 - acc: 0.6774 - val_loss: 59.8771 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 59.8413 - acc: 0.7097 - val_loss: 58.3449 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 58.3369 - acc: 0.7742 - val_loss: 56.8392 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 56.7706 - acc: 0.9677 - val_loss: 55.3615 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 55.3848 - acc: 0.8710 - val_loss: 53.9131 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 53.8788 - acc: 0.9355 - val_loss: 52.4914 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 52.4885 - acc: 0.9355 - val_loss: 51.0991 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 51.0866 - acc: 1.0000 - val_loss: 49.7319 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 49.7050 - acc: 1.0000 - val_loss: 48.3898 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 48.3575 - acc: 1.0000 - val_loss: 47.0695 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 47.0246 - acc: 1.0000 - val_loss: 45.7795 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 45.7284 - acc: 1.0000 - val_loss: 44.5187 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 44.4752 - acc: 1.0000 - val_loss: 43.2874 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 43.2349 - acc: 1.0000 - val_loss: 42.0866 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 42.0434 - acc: 1.0000 - val_loss: 40.9068 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 40.8723 - acc: 1.0000 - val_loss: 39.7588 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 39.7335 - acc: 1.0000 - val_loss: 38.6379 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 38.6119 - acc: 1.0000 - val_loss: 37.5426 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 37.5230 - acc: 1.0000 - val_loss: 36.4736 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 36.4566 - acc: 1.0000 - val_loss: 35.4226 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 35.4082 - acc: 1.0000 - val_loss: 34.3986 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 34.3872 - acc: 1.0000 - val_loss: 33.3955 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 33.3846 - acc: 1.0000 - val_loss: 32.4119 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 630ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5611fb47f4b440f18ecb48e0c0dd5161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.5439 - acc: 0.5161 - val_loss: 62.9390 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 62.8992 - acc: 0.5806 - val_loss: 61.3427 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 61.3089 - acc: 0.8065 - val_loss: 59.7614 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 59.7414 - acc: 0.9032 - val_loss: 58.2186 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 58.1924 - acc: 0.9355 - val_loss: 56.7034 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 56.6349 - acc: 1.0000 - val_loss: 55.2182 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 55.1982 - acc: 0.9677 - val_loss: 53.7640 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 53.7425 - acc: 0.9355 - val_loss: 52.3336 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 52.3035 - acc: 0.9677 - val_loss: 50.9301 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 50.9013 - acc: 1.0000 - val_loss: 49.5532 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 49.5286 - acc: 1.0000 - val_loss: 48.2075 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 48.1781 - acc: 1.0000 - val_loss: 46.8911 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 46.8604 - acc: 1.0000 - val_loss: 45.6136 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 45.5854 - acc: 1.0000 - val_loss: 44.3680 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 44.3453 - acc: 1.0000 - val_loss: 43.1515 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 43.1267 - acc: 1.0000 - val_loss: 41.9620 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 41.9433 - acc: 1.0000 - val_loss: 40.7945 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 40.7791 - acc: 1.0000 - val_loss: 39.6581 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 39.6468 - acc: 1.0000 - val_loss: 38.5433 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 38.5319 - acc: 1.0000 - val_loss: 37.4509 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 37.4424 - acc: 1.0000 - val_loss: 36.3851 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 36.3769 - acc: 1.0000 - val_loss: 35.3387 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 35.3330 - acc: 1.0000 - val_loss: 34.3162 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 34.3110 - acc: 1.0000 - val_loss: 33.3135 - val_acc: 1.0000\n",
      "WARNING:tensorflow:5 out of the last 83 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2dd5b45e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 83 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2dd5b45e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 483ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456d8e12d168443bb7dab88b2585e4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.5574 - acc: 0.3871 - val_loss: 62.9305 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 62.8977 - acc: 0.5806 - val_loss: 61.2893 - val_acc: 0.8000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 61.2810 - acc: 0.7097 - val_loss: 59.6895 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 59.6636 - acc: 0.9032 - val_loss: 58.1301 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 58.1267 - acc: 1.0000 - val_loss: 56.6016 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 56.5303 - acc: 1.0000 - val_loss: 55.1134 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 55.0871 - acc: 0.9677 - val_loss: 53.6551 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 53.6181 - acc: 1.0000 - val_loss: 52.2222 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 52.1914 - acc: 1.0000 - val_loss: 50.8247 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 50.7876 - acc: 1.0000 - val_loss: 49.4620 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 49.4289 - acc: 1.0000 - val_loss: 48.1355 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 48.1132 - acc: 1.0000 - val_loss: 46.8397 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 46.8180 - acc: 1.0000 - val_loss: 45.5765 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 45.5591 - acc: 1.0000 - val_loss: 44.3419 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 44.3304 - acc: 1.0000 - val_loss: 43.1339 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 43.1227 - acc: 1.0000 - val_loss: 41.9533 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 41.9440 - acc: 1.0000 - val_loss: 40.7924 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 40.7856 - acc: 1.0000 - val_loss: 39.6615 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 39.6562 - acc: 1.0000 - val_loss: 38.5533 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 38.5478 - acc: 1.0000 - val_loss: 37.4651 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 37.4613 - acc: 1.0000 - val_loss: 36.4018 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 36.3984 - acc: 1.0000 - val_loss: 35.3554 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 35.3527 - acc: 1.0000 - val_loss: 34.3345 - val_acc: 1.0000\n",
      "WARNING:tensorflow:6 out of the last 84 calls to <function Model.make_predict_function.<locals>.predict_function at 0x405556e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 84 calls to <function Model.make_predict_function.<locals>.predict_function at 0x405556e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 673ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b703d728e85845319b417375640104b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.5679 - acc: 0.4194 - val_loss: 62.9857 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 62.9908 - acc: 0.6452 - val_loss: 61.4277 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 61.4279 - acc: 0.5484 - val_loss: 59.8968 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 59.8917 - acc: 0.5806 - val_loss: 58.3975 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 58.3776 - acc: 0.6129 - val_loss: 56.9246 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 56.8985 - acc: 0.5484 - val_loss: 55.4782 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 55.4624 - acc: 0.5806 - val_loss: 54.0585 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 54.0290 - acc: 0.6452 - val_loss: 52.6626 - val_acc: 0.5333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 52.6303 - acc: 0.7097 - val_loss: 51.2951 - val_acc: 0.5333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 51.2933 - acc: 0.7097 - val_loss: 49.9499 - val_acc: 0.6000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 49.9329 - acc: 0.8065 - val_loss: 48.6290 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 48.6198 - acc: 0.8710 - val_loss: 47.3305 - val_acc: 0.9333\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 47.2918 - acc: 0.9355 - val_loss: 46.0608 - val_acc: 0.9333\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 46.0331 - acc: 0.9355 - val_loss: 44.8155 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 44.7930 - acc: 0.9355 - val_loss: 43.5919 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 43.5451 - acc: 1.0000 - val_loss: 42.3911 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 42.3428 - acc: 1.0000 - val_loss: 41.2078 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 41.1749 - acc: 0.9677 - val_loss: 40.0514 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 40.0028 - acc: 1.0000 - val_loss: 38.9134 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 38.8599 - acc: 1.0000 - val_loss: 37.7970 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 37.7424 - acc: 1.0000 - val_loss: 36.7039 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 36.6477 - acc: 1.0000 - val_loss: 35.6279 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 35.5717 - acc: 1.0000 - val_loss: 34.5798 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 34.5259 - acc: 1.0000 - val_loss: 33.5525 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 33.4970 - acc: 1.0000 - val_loss: 32.5485 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 32.4934 - acc: 1.0000 - val_loss: 31.5701 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 31.5145 - acc: 1.0000 - val_loss: 30.6139 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 30.5581 - acc: 1.0000 - val_loss: 29.6777 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 29.6275 - acc: 1.0000 - val_loss: 28.7637 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 28.7114 - acc: 1.0000 - val_loss: 27.8719 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 27.8280 - acc: 1.0000 - val_loss: 26.9989 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 26.9586 - acc: 1.0000 - val_loss: 26.1500 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 26.1089 - acc: 1.0000 - val_loss: 25.3219 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 25.2859 - acc: 1.0000 - val_loss: 24.5128 - val_acc: 1.0000\n",
      "1/1 [==============================] - 0s 485ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc7387d8152461684e60946b555ed14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.6466 - acc: 0.5484 - val_loss: 63.0581 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 63.0375 - acc: 0.6774 - val_loss: 61.4932 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 61.4771 - acc: 0.5806 - val_loss: 59.9534 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 59.9516 - acc: 0.6129 - val_loss: 58.4405 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 58.4196 - acc: 0.6129 - val_loss: 56.9513 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 56.9040 - acc: 0.8387 - val_loss: 55.4890 - val_acc: 0.6000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 55.4862 - acc: 0.7419 - val_loss: 54.0547 - val_acc: 0.8000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 54.0312 - acc: 0.8387 - val_loss: 52.6451 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 52.6168 - acc: 0.9032 - val_loss: 51.2624 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 51.2600 - acc: 0.9677 - val_loss: 49.9023 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 49.8890 - acc: 1.0000 - val_loss: 48.5666 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 48.5428 - acc: 1.0000 - val_loss: 47.2544 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 47.2178 - acc: 1.0000 - val_loss: 45.9692 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 45.9254 - acc: 1.0000 - val_loss: 44.7109 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 44.6751 - acc: 1.0000 - val_loss: 43.4763 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 43.4098 - acc: 1.0000 - val_loss: 42.2672 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 42.2073 - acc: 1.0000 - val_loss: 41.0761 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 41.0267 - acc: 1.0000 - val_loss: 39.9141 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 39.8645 - acc: 1.0000 - val_loss: 38.7738 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 38.7181 - acc: 1.0000 - val_loss: 37.6570 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 37.6046 - acc: 1.0000 - val_loss: 36.5679 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 36.5131 - acc: 1.0000 - val_loss: 35.4993 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 35.4537 - acc: 1.0000 - val_loss: 34.4575 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 34.4234 - acc: 1.0000 - val_loss: 33.4399 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 33.4035 - acc: 1.0000 - val_loss: 32.4462 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 32.4091 - acc: 1.0000 - val_loss: 31.4777 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 31.4465 - acc: 1.0000 - val_loss: 30.5320 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 30.5014 - acc: 1.0000 - val_loss: 29.6098 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 29.5834 - acc: 1.0000 - val_loss: 28.7094 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 28.6855 - acc: 1.0000 - val_loss: 27.8307 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 504ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336e775025e143b3b074594f60ef148b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.5642 - acc: 0.4516 - val_loss: 62.9578 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 62.9487 - acc: 0.5806 - val_loss: 61.3811 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 437ms/step - loss: 61.3748 - acc: 0.6129 - val_loss: 59.8261 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 59.8035 - acc: 0.8065 - val_loss: 58.3012 - val_acc: 0.8667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 58.2915 - acc: 0.8710 - val_loss: 56.8013 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 56.7608 - acc: 0.9677 - val_loss: 55.3295 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 55.3337 - acc: 0.8387 - val_loss: 53.8858 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 53.8831 - acc: 0.8710 - val_loss: 52.4651 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 52.4584 - acc: 0.9032 - val_loss: 51.0710 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 51.0763 - acc: 0.9677 - val_loss: 49.6999 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 49.6926 - acc: 1.0000 - val_loss: 48.3554 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 48.3557 - acc: 0.9677 - val_loss: 47.0340 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 46.9974 - acc: 1.0000 - val_loss: 45.7439 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 45.7084 - acc: 1.0000 - val_loss: 44.4822 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 44.4657 - acc: 1.0000 - val_loss: 43.2475 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 43.2015 - acc: 1.0000 - val_loss: 42.0427 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 42.0169 - acc: 1.0000 - val_loss: 40.8610 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 40.8451 - acc: 1.0000 - val_loss: 39.7100 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 39.6945 - acc: 1.0000 - val_loss: 38.5833 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 38.5620 - acc: 1.0000 - val_loss: 37.4842 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 37.4690 - acc: 1.0000 - val_loss: 36.4108 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 36.3928 - acc: 1.0000 - val_loss: 35.3554 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 35.3448 - acc: 1.0000 - val_loss: 34.3266 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 34.3176 - acc: 1.0000 - val_loss: 33.3217 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 33.3125 - acc: 1.0000 - val_loss: 32.3394 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 32.3309 - acc: 1.0000 - val_loss: 31.3806 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 31.3734 - acc: 1.0000 - val_loss: 30.4421 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 30.4356 - acc: 1.0000 - val_loss: 29.5254 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 29.5198 - acc: 1.0000 - val_loss: 28.6310 - val_acc: 1.0000\n",
      "1/1 [==============================] - 0s 496ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bca683d4b72428d95da6d59f3d1cf62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.5713 - acc: 0.4516 - val_loss: 62.9612 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 62.9379 - acc: 0.5484 - val_loss: 61.3662 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 61.3562 - acc: 0.6452 - val_loss: 59.7915 - val_acc: 0.7333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 59.7822 - acc: 0.8387 - val_loss: 58.2512 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 58.2229 - acc: 0.9355 - val_loss: 56.7390 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 56.6650 - acc: 1.0000 - val_loss: 55.2553 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 55.2684 - acc: 0.9355 - val_loss: 53.8046 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 53.7906 - acc: 0.8710 - val_loss: 52.3779 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 52.3538 - acc: 0.9355 - val_loss: 50.9769 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 50.9653 - acc: 1.0000 - val_loss: 49.6016 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 49.5852 - acc: 1.0000 - val_loss: 48.2577 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 48.2396 - acc: 1.0000 - val_loss: 46.9421 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 46.9081 - acc: 1.0000 - val_loss: 45.6617 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 45.6325 - acc: 1.0000 - val_loss: 44.4131 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 44.3935 - acc: 1.0000 - val_loss: 43.1931 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 43.1615 - acc: 1.0000 - val_loss: 42.0026 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 41.9789 - acc: 1.0000 - val_loss: 40.8341 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 40.8178 - acc: 1.0000 - val_loss: 39.6951 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 39.6786 - acc: 1.0000 - val_loss: 38.5776 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 38.5628 - acc: 1.0000 - val_loss: 37.4852 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 37.4756 - acc: 1.0000 - val_loss: 36.4187 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 36.4092 - acc: 1.0000 - val_loss: 35.3696 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 35.3613 - acc: 1.0000 - val_loss: 34.3464 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 34.3397 - acc: 1.0000 - val_loss: 33.3448 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 509ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883e10f4b09a4c2b97a3869d526bb080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.5175 - acc: 0.5161 - val_loss: 62.9325 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 62.9152 - acc: 0.5484 - val_loss: 61.3116 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 61.2759 - acc: 0.6129 - val_loss: 59.7208 - val_acc: 0.8667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 59.7117 - acc: 0.8065 - val_loss: 58.1755 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 58.1700 - acc: 0.8387 - val_loss: 56.6620 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 56.5879 - acc: 0.9677 - val_loss: 55.1837 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 55.1843 - acc: 0.9355 - val_loss: 53.7391 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 53.7001 - acc: 0.9355 - val_loss: 52.3167 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 52.2743 - acc: 0.9355 - val_loss: 50.9182 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 50.8813 - acc: 1.0000 - val_loss: 49.5418 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 49.5032 - acc: 1.0000 - val_loss: 48.1947 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 48.1721 - acc: 1.0000 - val_loss: 46.8796 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 46.8477 - acc: 1.0000 - val_loss: 45.6018 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 45.5773 - acc: 1.0000 - val_loss: 44.3557 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 44.3397 - acc: 1.0000 - val_loss: 43.1399 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 43.1155 - acc: 1.0000 - val_loss: 41.9533 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 41.9363 - acc: 1.0000 - val_loss: 40.7877 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 40.7748 - acc: 1.0000 - val_loss: 39.6522 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 39.6422 - acc: 1.0000 - val_loss: 38.5394 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 38.5308 - acc: 1.0000 - val_loss: 37.4510 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 37.4442 - acc: 1.0000 - val_loss: 36.3877 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 36.3811 - acc: 1.0000 - val_loss: 35.3408 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 35.3365 - acc: 1.0000 - val_loss: 34.3192 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 34.3151 - acc: 1.0000 - val_loss: 33.3191 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 512ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7698ea31e2c47e6887bc25a298e85f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 181.3713 - acc: 0.3871 - val_loss: 175.1026 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 175.0977 - acc: 0.5484 - val_loss: 168.9691 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 168.9539 - acc: 0.5484 - val_loss: 162.9953 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 162.9850 - acc: 0.5484 - val_loss: 157.2022 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 157.2020 - acc: 0.5484 - val_loss: 151.5502 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 151.5310 - acc: 0.6129 - val_loss: 146.0522 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 146.0377 - acc: 0.6774 - val_loss: 140.7138 - val_acc: 0.6000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 140.6961 - acc: 0.7742 - val_loss: 135.5234 - val_acc: 0.8000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 135.5234 - acc: 0.7742 - val_loss: 130.4892 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 130.4850 - acc: 0.9355 - val_loss: 125.5880 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 125.5626 - acc: 0.9677 - val_loss: 120.8258 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 120.8171 - acc: 0.9677 - val_loss: 116.1936 - val_acc: 0.9333\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 116.1673 - acc: 0.9677 - val_loss: 111.7170 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 111.6707 - acc: 1.0000 - val_loss: 107.3739 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 107.3323 - acc: 1.0000 - val_loss: 103.1556 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 103.0987 - acc: 1.0000 - val_loss: 99.0715 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 99.0200 - acc: 1.0000 - val_loss: 95.0797 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 95.0268 - acc: 1.0000 - val_loss: 91.2372 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 91.1758 - acc: 1.0000 - val_loss: 87.5047 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 87.4310 - acc: 1.0000 - val_loss: 83.8924 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 83.8184 - acc: 1.0000 - val_loss: 80.4109 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 80.3402 - acc: 1.0000 - val_loss: 77.0136 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 76.9454 - acc: 1.0000 - val_loss: 73.7493 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 73.6845 - acc: 1.0000 - val_loss: 70.5923 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 70.5300 - acc: 1.0000 - val_loss: 67.5443 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 67.4858 - acc: 1.0000 - val_loss: 64.6180 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 64.5629 - acc: 1.0000 - val_loss: 61.7844 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 61.7325 - acc: 1.0000 - val_loss: 59.0456 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 58.9996 - acc: 1.0000 - val_loss: 56.4107 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 56.3700 - acc: 1.0000 - val_loss: 53.8729 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 53.8356 - acc: 1.0000 - val_loss: 51.4099 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 51.3765 - acc: 1.0000 - val_loss: 49.0559 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 49.0254 - acc: 1.0000 - val_loss: 46.7892 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 508ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994d8157de6f4b25aa0f578fe761c13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 181.4371 - acc: 0.4839 - val_loss: 175.1627 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 175.1508 - acc: 0.5484 - val_loss: 169.0155 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 168.9890 - acc: 0.6452 - val_loss: 163.0242 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 163.0075 - acc: 0.6774 - val_loss: 157.2094 - val_acc: 0.8000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 157.2000 - acc: 0.7419 - val_loss: 151.5373 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 151.4841 - acc: 0.9677 - val_loss: 146.0243 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 146.0038 - acc: 0.9032 - val_loss: 140.6730 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 140.6414 - acc: 0.9677 - val_loss: 135.4712 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 135.4454 - acc: 0.9355 - val_loss: 130.4269 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 130.4126 - acc: 1.0000 - val_loss: 125.5109 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 125.4708 - acc: 1.0000 - val_loss: 120.7354 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 120.7040 - acc: 0.9677 - val_loss: 116.0857 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 116.0274 - acc: 1.0000 - val_loss: 111.5864 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 111.5257 - acc: 1.0000 - val_loss: 107.2186 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 107.1606 - acc: 1.0000 - val_loss: 102.9837 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 102.9093 - acc: 1.0000 - val_loss: 98.8958 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 98.8366 - acc: 1.0000 - val_loss: 94.9118 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 94.8628 - acc: 1.0000 - val_loss: 91.0830 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 91.0334 - acc: 1.0000 - val_loss: 87.3652 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 87.3201 - acc: 1.0000 - val_loss: 83.7704 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 83.7316 - acc: 1.0000 - val_loss: 80.3052 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 80.2724 - acc: 1.0000 - val_loss: 76.9260 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 76.8973 - acc: 1.0000 - val_loss: 73.6839 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 73.6590 - acc: 1.0000 - val_loss: 70.5457 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 70.5234 - acc: 1.0000 - val_loss: 67.5117 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 67.4922 - acc: 1.0000 - val_loss: 64.5964 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 515ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce01810a71d146a7a0972bf141d94bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 181.4291 - acc: 0.4839 - val_loss: 175.1478 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 175.1339 - acc: 0.5484 - val_loss: 168.9776 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 168.9531 - acc: 0.6129 - val_loss: 162.9649 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 162.9567 - acc: 0.8710 - val_loss: 157.1371 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 157.1454 - acc: 1.0000 - val_loss: 151.4533 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 151.4121 - acc: 1.0000 - val_loss: 145.9261 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 145.9017 - acc: 0.9677 - val_loss: 140.5553 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 140.5132 - acc: 1.0000 - val_loss: 135.3379 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 135.3087 - acc: 0.9677 - val_loss: 130.2845 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 130.2540 - acc: 1.0000 - val_loss: 125.3683 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 125.3233 - acc: 1.0000 - val_loss: 120.5956 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 120.5521 - acc: 1.0000 - val_loss: 115.9562 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 115.9097 - acc: 1.0000 - val_loss: 111.4774 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 111.4341 - acc: 1.0000 - val_loss: 107.1331 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 107.0944 - acc: 1.0000 - val_loss: 102.9197 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 102.8832 - acc: 1.0000 - val_loss: 98.8422 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 98.8122 - acc: 1.0000 - val_loss: 94.8596 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 94.8363 - acc: 1.0000 - val_loss: 91.0321 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 91.0117 - acc: 1.0000 - val_loss: 87.3176 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 87.2999 - acc: 1.0000 - val_loss: 83.7285 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 83.7137 - acc: 1.0000 - val_loss: 80.2714 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 80.2587 - acc: 1.0000 - val_loss: 76.8983 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 76.8878 - acc: 1.0000 - val_loss: 73.6585 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 73.6494 - acc: 1.0000 - val_loss: 70.5161 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 512ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d6f032e8ed413583e7183e31a4dc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 181.2305 - acc: 0.4516 - val_loss: 174.9312 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 174.9140 - acc: 0.5484 - val_loss: 168.7328 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 168.7099 - acc: 0.8387 - val_loss: 162.7107 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 162.7275 - acc: 0.9032 - val_loss: 156.8690 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 156.8698 - acc: 1.0000 - val_loss: 151.1778 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 151.1132 - acc: 1.0000 - val_loss: 145.6522 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 145.6409 - acc: 0.9677 - val_loss: 140.2841 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 140.2692 - acc: 0.9677 - val_loss: 135.0616 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 135.0466 - acc: 1.0000 - val_loss: 130.0059 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 129.9789 - acc: 1.0000 - val_loss: 125.0974 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 125.0733 - acc: 1.0000 - val_loss: 120.3430 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 120.3181 - acc: 1.0000 - val_loss: 115.7237 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 115.7018 - acc: 1.0000 - val_loss: 111.2651 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 111.2456 - acc: 1.0000 - val_loss: 106.9445 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 106.9293 - acc: 1.0000 - val_loss: 102.7578 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 102.7444 - acc: 1.0000 - val_loss: 98.7070 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 98.6966 - acc: 1.0000 - val_loss: 94.7530 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 94.7449 - acc: 1.0000 - val_loss: 90.9509 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 90.9438 - acc: 1.0000 - val_loss: 87.2559 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 87.2499 - acc: 1.0000 - val_loss: 83.6792 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 83.6742 - acc: 1.0000 - val_loss: 80.2313 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 80.2271 - acc: 1.0000 - val_loss: 76.8653 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 76.8616 - acc: 1.0000 - val_loss: 73.6303 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 517ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a93ba538aa4241ac50494f0bd8659d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 181.3318 - acc: 0.5161 - val_loss: 175.0653 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 175.0399 - acc: 0.5484 - val_loss: 168.8033 - val_acc: 1.0000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 168.7729 - acc: 0.9677 - val_loss: 162.7843 - val_acc: 0.8667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 162.8159 - acc: 0.8065 - val_loss: 156.9237 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 156.9418 - acc: 1.0000 - val_loss: 151.2286 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 151.1726 - acc: 1.0000 - val_loss: 145.7047 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 145.6911 - acc: 1.0000 - val_loss: 140.3303 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 140.2897 - acc: 1.0000 - val_loss: 135.1039 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 135.0734 - acc: 1.0000 - val_loss: 130.0508 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 130.0244 - acc: 1.0000 - val_loss: 125.1463 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 125.1250 - acc: 1.0000 - val_loss: 120.3941 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 120.3795 - acc: 1.0000 - val_loss: 115.7750 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 115.7621 - acc: 1.0000 - val_loss: 111.3089 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 111.2992 - acc: 1.0000 - val_loss: 106.9740 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 106.9673 - acc: 1.0000 - val_loss: 102.7725 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 102.7671 - acc: 1.0000 - val_loss: 98.7054 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 98.7020 - acc: 1.0000 - val_loss: 94.7344 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 94.7316 - acc: 1.0000 - val_loss: 90.9221 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 90.9198 - acc: 1.0000 - val_loss: 87.2271 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 87.2251 - acc: 1.0000 - val_loss: 83.6522 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 83.6507 - acc: 1.0000 - val_loss: 80.2046 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 80.2034 - acc: 1.0000 - val_loss: 76.8387 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 519ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31c2a23b6414c6495580b67e2aca5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 181.3831 - acc: 0.5161 - val_loss: 175.1129 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 175.1061 - acc: 0.6129 - val_loss: 168.9800 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 168.9671 - acc: 0.6129 - val_loss: 163.0062 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 163.0072 - acc: 0.5484 - val_loss: 157.2156 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 157.2168 - acc: 0.5484 - val_loss: 151.5680 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 151.5455 - acc: 0.6774 - val_loss: 146.0741 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 146.0667 - acc: 0.6452 - val_loss: 140.7428 - val_acc: 0.6667\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 140.7245 - acc: 0.7419 - val_loss: 135.5594 - val_acc: 0.6667\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 135.5411 - acc: 0.8387 - val_loss: 130.5328 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 130.5299 - acc: 0.8710 - val_loss: 125.6363 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 125.6242 - acc: 0.9032 - val_loss: 120.8775 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 120.8784 - acc: 0.9355 - val_loss: 116.2459 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 116.2224 - acc: 0.9677 - val_loss: 111.7724 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 111.7469 - acc: 0.9677 - val_loss: 107.4354 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 107.4045 - acc: 1.0000 - val_loss: 103.2221 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 103.1667 - acc: 1.0000 - val_loss: 99.1408 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 99.1031 - acc: 1.0000 - val_loss: 95.1510 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 95.1148 - acc: 1.0000 - val_loss: 91.3118 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 91.2662 - acc: 1.0000 - val_loss: 87.5824 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 87.5259 - acc: 1.0000 - val_loss: 83.9694 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 83.9084 - acc: 1.0000 - val_loss: 80.4834 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 80.4224 - acc: 1.0000 - val_loss: 77.0819 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 77.0284 - acc: 1.0000 - val_loss: 73.8152 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 73.7620 - acc: 1.0000 - val_loss: 70.6515 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 70.5958 - acc: 1.0000 - val_loss: 67.5965 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 67.5455 - acc: 1.0000 - val_loss: 64.6696 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 64.6197 - acc: 1.0000 - val_loss: 61.8391 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 61.7954 - acc: 1.0000 - val_loss: 59.1035 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 59.0591 - acc: 1.0000 - val_loss: 56.4669 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 56.4288 - acc: 1.0000 - val_loss: 53.9240 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 53.8898 - acc: 1.0000 - val_loss: 51.4535 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 541ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9383323c8a684208871b504eab8ffc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 181.3240 - acc: 0.4516 - val_loss: 175.0362 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 175.0347 - acc: 0.5806 - val_loss: 168.8998 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 168.8869 - acc: 0.5484 - val_loss: 162.9188 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 162.9126 - acc: 0.6452 - val_loss: 157.1100 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 157.1100 - acc: 0.7097 - val_loss: 151.4489 - val_acc: 0.6000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 151.4197 - acc: 0.8065 - val_loss: 145.9433 - val_acc: 0.8667\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 145.9285 - acc: 0.9355 - val_loss: 140.5953 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 140.5746 - acc: 0.9355 - val_loss: 135.3941 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 135.3957 - acc: 0.8710 - val_loss: 130.3564 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 130.3562 - acc: 1.0000 - val_loss: 125.4521 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 125.4277 - acc: 1.0000 - val_loss: 120.6825 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 120.6776 - acc: 0.9677 - val_loss: 116.0378 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 116.0067 - acc: 0.9677 - val_loss: 111.5482 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 111.5161 - acc: 1.0000 - val_loss: 107.1892 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 107.1558 - acc: 1.0000 - val_loss: 102.9612 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 102.8992 - acc: 1.0000 - val_loss: 98.8736 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 98.8332 - acc: 1.0000 - val_loss: 94.8826 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 94.8460 - acc: 1.0000 - val_loss: 91.0433 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 91.0099 - acc: 1.0000 - val_loss: 87.3185 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 87.2785 - acc: 1.0000 - val_loss: 83.7158 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 83.6752 - acc: 1.0000 - val_loss: 80.2429 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 80.2081 - acc: 1.0000 - val_loss: 76.8565 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 76.8256 - acc: 1.0000 - val_loss: 73.6017 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 73.5747 - acc: 1.0000 - val_loss: 70.4520 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 70.4299 - acc: 1.0000 - val_loss: 67.4091 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 67.3876 - acc: 1.0000 - val_loss: 64.4883 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 64.4714 - acc: 1.0000 - val_loss: 61.6626 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 583ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6444ecdf5c9d4d12a8a4e741fa0a394f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 181.1546 - acc: 0.2581 - val_loss: 174.8554 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 174.8490 - acc: 0.5806 - val_loss: 168.7100 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 168.6839 - acc: 0.6774 - val_loss: 162.7114 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 162.6935 - acc: 0.7742 - val_loss: 156.8883 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 156.8837 - acc: 0.8710 - val_loss: 151.2112 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 151.1606 - acc: 1.0000 - val_loss: 145.6893 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 145.6757 - acc: 0.9677 - val_loss: 140.3271 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 140.3192 - acc: 0.9032 - val_loss: 135.1159 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 135.1333 - acc: 0.9355 - val_loss: 130.0666 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 130.0771 - acc: 0.9355 - val_loss: 125.1554 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 125.1557 - acc: 1.0000 - val_loss: 120.3849 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 120.3802 - acc: 0.9355 - val_loss: 115.7493 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 115.7212 - acc: 1.0000 - val_loss: 111.2742 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 111.2466 - acc: 1.0000 - val_loss: 106.9412 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 106.9176 - acc: 1.0000 - val_loss: 102.7428 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 102.7064 - acc: 1.0000 - val_loss: 98.6821 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 98.6563 - acc: 1.0000 - val_loss: 94.7187 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 94.7013 - acc: 1.0000 - val_loss: 90.9057 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 90.8891 - acc: 1.0000 - val_loss: 87.2048 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 87.1899 - acc: 1.0000 - val_loss: 83.6266 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 83.6146 - acc: 1.0000 - val_loss: 80.1756 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 80.1647 - acc: 1.0000 - val_loss: 76.8062 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 76.7977 - acc: 1.0000 - val_loss: 73.5685 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 73.5615 - acc: 1.0000 - val_loss: 70.4301 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 70.4244 - acc: 1.0000 - val_loss: 67.3990 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 605ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8de14a7f9a47248a9dc0a298861ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 181.2425 - acc: 0.5484 - val_loss: 174.9268 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 174.9132 - acc: 0.7097 - val_loss: 168.7473 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 168.7609 - acc: 0.8065 - val_loss: 162.7253 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 162.7195 - acc: 0.8387 - val_loss: 156.8842 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 156.8899 - acc: 0.9677 - val_loss: 151.1910 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 151.1529 - acc: 1.0000 - val_loss: 145.6544 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 145.6682 - acc: 0.9677 - val_loss: 140.2838 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 438ms/step - loss: 140.2947 - acc: 0.9355 - val_loss: 135.0628 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 135.0870 - acc: 0.9355 - val_loss: 130.0054 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 130.0002 - acc: 1.0000 - val_loss: 125.0971 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 125.0923 - acc: 1.0000 - val_loss: 120.3425 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 120.3488 - acc: 1.0000 - val_loss: 115.7193 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 115.7155 - acc: 1.0000 - val_loss: 111.2568 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 111.2491 - acc: 1.0000 - val_loss: 106.9382 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 106.9301 - acc: 1.0000 - val_loss: 102.7485 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 102.7382 - acc: 1.0000 - val_loss: 98.6904 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 98.6855 - acc: 1.0000 - val_loss: 94.7301 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 94.7265 - acc: 1.0000 - val_loss: 90.9215 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 90.9185 - acc: 1.0000 - val_loss: 87.2241 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 87.2208 - acc: 1.0000 - val_loss: 83.6422 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 83.6393 - acc: 1.0000 - val_loss: 80.1903 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 80.1883 - acc: 1.0000 - val_loss: 76.8216 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 76.8200 - acc: 1.0000 - val_loss: 73.5833 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 687ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598b5843bd7d4be688804c068882082e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 181.3662 - acc: 0.5161 - val_loss: 175.1379 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 712ms/step - loss: 175.1245 - acc: 0.5484 - val_loss: 168.8366 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 502ms/step - loss: 168.8405 - acc: 0.9355 - val_loss: 162.8087 - val_acc: 0.8667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 605ms/step - loss: 162.8511 - acc: 0.7419 - val_loss: 156.9495 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 156.9946 - acc: 1.0000 - val_loss: 151.2447 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 151.2121 - acc: 1.0000 - val_loss: 145.7233 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 145.7090 - acc: 0.9677 - val_loss: 140.3666 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 140.3263 - acc: 1.0000 - val_loss: 135.1513 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 135.1225 - acc: 1.0000 - val_loss: 130.0951 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 130.0621 - acc: 1.0000 - val_loss: 125.1838 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 125.1656 - acc: 1.0000 - val_loss: 120.4262 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 120.4108 - acc: 1.0000 - val_loss: 115.8030 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 115.7914 - acc: 1.0000 - val_loss: 111.3383 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 111.3294 - acc: 1.0000 - val_loss: 107.0145 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 107.0068 - acc: 1.0000 - val_loss: 102.8219 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 102.8154 - acc: 1.0000 - val_loss: 98.7606 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 98.7561 - acc: 1.0000 - val_loss: 94.7976 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 94.7951 - acc: 1.0000 - val_loss: 90.9875 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 90.9851 - acc: 1.0000 - val_loss: 87.2850 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 87.2829 - acc: 1.0000 - val_loss: 83.7010 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 83.6993 - acc: 1.0000 - val_loss: 80.2490 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 80.2477 - acc: 1.0000 - val_loss: 76.8800 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 76.8789 - acc: 1.0000 - val_loss: 73.6396 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 73.6387 - acc: 1.0000 - val_loss: 70.4987 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 697ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ac1be255a1411ba3fce6fe9db6749d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 332.4652 - acc: 0.5161 - val_loss: 318.4462 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 318.4373 - acc: 0.5484 - val_loss: 304.8125 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 304.8067 - acc: 0.5484 - val_loss: 291.6195 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 291.6035 - acc: 0.5484 - val_loss: 278.9196 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 278.9074 - acc: 0.6129 - val_loss: 266.6168 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 470ms/step - loss: 266.5942 - acc: 0.7097 - val_loss: 254.7388 - val_acc: 0.6667\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 254.7262 - acc: 0.7419 - val_loss: 243.3044 - val_acc: 0.8667\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 243.2848 - acc: 0.8065 - val_loss: 232.2708 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 232.2664 - acc: 0.8387 - val_loss: 221.6673 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 221.6725 - acc: 1.0000 - val_loss: 211.4154 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 211.3918 - acc: 0.9677 - val_loss: 201.5325 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 201.5176 - acc: 1.0000 - val_loss: 191.9839 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 191.9504 - acc: 1.0000 - val_loss: 182.8589 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 182.8160 - acc: 1.0000 - val_loss: 174.0948 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 174.0453 - acc: 1.0000 - val_loss: 165.6668 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 165.6074 - acc: 1.0000 - val_loss: 157.5907 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 157.5293 - acc: 1.0000 - val_loss: 149.7479 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 149.6902 - acc: 1.0000 - val_loss: 142.2996 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 142.2350 - acc: 1.0000 - val_loss: 135.1283 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 135.0607 - acc: 1.0000 - val_loss: 128.2535 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 128.1916 - acc: 1.0000 - val_loss: 121.7100 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 121.6511 - acc: 1.0000 - val_loss: 115.3532 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 115.2975 - acc: 1.0000 - val_loss: 109.3260 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 109.2768 - acc: 1.0000 - val_loss: 103.5350 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 103.4908 - acc: 1.0000 - val_loss: 97.9983 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 97.9593 - acc: 1.0000 - val_loss: 92.7724 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 92.7371 - acc: 1.0000 - val_loss: 87.7775 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 87.7457 - acc: 1.0000 - val_loss: 83.0000 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 82.9714 - acc: 1.0000 - val_loss: 78.4550 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 78.4296 - acc: 1.0000 - val_loss: 74.1266 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 74.1039 - acc: 1.0000 - val_loss: 69.9645 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 697ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce6ddf9c17b4a73b91d9a660a5bea05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 332.4597 - acc: 0.4194 - val_loss: 318.4289 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 318.4268 - acc: 0.5484 - val_loss: 304.7828 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 304.7877 - acc: 0.5484 - val_loss: 291.5701 - val_acc: 0.6000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 555ms/step - loss: 291.5632 - acc: 0.7097 - val_loss: 278.8542 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 495ms/step - loss: 278.8542 - acc: 0.8065 - val_loss: 266.5328 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 579ms/step - loss: 266.5176 - acc: 0.9677 - val_loss: 254.6364 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 254.6287 - acc: 0.9677 - val_loss: 243.1766 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 243.1603 - acc: 0.9677 - val_loss: 232.1228 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 232.1028 - acc: 1.0000 - val_loss: 221.5079 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 221.5038 - acc: 1.0000 - val_loss: 211.2485 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 211.2207 - acc: 1.0000 - val_loss: 201.3632 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 201.3414 - acc: 1.0000 - val_loss: 191.8089 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 191.7720 - acc: 1.0000 - val_loss: 182.6711 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 182.6335 - acc: 1.0000 - val_loss: 173.8870 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 173.8463 - acc: 1.0000 - val_loss: 165.4453 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 165.3979 - acc: 1.0000 - val_loss: 157.3652 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 157.3279 - acc: 1.0000 - val_loss: 149.5263 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 149.4946 - acc: 1.0000 - val_loss: 142.0891 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 142.0574 - acc: 1.0000 - val_loss: 134.9326 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 134.9032 - acc: 1.0000 - val_loss: 128.0753 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 128.0515 - acc: 1.0000 - val_loss: 121.5543 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 121.5345 - acc: 1.0000 - val_loss: 115.2231 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 115.2054 - acc: 1.0000 - val_loss: 109.2136 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 109.1981 - acc: 1.0000 - val_loss: 103.4469 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 103.4336 - acc: 1.0000 - val_loss: 97.9359 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 839ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302a1d34c1e34892a606969f57847e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 332.4729 - acc: 0.5806 - val_loss: 318.4558 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 688ms/step - loss: 318.4357 - acc: 0.5484 - val_loss: 304.7701 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 497ms/step - loss: 304.7567 - acc: 0.7419 - val_loss: 291.5294 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 638ms/step - loss: 291.5235 - acc: 0.8710 - val_loss: 278.8000 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 278.8081 - acc: 0.9355 - val_loss: 266.4683 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 266.4395 - acc: 1.0000 - val_loss: 254.5674 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 254.5322 - acc: 1.0000 - val_loss: 243.1069 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 243.0724 - acc: 0.9677 - val_loss: 232.0387 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 232.0233 - acc: 1.0000 - val_loss: 221.4090 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 221.3851 - acc: 1.0000 - val_loss: 211.1437 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 211.1071 - acc: 1.0000 - val_loss: 201.2533 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 201.2316 - acc: 1.0000 - val_loss: 191.7164 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 191.6823 - acc: 1.0000 - val_loss: 182.5944 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 182.5645 - acc: 1.0000 - val_loss: 173.8320 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 173.8083 - acc: 1.0000 - val_loss: 165.4064 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 165.3840 - acc: 1.0000 - val_loss: 157.3349 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 157.3176 - acc: 1.0000 - val_loss: 149.5139 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 149.5005 - acc: 1.0000 - val_loss: 142.0906 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 142.0784 - acc: 1.0000 - val_loss: 134.9462 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 134.9357 - acc: 1.0000 - val_loss: 128.1012 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 128.0926 - acc: 1.0000 - val_loss: 121.5839 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 121.5765 - acc: 1.0000 - val_loss: 115.2519 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 115.2455 - acc: 1.0000 - val_loss: 109.2520 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 732ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce60c2bf78b7497498d256909cd3592e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 332.4667 - acc: 0.4194 - val_loss: 318.4230 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 721ms/step - loss: 318.4042 - acc: 0.5484 - val_loss: 304.6913 - val_acc: 1.0000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 304.6902 - acc: 0.9355 - val_loss: 291.4333 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 538ms/step - loss: 291.4351 - acc: 0.9677 - val_loss: 278.6815 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 278.6797 - acc: 0.9677 - val_loss: 266.3422 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 266.2883 - acc: 1.0000 - val_loss: 254.4265 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 254.4011 - acc: 1.0000 - val_loss: 242.9468 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 242.9010 - acc: 1.0000 - val_loss: 231.8804 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 231.8639 - acc: 0.9677 - val_loss: 221.2615 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 221.2438 - acc: 1.0000 - val_loss: 211.0157 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 210.9962 - acc: 1.0000 - val_loss: 201.1576 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 201.1407 - acc: 1.0000 - val_loss: 191.6498 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 191.6283 - acc: 1.0000 - val_loss: 182.5554 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 182.5377 - acc: 1.0000 - val_loss: 173.8084 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 173.7942 - acc: 1.0000 - val_loss: 165.4000 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 165.3873 - acc: 1.0000 - val_loss: 157.3474 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 157.3376 - acc: 1.0000 - val_loss: 149.5336 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 149.5257 - acc: 1.0000 - val_loss: 142.1135 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 142.1064 - acc: 1.0000 - val_loss: 134.9685 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 134.9623 - acc: 1.0000 - val_loss: 128.1209 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 128.1157 - acc: 1.0000 - val_loss: 121.6025 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 121.5980 - acc: 1.0000 - val_loss: 115.2764 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 943ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bcd5f00fec458e96c0aa63d24f8c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 332.3335 - acc: 0.6129 - val_loss: 318.3060 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 718ms/step - loss: 318.2751 - acc: 0.5484 - val_loss: 304.5205 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 304.5420 - acc: 0.8387 - val_loss: 291.2564 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 479ms/step - loss: 291.2555 - acc: 0.9355 - val_loss: 278.4929 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 278.4684 - acc: 1.0000 - val_loss: 266.1604 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 266.1006 - acc: 1.0000 - val_loss: 254.2444 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 456ms/step - loss: 254.2193 - acc: 0.9677 - val_loss: 242.7623 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 242.7306 - acc: 1.0000 - val_loss: 231.7047 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 231.6910 - acc: 1.0000 - val_loss: 221.0975 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 221.0851 - acc: 1.0000 - val_loss: 210.8629 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 210.8537 - acc: 1.0000 - val_loss: 201.0168 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 201.0088 - acc: 1.0000 - val_loss: 191.5174 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 191.5087 - acc: 1.0000 - val_loss: 182.4290 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 182.4224 - acc: 1.0000 - val_loss: 173.6893 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 173.6842 - acc: 1.0000 - val_loss: 165.2818 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 165.2778 - acc: 1.0000 - val_loss: 157.2298 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 157.2266 - acc: 1.0000 - val_loss: 149.4214 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 149.4189 - acc: 1.0000 - val_loss: 142.0094 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 142.0071 - acc: 1.0000 - val_loss: 134.8719 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 134.8699 - acc: 1.0000 - val_loss: 128.0290 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 128.0274 - acc: 1.0000 - val_loss: 121.5175 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 121.5161 - acc: 1.0000 - val_loss: 115.1971 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 115.1958 - acc: 1.0000 - val_loss: 109.2141 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 722ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c433980c846444a9a9966653ea9ca53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 332.4581 - acc: 0.3871 - val_loss: 318.4397 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 318.4385 - acc: 0.5484 - val_loss: 304.8098 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 304.8002 - acc: 0.5484 - val_loss: 291.6168 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 291.6056 - acc: 0.5484 - val_loss: 278.9220 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 278.9109 - acc: 0.5484 - val_loss: 266.6201 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 266.5905 - acc: 0.6452 - val_loss: 254.7465 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 254.7294 - acc: 0.6452 - val_loss: 243.3070 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 243.2852 - acc: 0.8065 - val_loss: 232.2740 - val_acc: 0.8667\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 232.2577 - acc: 0.8710 - val_loss: 221.6764 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 221.6799 - acc: 0.9677 - val_loss: 211.4304 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 211.4108 - acc: 0.9677 - val_loss: 201.5601 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 201.5484 - acc: 1.0000 - val_loss: 192.0262 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 191.9944 - acc: 1.0000 - val_loss: 182.9007 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 182.8651 - acc: 1.0000 - val_loss: 174.1275 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 174.0855 - acc: 1.0000 - val_loss: 165.6902 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 165.6319 - acc: 1.0000 - val_loss: 157.6115 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 157.5579 - acc: 1.0000 - val_loss: 149.7655 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 149.7118 - acc: 1.0000 - val_loss: 142.3136 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 142.2619 - acc: 1.0000 - val_loss: 135.1428 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 135.0827 - acc: 1.0000 - val_loss: 128.2742 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 128.2139 - acc: 1.0000 - val_loss: 121.7367 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 121.6812 - acc: 1.0000 - val_loss: 115.3897 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 115.3336 - acc: 1.0000 - val_loss: 109.3682 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 109.3209 - acc: 1.0000 - val_loss: 103.5890 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 103.5465 - acc: 1.0000 - val_loss: 98.0589 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 98.0214 - acc: 1.0000 - val_loss: 92.8240 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 92.7904 - acc: 1.0000 - val_loss: 87.8044 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 87.7743 - acc: 1.0000 - val_loss: 83.0074 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 82.9804 - acc: 1.0000 - val_loss: 78.4451 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 932ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f62530b2b8e462aaf959ae806109115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 332.5738 - acc: 0.5484 - val_loss: 318.5529 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 318.5540 - acc: 0.5484 - val_loss: 304.8965 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 304.8961 - acc: 0.5484 - val_loss: 291.6681 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 617ms/step - loss: 291.6565 - acc: 0.6452 - val_loss: 278.9388 - val_acc: 0.6000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 278.9323 - acc: 0.7097 - val_loss: 266.6058 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 266.5617 - acc: 0.9677 - val_loss: 254.7047 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 254.7056 - acc: 0.9032 - val_loss: 243.2435 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 243.2166 - acc: 0.9355 - val_loss: 232.1827 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 232.1767 - acc: 0.9677 - val_loss: 221.5586 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 221.5940 - acc: 1.0000 - val_loss: 211.2942 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 211.2900 - acc: 1.0000 - val_loss: 201.3934 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 201.4081 - acc: 0.9677 - val_loss: 191.8359 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 191.8094 - acc: 1.0000 - val_loss: 182.6949 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 182.6731 - acc: 1.0000 - val_loss: 173.9111 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 173.8897 - acc: 1.0000 - val_loss: 165.4702 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 165.4260 - acc: 1.0000 - val_loss: 157.3816 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 157.3538 - acc: 1.0000 - val_loss: 149.5410 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 149.5193 - acc: 1.0000 - val_loss: 142.1061 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 142.0770 - acc: 1.0000 - val_loss: 134.9533 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 134.9260 - acc: 1.0000 - val_loss: 128.1047 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 128.0815 - acc: 1.0000 - val_loss: 121.5844 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 121.5655 - acc: 1.0000 - val_loss: 115.2529 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 115.2362 - acc: 1.0000 - val_loss: 109.2491 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 109.2343 - acc: 1.0000 - val_loss: 103.4818 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 103.4702 - acc: 1.0000 - val_loss: 97.9675 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 97.9567 - acc: 1.0000 - val_loss: 92.7506 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 92.7420 - acc: 1.0000 - val_loss: 87.7535 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 766ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d90db9188204e3fa3732828ee3f32b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 332.5667 - acc: 0.4194 - val_loss: 318.5391 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 680ms/step - loss: 318.5151 - acc: 0.5484 - val_loss: 304.8541 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 719ms/step - loss: 304.8600 - acc: 0.6129 - val_loss: 291.6058 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 510ms/step - loss: 291.6119 - acc: 0.8387 - val_loss: 278.8662 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 519ms/step - loss: 278.8795 - acc: 0.9677 - val_loss: 266.5211 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 266.5029 - acc: 0.9677 - val_loss: 254.6033 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 592ms/step - loss: 254.6134 - acc: 0.9677 - val_loss: 243.1244 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 243.1263 - acc: 0.9677 - val_loss: 232.0483 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 232.0571 - acc: 0.9677 - val_loss: 221.4125 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 221.4258 - acc: 1.0000 - val_loss: 211.1366 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 211.1274 - acc: 1.0000 - val_loss: 201.2398 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 201.2378 - acc: 1.0000 - val_loss: 191.6974 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 191.6792 - acc: 1.0000 - val_loss: 182.5789 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 182.5613 - acc: 1.0000 - val_loss: 173.8257 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 173.8091 - acc: 1.0000 - val_loss: 165.4143 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 165.3968 - acc: 1.0000 - val_loss: 157.3549 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 157.3436 - acc: 1.0000 - val_loss: 149.5344 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 149.5282 - acc: 1.0000 - val_loss: 142.1142 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 142.1072 - acc: 1.0000 - val_loss: 134.9639 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 134.9561 - acc: 1.0000 - val_loss: 128.1171 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 128.1113 - acc: 1.0000 - val_loss: 121.6021 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 121.5972 - acc: 1.0000 - val_loss: 115.2735 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 115.2695 - acc: 1.0000 - val_loss: 109.2736 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 109.2703 - acc: 1.0000 - val_loss: 103.5103 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 959ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3973b7fbe9c04fd88a2fb04a87319b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 332.4017 - acc: 0.3871 - val_loss: 318.3543 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 318.3342 - acc: 0.5484 - val_loss: 304.6268 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 714ms/step - loss: 304.6230 - acc: 0.7742 - val_loss: 291.3726 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 291.3939 - acc: 0.9032 - val_loss: 278.6189 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 278.6401 - acc: 1.0000 - val_loss: 266.2682 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 596ms/step - loss: 266.2313 - acc: 1.0000 - val_loss: 254.3614 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 254.3510 - acc: 0.9677 - val_loss: 242.8916 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 242.8632 - acc: 1.0000 - val_loss: 231.8291 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 231.8158 - acc: 1.0000 - val_loss: 221.2117 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 221.1901 - acc: 1.0000 - val_loss: 210.9582 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 210.9410 - acc: 1.0000 - val_loss: 201.0907 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 201.0799 - acc: 1.0000 - val_loss: 191.5791 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 191.5646 - acc: 1.0000 - val_loss: 182.4846 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 182.4714 - acc: 1.0000 - val_loss: 173.7394 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 173.7283 - acc: 1.0000 - val_loss: 165.3314 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 165.3212 - acc: 1.0000 - val_loss: 157.2824 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 157.2755 - acc: 1.0000 - val_loss: 149.4783 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 149.4739 - acc: 1.0000 - val_loss: 142.0629 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 142.0580 - acc: 1.0000 - val_loss: 134.9234 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 134.9189 - acc: 1.0000 - val_loss: 128.0766 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 128.0729 - acc: 1.0000 - val_loss: 121.5538 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 121.5509 - acc: 1.0000 - val_loss: 115.2201 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 115.2175 - acc: 1.0000 - val_loss: 109.2230 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 750ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98739021e674ceea306f5e58dea9409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 332.3990 - acc: 0.5161 - val_loss: 318.3849 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 788ms/step - loss: 318.3648 - acc: 0.5484 - val_loss: 304.6100 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 677ms/step - loss: 304.6346 - acc: 0.9355 - val_loss: 291.3552 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 291.3811 - acc: 0.8710 - val_loss: 278.5994 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 278.5948 - acc: 1.0000 - val_loss: 266.2806 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 266.2138 - acc: 0.9677 - val_loss: 254.3712 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 254.3403 - acc: 0.9677 - val_loss: 242.8895 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 242.8521 - acc: 1.0000 - val_loss: 231.8175 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 231.8059 - acc: 1.0000 - val_loss: 221.2019 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 221.1861 - acc: 1.0000 - val_loss: 210.9594 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 210.9490 - acc: 1.0000 - val_loss: 201.0939 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 201.0882 - acc: 1.0000 - val_loss: 191.5788 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 191.5705 - acc: 1.0000 - val_loss: 182.4832 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 182.4760 - acc: 1.0000 - val_loss: 173.7387 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 173.7325 - acc: 1.0000 - val_loss: 165.3331 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 165.3281 - acc: 1.0000 - val_loss: 157.2824 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 157.2787 - acc: 1.0000 - val_loss: 149.4689 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 149.4659 - acc: 1.0000 - val_loss: 142.0549 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 142.0524 - acc: 1.0000 - val_loss: 134.9202 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 134.9180 - acc: 1.0000 - val_loss: 128.0805 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 128.0786 - acc: 1.0000 - val_loss: 121.5606 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 121.5591 - acc: 1.0000 - val_loss: 115.2251 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 115.2238 - acc: 1.0000 - val_loss: 109.2103 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 109.2091 - acc: 1.0000 - val_loss: 103.4294 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 770ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2540d13ef843ebbf3a67a38520e75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 511.6541 - acc: 0.7419 - val_loss: 486.8661 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 731ms/step - loss: 486.8604 - acc: 0.5484 - val_loss: 462.8448 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 610ms/step - loss: 462.8366 - acc: 0.5484 - val_loss: 439.7193 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 593ms/step - loss: 439.7022 - acc: 0.5806 - val_loss: 417.6223 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 417.6154 - acc: 0.6129 - val_loss: 396.3373 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 579ms/step - loss: 396.3014 - acc: 0.7419 - val_loss: 375.9167 - val_acc: 0.8000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 375.9107 - acc: 0.7419 - val_loss: 356.3733 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 356.3410 - acc: 0.9032 - val_loss: 337.6382 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 337.6243 - acc: 0.9355 - val_loss: 319.7807 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 319.7823 - acc: 1.0000 - val_loss: 302.6361 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 302.6179 - acc: 1.0000 - val_loss: 286.2444 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 286.2281 - acc: 1.0000 - val_loss: 270.5393 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 270.5007 - acc: 1.0000 - val_loss: 255.6737 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 255.6319 - acc: 1.0000 - val_loss: 241.5042 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 241.4540 - acc: 1.0000 - val_loss: 227.9867 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 227.9159 - acc: 1.0000 - val_loss: 215.1537 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 215.0900 - acc: 1.0000 - val_loss: 202.7565 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 202.6977 - acc: 1.0000 - val_loss: 191.1275 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 191.0652 - acc: 1.0000 - val_loss: 180.0209 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 179.9581 - acc: 1.0000 - val_loss: 169.4701 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 169.4123 - acc: 1.0000 - val_loss: 159.5401 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 159.4892 - acc: 1.0000 - val_loss: 149.9448 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 149.8983 - acc: 1.0000 - val_loss: 140.9718 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 140.9307 - acc: 1.0000 - val_loss: 132.4234 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 132.3869 - acc: 1.0000 - val_loss: 124.3211 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 124.2880 - acc: 1.0000 - val_loss: 116.7710 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 116.7423 - acc: 1.0000 - val_loss: 109.5981 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 109.5720 - acc: 1.0000 - val_loss: 102.7868 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 766ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58624e69f84a4bc38942ba9dc6386e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 511.4467 - acc: 0.5806 - val_loss: 486.6439 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 792ms/step - loss: 486.6381 - acc: 0.5484 - val_loss: 462.6022 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 691ms/step - loss: 462.5889 - acc: 0.5484 - val_loss: 439.4618 - val_acc: 0.6000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 439.4410 - acc: 0.7742 - val_loss: 417.3409 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 417.3207 - acc: 0.9355 - val_loss: 396.0350 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 599ms/step - loss: 395.9871 - acc: 1.0000 - val_loss: 375.6025 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 375.5904 - acc: 0.9677 - val_loss: 356.0544 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 356.0254 - acc: 0.9677 - val_loss: 337.3224 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 550ms/step - loss: 337.2984 - acc: 0.9677 - val_loss: 319.4759 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 319.4618 - acc: 1.0000 - val_loss: 302.3497 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 302.3184 - acc: 1.0000 - val_loss: 285.9816 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 285.9432 - acc: 1.0000 - val_loss: 270.2871 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 270.2388 - acc: 1.0000 - val_loss: 255.4272 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 255.3764 - acc: 1.0000 - val_loss: 241.2634 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 241.2166 - acc: 1.0000 - val_loss: 227.7666 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 227.7186 - acc: 1.0000 - val_loss: 214.9596 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 214.9203 - acc: 1.0000 - val_loss: 202.5911 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 202.5627 - acc: 1.0000 - val_loss: 190.9867 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 190.9585 - acc: 1.0000 - val_loss: 179.9129 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 179.8877 - acc: 1.0000 - val_loss: 169.3903 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 169.3694 - acc: 1.0000 - val_loss: 159.4852 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 159.4678 - acc: 1.0000 - val_loss: 149.9057 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 149.8909 - acc: 1.0000 - val_loss: 140.9420 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 140.9292 - acc: 1.0000 - val_loss: 132.4118 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 132.4007 - acc: 1.0000 - val_loss: 124.3255 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 744ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852647365b8b4aac84c33102e19c15b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 511.2755 - acc: 0.5484 - val_loss: 486.4656 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 719ms/step - loss: 486.4514 - acc: 0.5484 - val_loss: 462.3803 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 694ms/step - loss: 462.3948 - acc: 0.7742 - val_loss: 439.2138 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 439.2060 - acc: 0.9677 - val_loss: 417.0811 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 417.0662 - acc: 0.9677 - val_loss: 395.7708 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 395.7126 - acc: 1.0000 - val_loss: 375.3252 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 375.3042 - acc: 0.9677 - val_loss: 355.7610 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 355.7270 - acc: 1.0000 - val_loss: 337.0139 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 336.9929 - acc: 1.0000 - val_loss: 319.1673 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 319.1479 - acc: 1.0000 - val_loss: 302.0567 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 302.0352 - acc: 1.0000 - val_loss: 285.7123 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 285.6902 - acc: 1.0000 - val_loss: 270.0541 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 270.0275 - acc: 1.0000 - val_loss: 255.2311 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 255.2077 - acc: 1.0000 - val_loss: 241.1031 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 241.0823 - acc: 1.0000 - val_loss: 227.6261 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 227.6076 - acc: 1.0000 - val_loss: 214.8326 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 214.8182 - acc: 1.0000 - val_loss: 202.4783 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 202.4674 - acc: 1.0000 - val_loss: 190.8891 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 190.8789 - acc: 1.0000 - val_loss: 179.8199 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 179.8112 - acc: 1.0000 - val_loss: 169.3081 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 169.3007 - acc: 1.0000 - val_loss: 159.4173 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 159.4109 - acc: 1.0000 - val_loss: 149.8550 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 149.8495 - acc: 1.0000 - val_loss: 140.9072 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 751ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17664c19d4945b88b8acfffe8dde28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 511.5605 - acc: 0.6452 - val_loss: 486.7233 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 722ms/step - loss: 486.7077 - acc: 0.5484 - val_loss: 462.6081 - val_acc: 1.0000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 681ms/step - loss: 462.6198 - acc: 0.9355 - val_loss: 439.4146 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 583ms/step - loss: 439.3983 - acc: 0.9355 - val_loss: 417.2624 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 417.2476 - acc: 0.9355 - val_loss: 395.9128 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 395.8489 - acc: 1.0000 - val_loss: 375.4308 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 375.4154 - acc: 1.0000 - val_loss: 355.8583 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 355.8394 - acc: 1.0000 - val_loss: 337.1235 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 337.1143 - acc: 1.0000 - val_loss: 319.2894 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 319.2778 - acc: 1.0000 - val_loss: 302.1851 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 302.1687 - acc: 1.0000 - val_loss: 285.8292 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 285.8161 - acc: 1.0000 - val_loss: 270.1628 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 270.1495 - acc: 1.0000 - val_loss: 255.3107 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 255.2995 - acc: 1.0000 - val_loss: 241.1536 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 241.1446 - acc: 1.0000 - val_loss: 227.6582 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 227.6506 - acc: 1.0000 - val_loss: 214.8503 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 214.8444 - acc: 1.0000 - val_loss: 202.4944 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 202.4902 - acc: 1.0000 - val_loss: 190.9074 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 190.9034 - acc: 1.0000 - val_loss: 179.8345 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 179.8312 - acc: 1.0000 - val_loss: 169.3196 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 169.3167 - acc: 1.0000 - val_loss: 159.4340 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 159.4317 - acc: 1.0000 - val_loss: 149.8688 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 779ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f5f68f2d374972acde3c6cdcc7fe5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 511.4420 - acc: 0.3871 - val_loss: 486.7245 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 991ms/step - loss: 486.6710 - acc: 0.5484 - val_loss: 462.4522 - val_acc: 1.0000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 719ms/step - loss: 462.4719 - acc: 1.0000 - val_loss: 439.3015 - val_acc: 0.8667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 593ms/step - loss: 439.3122 - acc: 0.7742 - val_loss: 417.1129 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 417.1005 - acc: 1.0000 - val_loss: 395.7901 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 395.7374 - acc: 1.0000 - val_loss: 375.3678 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 375.3195 - acc: 0.9677 - val_loss: 355.8218 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 355.7696 - acc: 1.0000 - val_loss: 337.0810 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 337.0415 - acc: 1.0000 - val_loss: 319.2274 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 319.1957 - acc: 1.0000 - val_loss: 302.1171 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 302.0976 - acc: 1.0000 - val_loss: 285.7641 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 514ms/step - loss: 285.7505 - acc: 1.0000 - val_loss: 270.1014 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 270.0922 - acc: 1.0000 - val_loss: 255.2539 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 255.2477 - acc: 1.0000 - val_loss: 241.1094 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 241.1051 - acc: 1.0000 - val_loss: 227.6288 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 227.6256 - acc: 1.0000 - val_loss: 214.8213 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 214.8192 - acc: 1.0000 - val_loss: 202.4551 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 202.4536 - acc: 1.0000 - val_loss: 190.8620 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 190.8607 - acc: 1.0000 - val_loss: 179.7912 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 179.7903 - acc: 1.0000 - val_loss: 169.2844 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 169.2837 - acc: 1.0000 - val_loss: 159.4020 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 159.4015 - acc: 1.0000 - val_loss: 149.8421 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 807ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d01865e452489c93360908a34e071a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 511.4317 - acc: 0.4516 - val_loss: 486.6335 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 809ms/step - loss: 486.6236 - acc: 0.5484 - val_loss: 462.6179 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 711ms/step - loss: 462.6096 - acc: 0.5484 - val_loss: 439.5038 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 680ms/step - loss: 439.4868 - acc: 0.5484 - val_loss: 417.4047 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 576ms/step - loss: 417.3911 - acc: 0.5806 - val_loss: 396.1141 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 396.0768 - acc: 0.7742 - val_loss: 375.6985 - val_acc: 0.6000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 375.6884 - acc: 0.7419 - val_loss: 356.1740 - val_acc: 0.8667\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 356.1538 - acc: 0.8710 - val_loss: 337.4665 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 337.4561 - acc: 0.9355 - val_loss: 319.6451 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 319.6655 - acc: 0.9677 - val_loss: 302.5445 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 302.5383 - acc: 0.9677 - val_loss: 286.1875 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 286.1825 - acc: 0.9677 - val_loss: 270.5029 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 270.4833 - acc: 1.0000 - val_loss: 255.6380 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 255.6221 - acc: 1.0000 - val_loss: 241.4579 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 241.4271 - acc: 1.0000 - val_loss: 227.9434 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 227.8918 - acc: 1.0000 - val_loss: 215.1201 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 215.0824 - acc: 1.0000 - val_loss: 202.7280 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 202.6883 - acc: 1.0000 - val_loss: 191.0974 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 191.0545 - acc: 1.0000 - val_loss: 179.9936 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 179.9468 - acc: 1.0000 - val_loss: 169.4448 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 169.4016 - acc: 1.0000 - val_loss: 159.5224 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 159.4838 - acc: 1.0000 - val_loss: 149.9341 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 149.8966 - acc: 1.0000 - val_loss: 140.9722 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 140.9399 - acc: 1.0000 - val_loss: 132.4367 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 132.4070 - acc: 1.0000 - val_loss: 124.3540 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 124.3266 - acc: 1.0000 - val_loss: 116.8151 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 116.7914 - acc: 1.0000 - val_loss: 109.6552 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 109.6343 - acc: 1.0000 - val_loss: 102.8446 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 102.8244 - acc: 1.0000 - val_loss: 96.4376 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 810ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41cb186c27d41b497bad129c86f0a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 511.1757 - acc: 0.4194 - val_loss: 486.3673 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 776ms/step - loss: 486.3552 - acc: 0.5484 - val_loss: 462.3478 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 733ms/step - loss: 462.3389 - acc: 0.5484 - val_loss: 439.2172 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 439.2009 - acc: 0.7097 - val_loss: 417.1068 - val_acc: 0.8000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 417.1055 - acc: 0.7742 - val_loss: 395.8161 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 473ms/step - loss: 395.7741 - acc: 0.9677 - val_loss: 375.4032 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 375.3948 - acc: 0.9355 - val_loss: 355.8812 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 355.8664 - acc: 0.9355 - val_loss: 337.1752 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 337.1732 - acc: 0.9355 - val_loss: 319.3454 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 473ms/step - loss: 319.3642 - acc: 0.9677 - val_loss: 302.2325 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 302.2214 - acc: 1.0000 - val_loss: 285.8714 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 285.8580 - acc: 1.0000 - val_loss: 270.1836 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 270.1629 - acc: 1.0000 - val_loss: 255.3264 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 255.3005 - acc: 1.0000 - val_loss: 241.1643 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 241.1373 - acc: 1.0000 - val_loss: 227.6632 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 227.6230 - acc: 1.0000 - val_loss: 214.8574 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 214.8318 - acc: 1.0000 - val_loss: 202.5018 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 202.4846 - acc: 1.0000 - val_loss: 190.9243 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 190.9011 - acc: 1.0000 - val_loss: 179.8605 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 179.8388 - acc: 1.0000 - val_loss: 169.3438 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 169.3268 - acc: 1.0000 - val_loss: 159.4522 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 159.4384 - acc: 1.0000 - val_loss: 149.8864 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 149.8751 - acc: 1.0000 - val_loss: 140.9259 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 140.9160 - acc: 1.0000 - val_loss: 132.3970 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 132.3879 - acc: 1.0000 - val_loss: 124.3238 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 124.3156 - acc: 1.0000 - val_loss: 116.7928 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 770ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91f35c8db9d48c8bd428721384c51df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 511.4805 - acc: 0.4516 - val_loss: 486.6686 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 816ms/step - loss: 486.6568 - acc: 0.5484 - val_loss: 462.5913 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 622ms/step - loss: 462.5803 - acc: 0.7419 - val_loss: 439.4231 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 439.4278 - acc: 0.9677 - val_loss: 417.2850 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 604ms/step - loss: 417.2872 - acc: 0.9355 - val_loss: 395.9690 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 395.9329 - acc: 1.0000 - val_loss: 375.5292 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 375.5402 - acc: 1.0000 - val_loss: 355.9874 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 355.9843 - acc: 0.9032 - val_loss: 337.2576 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 337.2578 - acc: 0.9677 - val_loss: 319.4097 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 319.4141 - acc: 1.0000 - val_loss: 302.2800 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 302.2761 - acc: 1.0000 - val_loss: 285.9035 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 285.8961 - acc: 1.0000 - val_loss: 270.2145 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 270.1983 - acc: 1.0000 - val_loss: 255.3574 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 255.3402 - acc: 1.0000 - val_loss: 241.1934 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 241.1793 - acc: 1.0000 - val_loss: 227.6932 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 227.6761 - acc: 1.0000 - val_loss: 214.8743 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 214.8654 - acc: 1.0000 - val_loss: 202.5083 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 202.5024 - acc: 1.0000 - val_loss: 190.9141 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 190.9069 - acc: 1.0000 - val_loss: 179.8423 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 179.8358 - acc: 1.0000 - val_loss: 169.3317 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 169.3262 - acc: 1.0000 - val_loss: 159.4355 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 159.4313 - acc: 1.0000 - val_loss: 149.8647 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 149.8609 - acc: 1.0000 - val_loss: 140.9182 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 797ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d1b45e79ed41648e7319dd857456aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 511.4553 - acc: 0.4839 - val_loss: 486.6505 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 704ms/step - loss: 486.6173 - acc: 0.5484 - val_loss: 462.5158 - val_acc: 1.0000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 462.5265 - acc: 0.9355 - val_loss: 439.3372 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 439.3503 - acc: 0.9032 - val_loss: 417.1783 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 417.1746 - acc: 0.9677 - val_loss: 395.8409 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 670ms/step - loss: 395.7883 - acc: 1.0000 - val_loss: 375.3790 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 375.3733 - acc: 0.9677 - val_loss: 355.8101 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 355.8122 - acc: 1.0000 - val_loss: 337.0682 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 337.0817 - acc: 1.0000 - val_loss: 319.2281 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 319.2324 - acc: 1.0000 - val_loss: 302.1134 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 302.1108 - acc: 1.0000 - val_loss: 285.7612 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 285.7560 - acc: 1.0000 - val_loss: 270.0999 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 270.0907 - acc: 1.0000 - val_loss: 255.2713 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 255.2626 - acc: 1.0000 - val_loss: 241.1402 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 241.1321 - acc: 1.0000 - val_loss: 227.6573 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 227.6501 - acc: 1.0000 - val_loss: 214.8508 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 214.8463 - acc: 1.0000 - val_loss: 202.4939 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 493ms/step - loss: 202.4917 - acc: 1.0000 - val_loss: 190.9117 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 190.9088 - acc: 1.0000 - val_loss: 179.8461 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 179.8430 - acc: 1.0000 - val_loss: 169.3333 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 169.3308 - acc: 1.0000 - val_loss: 159.4279 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 159.4259 - acc: 1.0000 - val_loss: 149.8505 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 801ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cfaf396a004fa49df5b5b9f1314c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 511.2351 - acc: 0.5484 - val_loss: 486.4635 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 968ms/step - loss: 486.4282 - acc: 0.5484 - val_loss: 462.2745 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 462.2811 - acc: 0.8387 - val_loss: 439.0793 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 677ms/step - loss: 439.0971 - acc: 0.8387 - val_loss: 416.9111 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 416.8750 - acc: 1.0000 - val_loss: 395.6206 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 662ms/step - loss: 395.5445 - acc: 1.0000 - val_loss: 375.1864 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 375.1494 - acc: 0.9677 - val_loss: 355.6354 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 497ms/step - loss: 355.6175 - acc: 0.9677 - val_loss: 336.9271 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 336.9230 - acc: 1.0000 - val_loss: 319.1137 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 319.1035 - acc: 1.0000 - val_loss: 302.0345 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 302.0299 - acc: 1.0000 - val_loss: 285.7111 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 285.7074 - acc: 1.0000 - val_loss: 270.0740 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 480ms/step - loss: 270.0687 - acc: 1.0000 - val_loss: 255.2581 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 255.2532 - acc: 1.0000 - val_loss: 241.1418 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 241.1380 - acc: 1.0000 - val_loss: 227.6703 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 227.6673 - acc: 1.0000 - val_loss: 214.8780 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 214.8759 - acc: 1.0000 - val_loss: 202.5361 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 202.5346 - acc: 1.0000 - val_loss: 190.9556 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 190.9542 - acc: 1.0000 - val_loss: 179.8885 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 179.8874 - acc: 1.0000 - val_loss: 169.3690 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 169.3680 - acc: 1.0000 - val_loss: 159.4715 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 159.4707 - acc: 1.0000 - val_loss: 149.8996 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 149.8989 - acc: 1.0000 - val_loss: 140.9352 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 140.9346 - acc: 1.0000 - val_loss: 132.4023 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 834ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a8516937a643b4a86f91c3ee27ed53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 714.4105 - acc: 0.4839 - val_loss: 675.8151 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 739ms/step - loss: 675.8104 - acc: 0.5484 - val_loss: 638.5835 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 673ms/step - loss: 638.5759 - acc: 0.5484 - val_loss: 602.9236 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 602.9091 - acc: 0.5484 - val_loss: 569.0475 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 503ms/step - loss: 569.0333 - acc: 0.6452 - val_loss: 536.5853 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 536.5421 - acc: 0.8065 - val_loss: 505.6254 - val_acc: 0.6000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 686ms/step - loss: 505.6103 - acc: 0.8387 - val_loss: 476.2065 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 476.1823 - acc: 0.9032 - val_loss: 448.1966 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 448.1825 - acc: 0.9355 - val_loss: 421.7108 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 421.7088 - acc: 1.0000 - val_loss: 396.4600 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 396.4398 - acc: 1.0000 - val_loss: 372.4941 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 372.4676 - acc: 1.0000 - val_loss: 349.6656 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 349.6163 - acc: 1.0000 - val_loss: 328.2374 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 328.1859 - acc: 1.0000 - val_loss: 307.9828 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 307.9236 - acc: 1.0000 - val_loss: 288.8083 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 288.7346 - acc: 1.0000 - val_loss: 270.7619 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 270.6922 - acc: 1.0000 - val_loss: 253.4174 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 253.3545 - acc: 1.0000 - val_loss: 237.3450 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 237.2824 - acc: 1.0000 - val_loss: 222.0890 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 222.0254 - acc: 1.0000 - val_loss: 207.7207 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 207.6643 - acc: 1.0000 - val_loss: 194.3395 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 194.2887 - acc: 1.0000 - val_loss: 181.4239 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 181.3792 - acc: 1.0000 - val_loss: 169.4789 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 169.4402 - acc: 1.0000 - val_loss: 158.1850 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 158.1506 - acc: 1.0000 - val_loss: 147.5954 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 147.5647 - acc: 1.0000 - val_loss: 137.8533 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 137.8263 - acc: 1.0000 - val_loss: 128.6714 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 128.6472 - acc: 1.0000 - val_loss: 120.0061 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 119.9845 - acc: 1.0000 - val_loss: 111.9424 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 111.9229 - acc: 1.0000 - val_loss: 104.4204 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 802ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc91f819cc5e4e9fb638d6ea7646ae2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 714.5026 - acc: 0.4839 - val_loss: 675.9103 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 787ms/step - loss: 675.8958 - acc: 0.5484 - val_loss: 638.6528 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 755ms/step - loss: 638.6298 - acc: 0.5806 - val_loss: 602.9472 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 602.9420 - acc: 0.7742 - val_loss: 569.0349 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 569.0392 - acc: 0.9355 - val_loss: 536.5502 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 712ms/step - loss: 536.5067 - acc: 1.0000 - val_loss: 505.5799 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 505.5697 - acc: 1.0000 - val_loss: 476.1558 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 476.1323 - acc: 1.0000 - val_loss: 448.1407 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 448.1216 - acc: 1.0000 - val_loss: 421.6489 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 421.6242 - acc: 1.0000 - val_loss: 396.3805 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 396.3437 - acc: 1.0000 - val_loss: 372.3868 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 372.3523 - acc: 1.0000 - val_loss: 349.5297 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 349.4853 - acc: 1.0000 - val_loss: 328.0734 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 328.0301 - acc: 1.0000 - val_loss: 307.7824 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 307.7444 - acc: 1.0000 - val_loss: 288.5923 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 288.5545 - acc: 1.0000 - val_loss: 270.5365 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 270.5056 - acc: 1.0000 - val_loss: 253.1912 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 253.1671 - acc: 1.0000 - val_loss: 237.1209 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 237.1000 - acc: 1.0000 - val_loss: 221.8794 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 221.8612 - acc: 1.0000 - val_loss: 207.5352 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 207.5206 - acc: 1.0000 - val_loss: 194.1796 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 194.1674 - acc: 1.0000 - val_loss: 181.2834 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 181.2730 - acc: 1.0000 - val_loss: 169.3621 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 169.3533 - acc: 1.0000 - val_loss: 158.0967 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 821ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4846628151604e8bac3b0d7b5b81856f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 714.2352 - acc: 0.6452 - val_loss: 675.6418 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 826ms/step - loss: 675.6242 - acc: 0.5484 - val_loss: 638.3402 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 768ms/step - loss: 638.3250 - acc: 0.8387 - val_loss: 602.6353 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 643ms/step - loss: 602.6442 - acc: 0.9677 - val_loss: 568.7148 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 568.7099 - acc: 0.9677 - val_loss: 536.2159 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 536.1492 - acc: 1.0000 - val_loss: 505.2403 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 505.2167 - acc: 1.0000 - val_loss: 475.8064 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 602ms/step - loss: 475.7672 - acc: 1.0000 - val_loss: 447.8065 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 447.7872 - acc: 1.0000 - val_loss: 421.3401 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 574ms/step - loss: 421.3092 - acc: 1.0000 - val_loss: 396.1049 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 396.0803 - acc: 1.0000 - val_loss: 372.1545 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 372.1311 - acc: 1.0000 - val_loss: 349.3410 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 349.3196 - acc: 1.0000 - val_loss: 327.9216 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 327.9029 - acc: 1.0000 - val_loss: 307.6583 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 307.6427 - acc: 1.0000 - val_loss: 288.4839 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 288.4704 - acc: 1.0000 - val_loss: 270.4532 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 270.4427 - acc: 1.0000 - val_loss: 253.1342 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 253.1261 - acc: 1.0000 - val_loss: 237.0895 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 237.0827 - acc: 1.0000 - val_loss: 221.8703 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 221.8645 - acc: 1.0000 - val_loss: 207.5440 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 207.5391 - acc: 1.0000 - val_loss: 194.2011 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 194.1969 - acc: 1.0000 - val_loss: 181.3148 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 181.3111 - acc: 1.0000 - val_loss: 169.4124 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 875ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba29b67838ab4a33abde8f3010873a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 714.5789 - acc: 0.4839 - val_loss: 675.9916 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 970ms/step - loss: 675.9691 - acc: 0.5484 - val_loss: 638.6323 - val_acc: 1.0000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 810ms/step - loss: 638.6247 - acc: 0.9355 - val_loss: 602.9109 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 674ms/step - loss: 602.9141 - acc: 1.0000 - val_loss: 568.9853 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 755ms/step - loss: 568.9580 - acc: 1.0000 - val_loss: 536.4786 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 487ms/step - loss: 536.3896 - acc: 1.0000 - val_loss: 505.4824 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 505.4417 - acc: 1.0000 - val_loss: 476.0334 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 475.9904 - acc: 1.0000 - val_loss: 448.0111 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 447.9849 - acc: 1.0000 - val_loss: 421.5271 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 421.5003 - acc: 1.0000 - val_loss: 396.2794 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 396.2589 - acc: 1.0000 - val_loss: 372.3081 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 372.2899 - acc: 1.0000 - val_loss: 349.4665 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 349.4518 - acc: 1.0000 - val_loss: 328.0242 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 328.0125 - acc: 1.0000 - val_loss: 307.7426 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 485ms/step - loss: 307.7336 - acc: 1.0000 - val_loss: 288.5601 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 288.5527 - acc: 1.0000 - val_loss: 270.5119 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 270.5060 - acc: 1.0000 - val_loss: 253.1603 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 253.1556 - acc: 1.0000 - val_loss: 237.0928 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 237.0888 - acc: 1.0000 - val_loss: 221.8546 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 221.8512 - acc: 1.0000 - val_loss: 207.5147 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 207.5118 - acc: 1.0000 - val_loss: 194.1776 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 194.1751 - acc: 1.0000 - val_loss: 181.2971 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 879ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605b4699f5de4584801bd99b946c9022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 714.3252 - acc: 0.4516 - val_loss: 675.7163 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 675.6967 - acc: 0.5484 - val_loss: 638.3231 - val_acc: 0.8000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 917ms/step - loss: 638.3497 - acc: 0.7097 - val_loss: 602.5441 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 729ms/step - loss: 602.5557 - acc: 0.9677 - val_loss: 568.6346 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 756ms/step - loss: 568.6040 - acc: 0.9355 - val_loss: 536.1323 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 682ms/step - loss: 536.0458 - acc: 1.0000 - val_loss: 505.1281 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 505.1002 - acc: 0.9677 - val_loss: 475.7095 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 620ms/step - loss: 475.6869 - acc: 1.0000 - val_loss: 447.7307 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 447.7277 - acc: 1.0000 - val_loss: 421.2761 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 421.2635 - acc: 1.0000 - val_loss: 396.0465 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 396.0385 - acc: 1.0000 - val_loss: 372.0983 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 372.0926 - acc: 1.0000 - val_loss: 349.2943 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 349.2894 - acc: 1.0000 - val_loss: 327.8895 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 327.8858 - acc: 1.0000 - val_loss: 307.6368 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 307.6343 - acc: 1.0000 - val_loss: 288.4763 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 288.4742 - acc: 1.0000 - val_loss: 270.4516 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 270.4502 - acc: 1.0000 - val_loss: 253.1451 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 253.1439 - acc: 1.0000 - val_loss: 237.1099 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 237.1089 - acc: 1.0000 - val_loss: 221.8844 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 221.8835 - acc: 1.0000 - val_loss: 207.5382 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 207.5375 - acc: 1.0000 - val_loss: 194.1785 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 194.1779 - acc: 1.0000 - val_loss: 181.2812 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 181.2806 - acc: 1.0000 - val_loss: 169.3730 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 878ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf1b911997d4a0d982201c29b5d4d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 714.3359 - acc: 0.5806 - val_loss: 675.7526 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 945ms/step - loss: 675.7415 - acc: 0.5484 - val_loss: 638.5374 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 538ms/step - loss: 638.5305 - acc: 0.5484 - val_loss: 602.8844 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 703ms/step - loss: 602.8759 - acc: 0.5484 - val_loss: 569.0109 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 695ms/step - loss: 569.0054 - acc: 0.5806 - val_loss: 536.5580 - val_acc: 0.6000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 754ms/step - loss: 536.5206 - acc: 0.7419 - val_loss: 505.6389 - val_acc: 0.6000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 722ms/step - loss: 505.6355 - acc: 0.8065 - val_loss: 476.2643 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 476.2396 - acc: 0.8710 - val_loss: 448.2830 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 448.2805 - acc: 0.8710 - val_loss: 421.8113 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 421.8089 - acc: 0.9677 - val_loss: 396.5596 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 396.5402 - acc: 0.9677 - val_loss: 372.5828 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 372.5727 - acc: 1.0000 - val_loss: 349.7235 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 349.6920 - acc: 1.0000 - val_loss: 328.2649 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 328.2296 - acc: 1.0000 - val_loss: 307.9640 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 307.9219 - acc: 1.0000 - val_loss: 288.7589 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 288.7029 - acc: 1.0000 - val_loss: 270.6985 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 270.6507 - acc: 1.0000 - val_loss: 253.3408 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 253.2958 - acc: 1.0000 - val_loss: 237.2617 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 237.2167 - acc: 1.0000 - val_loss: 222.0132 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 221.9619 - acc: 1.0000 - val_loss: 207.6540 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 207.6079 - acc: 1.0000 - val_loss: 194.2789 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 194.2372 - acc: 1.0000 - val_loss: 181.3878 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 181.3500 - acc: 1.0000 - val_loss: 169.4795 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 169.4502 - acc: 1.0000 - val_loss: 158.2279 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 158.2001 - acc: 1.0000 - val_loss: 147.6458 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 147.6220 - acc: 1.0000 - val_loss: 137.9077 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 137.8869 - acc: 1.0000 - val_loss: 128.7263 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 128.7089 - acc: 1.0000 - val_loss: 120.0642 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 844ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0744ea8422d4cfaa3cb13655426e9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 714.1532 - acc: 0.7097 - val_loss: 675.5665 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 675.5696 - acc: 0.5484 - val_loss: 638.3376 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 770ms/step - loss: 638.3337 - acc: 0.5484 - val_loss: 602.6896 - val_acc: 0.7333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 758ms/step - loss: 602.6697 - acc: 0.8065 - val_loss: 568.8175 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 624ms/step - loss: 568.7990 - acc: 0.9355 - val_loss: 536.3671 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 782ms/step - loss: 536.3180 - acc: 0.9677 - val_loss: 505.4228 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 505.4263 - acc: 0.9355 - val_loss: 476.0281 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 476.0193 - acc: 0.9032 - val_loss: 448.0389 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 448.0367 - acc: 0.8710 - val_loss: 421.5657 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 421.5806 - acc: 1.0000 - val_loss: 396.3094 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 396.3161 - acc: 1.0000 - val_loss: 372.3227 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 372.3250 - acc: 0.9677 - val_loss: 349.4711 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 349.4527 - acc: 0.9677 - val_loss: 328.0143 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 327.9903 - acc: 1.0000 - val_loss: 307.7224 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 307.6937 - acc: 1.0000 - val_loss: 288.5247 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 288.4861 - acc: 1.0000 - val_loss: 270.4718 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 270.4406 - acc: 1.0000 - val_loss: 253.1387 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 253.1151 - acc: 1.0000 - val_loss: 237.0931 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 237.0703 - acc: 1.0000 - val_loss: 221.8681 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 221.8453 - acc: 1.0000 - val_loss: 207.5302 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 207.5116 - acc: 1.0000 - val_loss: 194.1789 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 194.1637 - acc: 1.0000 - val_loss: 181.3122 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 181.2991 - acc: 1.0000 - val_loss: 169.4247 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 169.4138 - acc: 1.0000 - val_loss: 158.1915 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 884ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173afadc29b5429daffb6f78b32f325e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 714.2191 - acc: 0.5161 - val_loss: 675.6033 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 677ms/step - loss: 675.5852 - acc: 0.5484 - val_loss: 638.3226 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 767ms/step - loss: 638.3279 - acc: 0.8065 - val_loss: 602.6099 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 714ms/step - loss: 602.6248 - acc: 0.8387 - val_loss: 568.6860 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 568.7039 - acc: 0.9677 - val_loss: 536.1906 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 536.1374 - acc: 1.0000 - val_loss: 505.2331 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 706ms/step - loss: 505.2655 - acc: 0.9355 - val_loss: 475.8223 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 704ms/step - loss: 475.8273 - acc: 0.9677 - val_loss: 447.8152 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 447.8411 - acc: 0.9677 - val_loss: 421.3381 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 421.3410 - acc: 1.0000 - val_loss: 396.0935 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 396.0970 - acc: 1.0000 - val_loss: 372.1394 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 372.1397 - acc: 1.0000 - val_loss: 349.3396 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 349.3273 - acc: 1.0000 - val_loss: 327.9241 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 327.9126 - acc: 1.0000 - val_loss: 307.6718 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 307.6611 - acc: 1.0000 - val_loss: 288.5052 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 288.4941 - acc: 1.0000 - val_loss: 270.4749 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 270.4669 - acc: 1.0000 - val_loss: 253.1559 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 253.1517 - acc: 1.0000 - val_loss: 237.1042 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 237.0991 - acc: 1.0000 - val_loss: 221.8723 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 221.8679 - acc: 1.0000 - val_loss: 207.5320 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 207.5284 - acc: 1.0000 - val_loss: 194.1873 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 194.1843 - acc: 1.0000 - val_loss: 181.3141 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 181.3116 - acc: 1.0000 - val_loss: 169.4157 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 892ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4944230cec1642c29cd83e72a91490ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 714.3062 - acc: 0.5484 - val_loss: 675.6776 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 675.6600 - acc: 0.5806 - val_loss: 638.3445 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 992ms/step - loss: 638.3624 - acc: 0.8710 - val_loss: 602.6038 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 586ms/step - loss: 602.6086 - acc: 0.9355 - val_loss: 568.6814 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 710ms/step - loss: 568.6686 - acc: 0.9355 - val_loss: 536.1712 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 536.1021 - acc: 1.0000 - val_loss: 505.1862 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 692ms/step - loss: 505.1851 - acc: 0.9677 - val_loss: 475.7573 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 475.7551 - acc: 1.0000 - val_loss: 447.7496 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 495ms/step - loss: 447.7578 - acc: 1.0000 - val_loss: 421.2803 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 587ms/step - loss: 421.2702 - acc: 1.0000 - val_loss: 396.0480 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 396.0360 - acc: 1.0000 - val_loss: 372.0891 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 372.0778 - acc: 1.0000 - val_loss: 349.2679 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 349.2589 - acc: 1.0000 - val_loss: 327.8349 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 327.8269 - acc: 1.0000 - val_loss: 307.5606 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 479ms/step - loss: 307.5534 - acc: 1.0000 - val_loss: 288.3739 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 522ms/step - loss: 288.3679 - acc: 1.0000 - val_loss: 270.3473 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 488ms/step - loss: 270.3433 - acc: 1.0000 - val_loss: 253.0523 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 253.0500 - acc: 1.0000 - val_loss: 237.0363 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 237.0338 - acc: 1.0000 - val_loss: 221.8428 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 221.8406 - acc: 1.0000 - val_loss: 207.5160 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 207.5142 - acc: 1.0000 - val_loss: 194.1701 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 194.1686 - acc: 1.0000 - val_loss: 181.2903 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 181.2891 - acc: 1.0000 - val_loss: 169.3832 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 913ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e670b4f0d4e4b459ba74fdb25d2976b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 714.4495 - acc: 0.5161 - val_loss: 675.9959 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 675.9658 - acc: 0.5484 - val_loss: 638.4797 - val_acc: 1.0000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 638.5005 - acc: 0.9677 - val_loss: 602.7811 - val_acc: 0.8667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 925ms/step - loss: 602.8027 - acc: 0.7097 - val_loss: 568.8130 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 831ms/step - loss: 568.8228 - acc: 0.9677 - val_loss: 536.3225 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 845ms/step - loss: 536.2613 - acc: 1.0000 - val_loss: 505.3885 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 833ms/step - loss: 505.3646 - acc: 0.9677 - val_loss: 475.9742 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 537ms/step - loss: 475.9430 - acc: 1.0000 - val_loss: 447.9513 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 447.9298 - acc: 1.0000 - val_loss: 421.4507 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 421.4273 - acc: 1.0000 - val_loss: 396.1956 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 498ms/step - loss: 396.1830 - acc: 1.0000 - val_loss: 372.2289 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 372.2220 - acc: 1.0000 - val_loss: 349.4182 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 349.4118 - acc: 1.0000 - val_loss: 327.9892 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 327.9846 - acc: 1.0000 - val_loss: 307.7176 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 307.7145 - acc: 1.0000 - val_loss: 288.5331 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 288.5307 - acc: 1.0000 - val_loss: 270.4925 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 270.4910 - acc: 1.0000 - val_loss: 253.1541 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 253.1530 - acc: 1.0000 - val_loss: 237.0807 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 237.0798 - acc: 1.0000 - val_loss: 221.8216 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 221.8208 - acc: 1.0000 - val_loss: 207.4699 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 207.4693 - acc: 1.0000 - val_loss: 194.1265 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 194.1261 - acc: 1.0000 - val_loss: 181.2542 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 956ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5841891e83e49f49593d2b53649008a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 32.6477 - acc: 0.3226 - val_loss: 31.8380 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 871ms/step - loss: 31.8353 - acc: 0.6452 - val_loss: 31.0429 - val_acc: 0.7333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 793ms/step - loss: 31.0335 - acc: 0.7742 - val_loss: 30.2620 - val_acc: 0.7333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 706ms/step - loss: 30.2549 - acc: 0.8065 - val_loss: 29.4979 - val_acc: 0.8667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 694ms/step - loss: 29.4858 - acc: 0.8065 - val_loss: 28.7458 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 28.7230 - acc: 0.9032 - val_loss: 28.0062 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 27.9936 - acc: 0.9032 - val_loss: 27.2794 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 862ms/step - loss: 27.2614 - acc: 0.9677 - val_loss: 26.5644 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 26.5441 - acc: 0.9677 - val_loss: 25.8626 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 25.8347 - acc: 0.9677 - val_loss: 25.1719 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 25.1424 - acc: 0.9677 - val_loss: 24.4934 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 24.4538 - acc: 1.0000 - val_loss: 23.8261 - val_acc: 0.9333\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 23.7867 - acc: 1.0000 - val_loss: 23.1727 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 23.1215 - acc: 1.0000 - val_loss: 22.5308 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 22.4770 - acc: 1.0000 - val_loss: 21.9010 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 21.8398 - acc: 1.0000 - val_loss: 21.2834 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 21.2224 - acc: 1.0000 - val_loss: 20.6766 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 20.6211 - acc: 1.0000 - val_loss: 20.0855 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 20.0275 - acc: 1.0000 - val_loss: 19.5053 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 19.4466 - acc: 1.0000 - val_loss: 18.9371 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 18.8793 - acc: 1.0000 - val_loss: 18.3852 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 18.3278 - acc: 1.0000 - val_loss: 17.8444 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 17.7901 - acc: 1.0000 - val_loss: 17.3181 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 17.2670 - acc: 1.0000 - val_loss: 16.8040 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 16.7580 - acc: 1.0000 - val_loss: 16.3021 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 16.2558 - acc: 1.0000 - val_loss: 15.8130 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 15.7690 - acc: 1.0000 - val_loss: 15.3358 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 15.2997 - acc: 1.0000 - val_loss: 14.8702 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 14.8317 - acc: 1.0000 - val_loss: 14.4174 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 14.3860 - acc: 1.0000 - val_loss: 13.9758 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 13.9455 - acc: 1.0000 - val_loss: 13.5437 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 13.5149 - acc: 1.0000 - val_loss: 13.1234 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 13.0989 - acc: 1.0000 - val_loss: 12.7143 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 864ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4c6626327741d380b2e0bfb804f200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 32.5836 - acc: 0.4839 - val_loss: 31.7617 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 793ms/step - loss: 31.7672 - acc: 0.8065 - val_loss: 30.9608 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 781ms/step - loss: 30.9517 - acc: 0.9355 - val_loss: 30.1751 - val_acc: 0.8667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 782ms/step - loss: 30.1670 - acc: 0.9355 - val_loss: 29.4051 - val_acc: 0.8667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 871ms/step - loss: 29.4059 - acc: 0.8387 - val_loss: 28.6473 - val_acc: 0.8667\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 28.6297 - acc: 0.9355 - val_loss: 27.9014 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 27.8899 - acc: 0.9355 - val_loss: 27.1694 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 27.1483 - acc: 0.9677 - val_loss: 26.4503 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 26.4243 - acc: 0.9677 - val_loss: 25.7428 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 25.7124 - acc: 1.0000 - val_loss: 25.0458 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 25.0026 - acc: 1.0000 - val_loss: 24.3600 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 24.3309 - acc: 1.0000 - val_loss: 23.6864 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 23.6371 - acc: 1.0000 - val_loss: 23.0263 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 22.9757 - acc: 1.0000 - val_loss: 22.3792 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 22.3312 - acc: 1.0000 - val_loss: 21.7473 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 21.6911 - acc: 1.0000 - val_loss: 21.1292 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 21.0759 - acc: 1.0000 - val_loss: 20.5227 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 20.4718 - acc: 1.0000 - val_loss: 19.9333 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 19.8846 - acc: 1.0000 - val_loss: 19.3579 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 19.3158 - acc: 1.0000 - val_loss: 18.7953 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 18.7579 - acc: 1.0000 - val_loss: 18.2478 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 18.2154 - acc: 1.0000 - val_loss: 17.7128 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 17.6825 - acc: 1.0000 - val_loss: 17.1926 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 17.1688 - acc: 1.0000 - val_loss: 16.6863 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 16.6650 - acc: 1.0000 - val_loss: 16.1924 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 16.1727 - acc: 1.0000 - val_loss: 15.7115 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 15.6945 - acc: 1.0000 - val_loss: 15.2406 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 15.2252 - acc: 1.0000 - val_loss: 14.7801 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 14.7657 - acc: 1.0000 - val_loss: 14.3305 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 14.3178 - acc: 1.0000 - val_loss: 13.8912 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 857ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c26e3d93e424df890e9f5287a68ecc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 32.6141 - acc: 0.5484 - val_loss: 31.7805 - val_acc: 0.7333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 31.7705 - acc: 0.8387 - val_loss: 30.9550 - val_acc: 0.8000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 747ms/step - loss: 30.9268 - acc: 0.8387 - val_loss: 30.1453 - val_acc: 0.8667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 826ms/step - loss: 30.1184 - acc: 0.9355 - val_loss: 29.3508 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 814ms/step - loss: 29.3225 - acc: 0.9677 - val_loss: 28.5689 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 603ms/step - loss: 28.5113 - acc: 0.9355 - val_loss: 27.8005 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 27.7592 - acc: 1.0000 - val_loss: 27.0458 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 26.9852 - acc: 1.0000 - val_loss: 26.3065 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 493ms/step - loss: 26.2479 - acc: 1.0000 - val_loss: 25.5834 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 25.5409 - acc: 1.0000 - val_loss: 24.8761 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 24.8217 - acc: 1.0000 - val_loss: 24.1869 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 24.1488 - acc: 1.0000 - val_loss: 23.5155 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 23.4764 - acc: 1.0000 - val_loss: 22.8642 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 22.8319 - acc: 1.0000 - val_loss: 22.2300 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 22.2005 - acc: 1.0000 - val_loss: 21.6131 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 21.5837 - acc: 1.0000 - val_loss: 21.0130 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 20.9916 - acc: 1.0000 - val_loss: 20.4241 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 20.4045 - acc: 1.0000 - val_loss: 19.8507 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 19.8341 - acc: 1.0000 - val_loss: 19.2902 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 19.2770 - acc: 1.0000 - val_loss: 18.7427 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 18.7323 - acc: 1.0000 - val_loss: 18.2074 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 18.1984 - acc: 1.0000 - val_loss: 17.6801 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 17.6720 - acc: 1.0000 - val_loss: 17.1649 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 17.1587 - acc: 1.0000 - val_loss: 16.6608 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 16.6552 - acc: 1.0000 - val_loss: 16.1678 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 16.1625 - acc: 1.0000 - val_loss: 15.6891 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 727ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e454d6fea934f459c0974dda085a71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 32.6186 - acc: 0.5484 - val_loss: 31.7510 - val_acc: 0.8667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 31.7427 - acc: 0.8710 - val_loss: 30.9115 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 830ms/step - loss: 30.8777 - acc: 0.9032 - val_loss: 30.0891 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 30.0562 - acc: 0.9355 - val_loss: 29.2812 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 755ms/step - loss: 29.2555 - acc: 0.9355 - val_loss: 28.4861 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 28.4327 - acc: 1.0000 - val_loss: 27.7080 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 27.6823 - acc: 1.0000 - val_loss: 26.9485 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 26.9169 - acc: 1.0000 - val_loss: 26.2079 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 26.1685 - acc: 1.0000 - val_loss: 25.4912 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 25.4596 - acc: 1.0000 - val_loss: 24.7952 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 574ms/step - loss: 24.7622 - acc: 1.0000 - val_loss: 24.1187 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 24.0951 - acc: 1.0000 - val_loss: 23.4598 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 23.4349 - acc: 1.0000 - val_loss: 22.8207 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 22.8014 - acc: 1.0000 - val_loss: 22.1978 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 22.1839 - acc: 1.0000 - val_loss: 21.5907 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 21.5761 - acc: 1.0000 - val_loss: 20.9991 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 20.9894 - acc: 1.0000 - val_loss: 20.4166 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 20.4090 - acc: 1.0000 - val_loss: 19.8476 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 19.8408 - acc: 1.0000 - val_loss: 19.2890 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 19.2831 - acc: 1.0000 - val_loss: 18.7428 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 18.7382 - acc: 1.0000 - val_loss: 18.2094 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 18.2057 - acc: 1.0000 - val_loss: 17.6848 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 17.6816 - acc: 1.0000 - val_loss: 17.1717 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 17.1694 - acc: 1.0000 - val_loss: 16.6679 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 16.6661 - acc: 1.0000 - val_loss: 16.1756 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 855ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbd01f2f00f41bbbb2f41a4156f53c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 32.6604 - acc: 0.2581 - val_loss: 31.7944 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 948ms/step - loss: 31.7759 - acc: 0.7742 - val_loss: 30.9474 - val_acc: 0.7333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 720ms/step - loss: 30.9044 - acc: 0.8387 - val_loss: 30.1146 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 832ms/step - loss: 30.0697 - acc: 0.9677 - val_loss: 29.2972 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 875ms/step - loss: 29.2585 - acc: 0.9355 - val_loss: 28.4945 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 602ms/step - loss: 28.4297 - acc: 0.9677 - val_loss: 27.7095 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 27.6751 - acc: 1.0000 - val_loss: 26.9453 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 26.9065 - acc: 1.0000 - val_loss: 26.2037 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 468ms/step - loss: 26.1666 - acc: 1.0000 - val_loss: 25.4865 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 25.4584 - acc: 1.0000 - val_loss: 24.7930 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 24.7647 - acc: 1.0000 - val_loss: 24.1201 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 24.1010 - acc: 1.0000 - val_loss: 23.4657 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 23.4475 - acc: 1.0000 - val_loss: 22.8297 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 22.8166 - acc: 1.0000 - val_loss: 22.2091 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 22.2002 - acc: 1.0000 - val_loss: 21.6033 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 21.5930 - acc: 1.0000 - val_loss: 21.0112 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 21.0053 - acc: 1.0000 - val_loss: 20.4295 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 20.4244 - acc: 1.0000 - val_loss: 19.8615 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 19.8571 - acc: 1.0000 - val_loss: 19.3043 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 19.3005 - acc: 1.0000 - val_loss: 18.7577 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 18.7548 - acc: 1.0000 - val_loss: 18.2227 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 18.2204 - acc: 1.0000 - val_loss: 17.6978 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 17.6959 - acc: 1.0000 - val_loss: 17.1848 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 17.1831 - acc: 1.0000 - val_loss: 16.6825 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 16.6811 - acc: 1.0000 - val_loss: 16.1896 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 897ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7cd9f3789d483e8d94eab25f37a94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 32.7373 - acc: 0.3871 - val_loss: 31.9364 - val_acc: 0.6000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 31.9317 - acc: 0.6129 - val_loss: 31.1479 - val_acc: 0.6000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 965ms/step - loss: 31.1502 - acc: 0.6452 - val_loss: 30.3740 - val_acc: 0.6000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 754ms/step - loss: 30.3678 - acc: 0.7097 - val_loss: 29.6160 - val_acc: 0.6667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 493ms/step - loss: 29.6035 - acc: 0.7419 - val_loss: 28.8715 - val_acc: 0.6667\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 28.8562 - acc: 0.7419 - val_loss: 28.1391 - val_acc: 0.7333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 28.1222 - acc: 0.7419 - val_loss: 27.4197 - val_acc: 0.7333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 27.4053 - acc: 0.9032 - val_loss: 26.7126 - val_acc: 0.7333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 26.7025 - acc: 0.8710 - val_loss: 26.0186 - val_acc: 0.7333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 454ms/step - loss: 26.0002 - acc: 0.8387 - val_loss: 25.3359 - val_acc: 0.8667\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 468ms/step - loss: 25.3130 - acc: 0.9355 - val_loss: 24.6640 - val_acc: 0.8667\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1077s 1077s/step - loss: 24.6477 - acc: 0.9355 - val_loss: 24.0020 - val_acc: 0.9333\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 23.9727 - acc: 0.9355 - val_loss: 23.3522 - val_acc: 0.9333\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 23.3181 - acc: 0.9355 - val_loss: 22.7140 - val_acc: 0.9333\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 22.6812 - acc: 0.9677 - val_loss: 22.0873 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 22.0398 - acc: 0.9677 - val_loss: 21.4709 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 21.4411 - acc: 0.9677 - val_loss: 20.8618 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 20.8292 - acc: 0.9677 - val_loss: 20.2660 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 20.2216 - acc: 1.0000 - val_loss: 19.6808 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 19.6376 - acc: 1.0000 - val_loss: 19.1071 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 19.0617 - acc: 1.0000 - val_loss: 18.5450 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 18.4886 - acc: 1.0000 - val_loss: 17.9928 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 17.9412 - acc: 1.0000 - val_loss: 17.4559 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 17.4045 - acc: 1.0000 - val_loss: 16.9307 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 16.8871 - acc: 1.0000 - val_loss: 16.4158 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 16.3668 - acc: 1.0000 - val_loss: 15.9128 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 15.8676 - acc: 1.0000 - val_loss: 15.4204 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 15.3799 - acc: 1.0000 - val_loss: 14.9412 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 14.8990 - acc: 1.0000 - val_loss: 14.4758 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 14.4381 - acc: 1.0000 - val_loss: 14.0228 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 13.9903 - acc: 1.0000 - val_loss: 13.5793 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 13.5454 - acc: 1.0000 - val_loss: 13.1477 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 13.1215 - acc: 1.0000 - val_loss: 12.7263 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 12.6994 - acc: 1.0000 - val_loss: 12.3163 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 12.2923 - acc: 1.0000 - val_loss: 11.9187 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 871ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c865e02884a842e7b5a28f6c81f0edf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 672s 672s/step - loss: 32.5581 - acc: 0.5806 - val_loss: 31.7504 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 864ms/step - loss: 31.7561 - acc: 0.7097 - val_loss: 30.9490 - val_acc: 0.8000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 30.9483 - acc: 0.8387 - val_loss: 30.1634 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 740ms/step - loss: 30.1592 - acc: 0.8065 - val_loss: 29.3932 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 786ms/step - loss: 29.3943 - acc: 0.8387 - val_loss: 28.6348 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 28.6189 - acc: 0.9677 - val_loss: 27.8893 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 27.8719 - acc: 0.9677 - val_loss: 27.1564 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 27.1404 - acc: 0.9677 - val_loss: 26.4347 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 820ms/step - loss: 26.4230 - acc: 0.9677 - val_loss: 25.7267 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 885ms/step - loss: 25.7108 - acc: 1.0000 - val_loss: 25.0291 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 25.0125 - acc: 0.9677 - val_loss: 24.3427 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 24.3255 - acc: 1.0000 - val_loss: 23.6691 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 23.6435 - acc: 1.0000 - val_loss: 23.0108 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 22.9832 - acc: 1.0000 - val_loss: 22.3652 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 22.3348 - acc: 1.0000 - val_loss: 21.7326 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 21.6923 - acc: 1.0000 - val_loss: 21.1141 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 21.0848 - acc: 1.0000 - val_loss: 20.5071 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 20.4715 - acc: 1.0000 - val_loss: 19.9166 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 19.8879 - acc: 1.0000 - val_loss: 19.3407 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 19.3136 - acc: 1.0000 - val_loss: 18.7807 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 18.7533 - acc: 1.0000 - val_loss: 18.2369 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 18.2131 - acc: 1.0000 - val_loss: 17.7057 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 17.6851 - acc: 1.0000 - val_loss: 17.1893 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 17.1717 - acc: 1.0000 - val_loss: 16.6848 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 16.6701 - acc: 1.0000 - val_loss: 16.1925 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 16.1789 - acc: 1.0000 - val_loss: 15.7131 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 15.6992 - acc: 1.0000 - val_loss: 15.2443 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 15.2329 - acc: 1.0000 - val_loss: 14.7861 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 14.7753 - acc: 1.0000 - val_loss: 14.3385 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 891ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32d7fc18baa44b29eaf09aece2161ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 32.6882 - acc: 0.5484 - val_loss: 31.8510 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 704ms/step - loss: 31.8410 - acc: 0.8065 - val_loss: 31.0255 - val_acc: 0.8000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 31.0259 - acc: 0.8065 - val_loss: 30.2151 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 802ms/step - loss: 30.2107 - acc: 0.9032 - val_loss: 29.4211 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 29.4196 - acc: 0.9032 - val_loss: 28.6401 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 28.6293 - acc: 0.9677 - val_loss: 27.8728 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 890ms/step - loss: 27.8583 - acc: 0.9677 - val_loss: 27.1210 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 763ms/step - loss: 27.1070 - acc: 1.0000 - val_loss: 26.3839 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 26.3703 - acc: 1.0000 - val_loss: 25.6614 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 503ms/step - loss: 25.6586 - acc: 1.0000 - val_loss: 24.9543 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 24.9449 - acc: 1.0000 - val_loss: 24.2642 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 24.2548 - acc: 1.0000 - val_loss: 23.5924 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 23.5810 - acc: 1.0000 - val_loss: 22.9387 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 22.9294 - acc: 1.0000 - val_loss: 22.3015 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 22.2912 - acc: 1.0000 - val_loss: 21.6818 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 21.6699 - acc: 1.0000 - val_loss: 21.0783 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 21.0671 - acc: 1.0000 - val_loss: 20.4877 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 20.4786 - acc: 1.0000 - val_loss: 19.9139 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 19.9074 - acc: 1.0000 - val_loss: 19.3524 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 19.3456 - acc: 1.0000 - val_loss: 18.8014 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 18.7964 - acc: 1.0000 - val_loss: 18.2622 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 18.2584 - acc: 1.0000 - val_loss: 17.7320 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 17.7280 - acc: 1.0000 - val_loss: 17.2150 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 17.2123 - acc: 1.0000 - val_loss: 16.7086 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 16.7060 - acc: 1.0000 - val_loss: 16.2142 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 925ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c835686f5832497c9fb9e47b92d9e0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 32.6545 - acc: 0.5161 - val_loss: 31.8068 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 725ms/step - loss: 31.7922 - acc: 0.7419 - val_loss: 30.9684 - val_acc: 0.7333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 30.9554 - acc: 0.8387 - val_loss: 30.1438 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 983ms/step - loss: 30.1250 - acc: 0.9032 - val_loss: 29.3345 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 841ms/step - loss: 29.3137 - acc: 0.9677 - val_loss: 28.5394 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 28.5100 - acc: 0.9677 - val_loss: 27.7609 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 507ms/step - loss: 27.7348 - acc: 1.0000 - val_loss: 27.0011 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 26.9743 - acc: 1.0000 - val_loss: 26.2589 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 26.2353 - acc: 1.0000 - val_loss: 25.5390 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 25.5182 - acc: 1.0000 - val_loss: 24.8395 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 24.8158 - acc: 1.0000 - val_loss: 24.1605 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 24.1420 - acc: 1.0000 - val_loss: 23.4993 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 752ms/step - loss: 23.4821 - acc: 1.0000 - val_loss: 22.8571 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 22.8452 - acc: 1.0000 - val_loss: 22.2308 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 608ms/step - loss: 22.2203 - acc: 1.0000 - val_loss: 21.6196 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 21.6062 - acc: 1.0000 - val_loss: 21.0233 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 21.0143 - acc: 1.0000 - val_loss: 20.4380 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 20.4308 - acc: 1.0000 - val_loss: 19.8684 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 19.8615 - acc: 1.0000 - val_loss: 19.3096 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 19.3046 - acc: 1.0000 - val_loss: 18.7628 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 18.7592 - acc: 1.0000 - val_loss: 18.2301 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 18.2268 - acc: 1.0000 - val_loss: 17.7064 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 17.7038 - acc: 1.0000 - val_loss: 17.1939 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 17.1920 - acc: 1.0000 - val_loss: 16.6914 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 897ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e444583dc6493a808c2c567bfcbd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 32.5670 - acc: 0.6452 - val_loss: 31.7191 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 31.6998 - acc: 0.8710 - val_loss: 30.8782 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 30.8570 - acc: 0.8710 - val_loss: 30.0525 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 891ms/step - loss: 30.0281 - acc: 0.9032 - val_loss: 29.2410 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 854ms/step - loss: 29.2111 - acc: 0.9677 - val_loss: 28.4427 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 28.3899 - acc: 1.0000 - val_loss: 27.6608 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 27.6208 - acc: 0.9677 - val_loss: 26.8997 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 26.8602 - acc: 1.0000 - val_loss: 26.1582 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 26.1278 - acc: 1.0000 - val_loss: 25.4409 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 820ms/step - loss: 25.4012 - acc: 1.0000 - val_loss: 24.7431 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 24.7166 - acc: 1.0000 - val_loss: 24.0646 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 24.0396 - acc: 1.0000 - val_loss: 23.4059 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 695ms/step - loss: 23.3890 - acc: 1.0000 - val_loss: 22.7664 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 22.7540 - acc: 1.0000 - val_loss: 22.1449 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 22.1331 - acc: 1.0000 - val_loss: 21.5389 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 21.5308 - acc: 1.0000 - val_loss: 20.9475 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 20.9402 - acc: 1.0000 - val_loss: 20.3668 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 20.3613 - acc: 1.0000 - val_loss: 19.8008 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 19.7954 - acc: 1.0000 - val_loss: 19.2458 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 19.2417 - acc: 1.0000 - val_loss: 18.7024 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 18.6995 - acc: 1.0000 - val_loss: 18.1716 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 18.1690 - acc: 1.0000 - val_loss: 17.6492 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 17.6471 - acc: 1.0000 - val_loss: 17.1401 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 17.1384 - acc: 1.0000 - val_loss: 16.6414 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 16.6399 - acc: 1.0000 - val_loss: 16.1538 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 772ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083ce133dc094de7a14f2708a33ca6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 90.9321 - acc: 0.5484 - val_loss: 87.7875 - val_acc: 0.6000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 87.7865 - acc: 0.6452 - val_loss: 84.7093 - val_acc: 0.6667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 84.7029 - acc: 0.7097 - val_loss: 81.7115 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 81.7047 - acc: 0.7742 - val_loss: 78.8050 - val_acc: 0.7333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 593ms/step - loss: 78.7937 - acc: 0.7097 - val_loss: 75.9702 - val_acc: 0.7333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 75.9552 - acc: 0.8710 - val_loss: 73.2133 - val_acc: 0.8000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 790ms/step - loss: 73.2013 - acc: 0.8710 - val_loss: 70.5356 - val_acc: 0.8000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 605ms/step - loss: 70.5143 - acc: 0.9355 - val_loss: 67.9303 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 67.9081 - acc: 0.9677 - val_loss: 65.4049 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 65.3884 - acc: 1.0000 - val_loss: 62.9467 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 62.9193 - acc: 0.9677 - val_loss: 60.5540 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 60.5231 - acc: 1.0000 - val_loss: 58.2208 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 58.1781 - acc: 1.0000 - val_loss: 55.9620 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 55.9192 - acc: 1.0000 - val_loss: 53.7682 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 53.7190 - acc: 1.0000 - val_loss: 51.6414 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 51.5870 - acc: 1.0000 - val_loss: 49.5826 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 49.5319 - acc: 1.0000 - val_loss: 47.5739 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 47.5210 - acc: 1.0000 - val_loss: 45.6426 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 45.5900 - acc: 1.0000 - val_loss: 43.7711 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 43.7172 - acc: 1.0000 - val_loss: 41.9603 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 41.9111 - acc: 1.0000 - val_loss: 40.2132 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 40.1625 - acc: 1.0000 - val_loss: 38.5140 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 38.4685 - acc: 1.0000 - val_loss: 36.8800 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 36.8396 - acc: 1.0000 - val_loss: 35.2994 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 35.2621 - acc: 1.0000 - val_loss: 33.7709 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 33.7378 - acc: 1.0000 - val_loss: 32.3047 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 32.2743 - acc: 1.0000 - val_loss: 30.8906 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 30.8633 - acc: 1.0000 - val_loss: 29.5262 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 29.5020 - acc: 1.0000 - val_loss: 28.2129 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 28.1917 - acc: 1.0000 - val_loss: 26.9463 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 26.9279 - acc: 1.0000 - val_loss: 25.7191 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 25.7020 - acc: 1.0000 - val_loss: 24.5431 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 940ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae2fbd7230c469f9b250c8c4a9395ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 90.9464 - acc: 0.3871 - val_loss: 87.7916 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 87.7872 - acc: 0.9032 - val_loss: 84.7051 - val_acc: 0.7333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 84.6985 - acc: 0.7419 - val_loss: 81.6970 - val_acc: 0.7333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 734ms/step - loss: 81.6830 - acc: 0.8710 - val_loss: 78.7805 - val_acc: 0.7333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 816ms/step - loss: 78.7635 - acc: 0.8710 - val_loss: 75.9347 - val_acc: 0.8667\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 753ms/step - loss: 75.9042 - acc: 0.9355 - val_loss: 73.1629 - val_acc: 0.8667\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 73.1387 - acc: 0.9355 - val_loss: 70.4692 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 698ms/step - loss: 70.4295 - acc: 0.9677 - val_loss: 67.8479 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 67.8076 - acc: 0.9677 - val_loss: 65.3069 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 65.2653 - acc: 1.0000 - val_loss: 62.8327 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 62.7725 - acc: 1.0000 - val_loss: 60.4263 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 60.3683 - acc: 1.0000 - val_loss: 58.0836 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 481ms/step - loss: 58.0167 - acc: 1.0000 - val_loss: 55.8201 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 468ms/step - loss: 55.7522 - acc: 1.0000 - val_loss: 53.6289 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 53.5648 - acc: 1.0000 - val_loss: 51.5033 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 51.4403 - acc: 1.0000 - val_loss: 49.4500 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 49.3917 - acc: 1.0000 - val_loss: 47.4525 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 47.4007 - acc: 1.0000 - val_loss: 45.5353 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 45.4897 - acc: 1.0000 - val_loss: 43.6759 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 43.6362 - acc: 1.0000 - val_loss: 41.8793 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 41.8460 - acc: 1.0000 - val_loss: 40.1499 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 40.1209 - acc: 1.0000 - val_loss: 38.4602 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 38.4351 - acc: 1.0000 - val_loss: 36.8361 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 36.8152 - acc: 1.0000 - val_loss: 35.2626 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 35.2449 - acc: 1.0000 - val_loss: 33.7422 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 33.7269 - acc: 1.0000 - val_loss: 32.2814 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 32.2680 - acc: 1.0000 - val_loss: 30.8673 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 30.8556 - acc: 1.0000 - val_loss: 29.5008 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 29.4903 - acc: 1.0000 - val_loss: 28.1839 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 28.1746 - acc: 1.0000 - val_loss: 26.9174 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 26.9092 - acc: 1.0000 - val_loss: 25.6903 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 977ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ca564c86524a34ae2acadb6e533721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 91.0777 - acc: 0.4516 - val_loss: 87.8741 - val_acc: 0.7333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 87.8655 - acc: 0.8387 - val_loss: 84.7549 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 983ms/step - loss: 84.7327 - acc: 0.9355 - val_loss: 81.7164 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 959ms/step - loss: 81.6881 - acc: 0.9677 - val_loss: 78.7656 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 866ms/step - loss: 78.7347 - acc: 0.9355 - val_loss: 75.8832 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 809ms/step - loss: 75.8350 - acc: 1.0000 - val_loss: 73.0789 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 766ms/step - loss: 73.0389 - acc: 1.0000 - val_loss: 70.3602 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 70.3073 - acc: 1.0000 - val_loss: 67.7226 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 67.6752 - acc: 1.0000 - val_loss: 65.1729 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 65.1332 - acc: 1.0000 - val_loss: 62.6943 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 62.6574 - acc: 1.0000 - val_loss: 60.2934 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 468ms/step - loss: 60.2599 - acc: 1.0000 - val_loss: 57.9632 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 57.9338 - acc: 1.0000 - val_loss: 55.7141 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 55.6904 - acc: 1.0000 - val_loss: 53.5363 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 633ms/step - loss: 53.5170 - acc: 1.0000 - val_loss: 51.4283 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 51.4129 - acc: 1.0000 - val_loss: 49.3925 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 49.3801 - acc: 1.0000 - val_loss: 47.4108 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 47.4009 - acc: 1.0000 - val_loss: 45.5031 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 45.4951 - acc: 1.0000 - val_loss: 43.6513 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 43.6450 - acc: 1.0000 - val_loss: 41.8573 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 41.8519 - acc: 1.0000 - val_loss: 40.1272 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 40.1227 - acc: 1.0000 - val_loss: 38.4427 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 38.4388 - acc: 1.0000 - val_loss: 36.8243 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 36.8210 - acc: 1.0000 - val_loss: 35.2550 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 35.2521 - acc: 1.0000 - val_loss: 33.7380 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 947ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65f2e53e9074fcfbe74bf6438024177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 90.8704 - acc: 0.2581 - val_loss: 87.6415 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 87.6327 - acc: 0.9032 - val_loss: 84.5010 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 84.4925 - acc: 0.9677 - val_loss: 81.4430 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 977ms/step - loss: 81.4203 - acc: 0.9677 - val_loss: 78.4755 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 78.4461 - acc: 0.9677 - val_loss: 75.5863 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 988ms/step - loss: 75.5448 - acc: 1.0000 - val_loss: 72.7845 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 72.7524 - acc: 1.0000 - val_loss: 70.0747 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 70.0387 - acc: 1.0000 - val_loss: 67.4521 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 512ms/step - loss: 67.4258 - acc: 1.0000 - val_loss: 64.9193 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 64.8929 - acc: 1.0000 - val_loss: 62.4615 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 62.4390 - acc: 1.0000 - val_loss: 60.0832 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 60.0652 - acc: 1.0000 - val_loss: 57.7771 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 57.7616 - acc: 1.0000 - val_loss: 55.5538 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 55.5415 - acc: 1.0000 - val_loss: 53.3997 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 53.3904 - acc: 1.0000 - val_loss: 51.3094 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 51.3020 - acc: 1.0000 - val_loss: 49.2874 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 49.2818 - acc: 1.0000 - val_loss: 47.3149 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 47.3103 - acc: 1.0000 - val_loss: 45.4193 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 45.4156 - acc: 1.0000 - val_loss: 43.5782 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 43.5753 - acc: 1.0000 - val_loss: 41.7935 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 41.7911 - acc: 1.0000 - val_loss: 40.0724 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 40.0702 - acc: 1.0000 - val_loss: 38.3946 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 38.3928 - acc: 1.0000 - val_loss: 36.7815 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 36.7799 - acc: 1.0000 - val_loss: 35.2211 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab3e656575e41eaae06d06c7403f5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 90.9764 - acc: 0.4839 - val_loss: 87.7291 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 87.7136 - acc: 0.8710 - val_loss: 84.5668 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 84.5374 - acc: 0.9355 - val_loss: 81.4896 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 81.4592 - acc: 1.0000 - val_loss: 78.5089 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 78.4799 - acc: 1.0000 - val_loss: 75.6176 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 75.5682 - acc: 1.0000 - val_loss: 72.8207 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 782ms/step - loss: 72.7941 - acc: 1.0000 - val_loss: 70.1194 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 923ms/step - loss: 70.0869 - acc: 1.0000 - val_loss: 67.5063 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 67.4858 - acc: 1.0000 - val_loss: 64.9822 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 64.9621 - acc: 1.0000 - val_loss: 62.5328 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 62.5172 - acc: 1.0000 - val_loss: 60.1598 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 60.1487 - acc: 1.0000 - val_loss: 57.8573 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 57.8480 - acc: 1.0000 - val_loss: 55.6311 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 55.6239 - acc: 1.0000 - val_loss: 53.4731 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 53.4678 - acc: 1.0000 - val_loss: 51.3799 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 51.3756 - acc: 1.0000 - val_loss: 49.3573 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 49.3540 - acc: 1.0000 - val_loss: 47.3857 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 47.3829 - acc: 1.0000 - val_loss: 45.4847 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 45.4825 - acc: 1.0000 - val_loss: 43.6355 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 43.6336 - acc: 1.0000 - val_loss: 41.8448 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 41.8431 - acc: 1.0000 - val_loss: 40.1192 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 40.1178 - acc: 1.0000 - val_loss: 38.4340 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 38.4327 - acc: 1.0000 - val_loss: 36.8128 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 985ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9577e7a173b544adb979a128245b25df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 90.9324 - acc: 0.4839 - val_loss: 87.7900 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 87.7849 - acc: 0.7419 - val_loss: 84.7159 - val_acc: 0.6667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 84.7081 - acc: 0.7419 - val_loss: 81.7194 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 81.7061 - acc: 0.7742 - val_loss: 78.8149 - val_acc: 0.6667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 834ms/step - loss: 78.8030 - acc: 0.7742 - val_loss: 75.9857 - val_acc: 0.7333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 75.9651 - acc: 0.8065 - val_loss: 73.2307 - val_acc: 0.7333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 73.2071 - acc: 0.8387 - val_loss: 70.5552 - val_acc: 0.8000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 802ms/step - loss: 70.5261 - acc: 0.9677 - val_loss: 67.9523 - val_acc: 0.8667\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 628ms/step - loss: 67.9170 - acc: 0.9032 - val_loss: 65.4271 - val_acc: 0.8667\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 65.4031 - acc: 0.9677 - val_loss: 62.9651 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 62.9300 - acc: 0.9677 - val_loss: 60.5709 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 783ms/step - loss: 60.5369 - acc: 0.9677 - val_loss: 58.2403 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 58.1979 - acc: 1.0000 - val_loss: 55.9885 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 55.9394 - acc: 1.0000 - val_loss: 53.8048 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 53.7557 - acc: 1.0000 - val_loss: 51.6828 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 51.6192 - acc: 1.0000 - val_loss: 49.6262 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 509ms/step - loss: 49.5688 - acc: 1.0000 - val_loss: 47.6168 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 47.5599 - acc: 1.0000 - val_loss: 45.6866 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 45.6335 - acc: 1.0000 - val_loss: 43.8128 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 43.7590 - acc: 1.0000 - val_loss: 41.9992 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 41.9488 - acc: 1.0000 - val_loss: 40.2509 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 40.1951 - acc: 1.0000 - val_loss: 38.5478 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 38.4981 - acc: 1.0000 - val_loss: 36.9092 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 36.8682 - acc: 1.0000 - val_loss: 35.3225 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 35.2829 - acc: 1.0000 - val_loss: 33.7943 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 33.7620 - acc: 1.0000 - val_loss: 32.3288 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 32.2954 - acc: 1.0000 - val_loss: 30.9119 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 30.8817 - acc: 1.0000 - val_loss: 29.5441 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 29.5186 - acc: 1.0000 - val_loss: 28.2269 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 28.2033 - acc: 1.0000 - val_loss: 26.9604 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 26.9392 - acc: 1.0000 - val_loss: 25.7305 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 987ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43854ca870ac4bd4b6f7df3d14cde487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 90.9817 - acc: 0.5806 - val_loss: 87.8224 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 87.8224 - acc: 0.6774 - val_loss: 84.7315 - val_acc: 0.6667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 84.7229 - acc: 0.7419 - val_loss: 81.7218 - val_acc: 0.7333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 851ms/step - loss: 81.7110 - acc: 0.8710 - val_loss: 78.8036 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 769ms/step - loss: 78.7923 - acc: 0.9355 - val_loss: 75.9584 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 780ms/step - loss: 75.9365 - acc: 0.9355 - val_loss: 73.1884 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 73.1621 - acc: 0.9677 - val_loss: 70.4977 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 70.4661 - acc: 1.0000 - val_loss: 67.8755 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 67.8478 - acc: 0.9677 - val_loss: 65.3328 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 65.3005 - acc: 1.0000 - val_loss: 62.8550 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 62.8172 - acc: 1.0000 - val_loss: 60.4455 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 643ms/step - loss: 60.4087 - acc: 1.0000 - val_loss: 58.1047 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 499ms/step - loss: 58.0596 - acc: 1.0000 - val_loss: 55.8445 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 55.7984 - acc: 1.0000 - val_loss: 53.6539 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 53.6149 - acc: 1.0000 - val_loss: 51.5301 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 51.4883 - acc: 1.0000 - val_loss: 49.4791 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 49.4432 - acc: 1.0000 - val_loss: 47.4802 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 47.4518 - acc: 1.0000 - val_loss: 45.5571 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 45.5312 - acc: 1.0000 - val_loss: 43.6921 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 43.6667 - acc: 1.0000 - val_loss: 41.8874 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 41.8660 - acc: 1.0000 - val_loss: 40.1515 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 40.1330 - acc: 1.0000 - val_loss: 38.4617 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 38.4457 - acc: 1.0000 - val_loss: 36.8382 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 36.8257 - acc: 1.0000 - val_loss: 35.2634 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 35.2524 - acc: 1.0000 - val_loss: 33.7456 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 33.7350 - acc: 1.0000 - val_loss: 32.2891 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 955ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f0e94652724ec78d809661f6f10908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 91.0817 - acc: 0.5806 - val_loss: 87.8926 - val_acc: 0.7333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 87.8956 - acc: 0.8065 - val_loss: 84.7697 - val_acc: 0.8000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 735ms/step - loss: 84.7565 - acc: 0.8710 - val_loss: 81.7294 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 945ms/step - loss: 81.7154 - acc: 0.9355 - val_loss: 78.7797 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 78.7607 - acc: 0.9677 - val_loss: 75.9007 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 75.8646 - acc: 1.0000 - val_loss: 73.0996 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 73.0834 - acc: 1.0000 - val_loss: 70.3789 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 70.3544 - acc: 1.0000 - val_loss: 67.7350 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 67.7113 - acc: 1.0000 - val_loss: 65.1784 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 65.1543 - acc: 1.0000 - val_loss: 62.6962 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 621ms/step - loss: 62.6788 - acc: 1.0000 - val_loss: 60.2936 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 60.2753 - acc: 1.0000 - val_loss: 57.9630 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 57.9454 - acc: 1.0000 - val_loss: 55.7184 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 545ms/step - loss: 55.7000 - acc: 1.0000 - val_loss: 53.5458 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 53.5295 - acc: 1.0000 - val_loss: 51.4391 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 51.4247 - acc: 1.0000 - val_loss: 49.3982 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 49.3885 - acc: 1.0000 - val_loss: 47.4128 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 47.4030 - acc: 1.0000 - val_loss: 45.5045 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 45.4966 - acc: 1.0000 - val_loss: 43.6534 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 43.6475 - acc: 1.0000 - val_loss: 41.8621 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 41.8568 - acc: 1.0000 - val_loss: 40.1340 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 40.1292 - acc: 1.0000 - val_loss: 38.4461 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 38.4422 - acc: 1.0000 - val_loss: 36.8201 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 36.8171 - acc: 1.0000 - val_loss: 35.2459 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 35.2429 - acc: 1.0000 - val_loss: 33.7244 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 980ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce87dc6789124b36a0cd1ecab1790271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 90.9286 - acc: 0.6129 - val_loss: 87.7239 - val_acc: 0.6000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 87.7091 - acc: 0.8065 - val_loss: 84.5945 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 84.5709 - acc: 0.9032 - val_loss: 81.5488 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 999ms/step - loss: 81.5275 - acc: 0.9677 - val_loss: 78.5944 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 776ms/step - loss: 78.5705 - acc: 0.9677 - val_loss: 75.7130 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 537ms/step - loss: 75.6732 - acc: 1.0000 - val_loss: 72.9115 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 72.8812 - acc: 1.0000 - val_loss: 70.1970 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 596ms/step - loss: 70.1565 - acc: 1.0000 - val_loss: 67.5688 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 67.5389 - acc: 1.0000 - val_loss: 65.0297 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 487ms/step - loss: 65.0028 - acc: 1.0000 - val_loss: 62.5670 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 606ms/step - loss: 62.5470 - acc: 1.0000 - val_loss: 60.1808 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 60.1618 - acc: 1.0000 - val_loss: 57.8660 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 57.8479 - acc: 1.0000 - val_loss: 55.6323 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 55.6159 - acc: 1.0000 - val_loss: 53.4669 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 53.4558 - acc: 1.0000 - val_loss: 51.3693 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 51.3595 - acc: 1.0000 - val_loss: 49.3397 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 49.3328 - acc: 1.0000 - val_loss: 47.3605 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 47.3551 - acc: 1.0000 - val_loss: 45.4553 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 45.4507 - acc: 1.0000 - val_loss: 43.6065 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 43.6029 - acc: 1.0000 - val_loss: 41.8185 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 41.8156 - acc: 1.0000 - val_loss: 40.0935 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 40.0911 - acc: 1.0000 - val_loss: 38.4090 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 38.4069 - acc: 1.0000 - val_loss: 36.7851 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 36.7835 - acc: 1.0000 - val_loss: 35.2126 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 35.2111 - acc: 1.0000 - val_loss: 33.6922 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 969ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea31697ed084132918f9864a3dec7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 90.9961 - acc: 0.4516 - val_loss: 87.7477 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 87.7469 - acc: 0.7742 - val_loss: 84.5936 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 991ms/step - loss: 84.5865 - acc: 0.9032 - val_loss: 81.5219 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 81.4967 - acc: 0.9677 - val_loss: 78.5454 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 78.5194 - acc: 0.9677 - val_loss: 75.6513 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 700ms/step - loss: 75.6128 - acc: 1.0000 - val_loss: 72.8515 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 731ms/step - loss: 72.8302 - acc: 1.0000 - val_loss: 70.1483 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 70.1273 - acc: 1.0000 - val_loss: 67.5306 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 603ms/step - loss: 67.5161 - acc: 1.0000 - val_loss: 65.0032 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 64.9888 - acc: 1.0000 - val_loss: 62.5470 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 62.5360 - acc: 1.0000 - val_loss: 60.1660 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 60.1575 - acc: 1.0000 - val_loss: 57.8574 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 57.8502 - acc: 1.0000 - val_loss: 55.6255 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 55.6193 - acc: 1.0000 - val_loss: 53.4591 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 53.4549 - acc: 1.0000 - val_loss: 51.3590 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 51.3556 - acc: 1.0000 - val_loss: 49.3263 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 49.3237 - acc: 1.0000 - val_loss: 47.3415 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 47.3398 - acc: 1.0000 - val_loss: 45.4340 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 45.4324 - acc: 1.0000 - val_loss: 43.5846 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 43.5833 - acc: 1.0000 - val_loss: 41.7973 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 41.7963 - acc: 1.0000 - val_loss: 40.0779 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 40.0769 - acc: 1.0000 - val_loss: 38.3981 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 38.3973 - acc: 1.0000 - val_loss: 36.7845 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 36.7838 - acc: 1.0000 - val_loss: 35.2193 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235ca5d3e8734949b48927a395531a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 166.6162 - acc: 0.2903 - val_loss: 159.5881 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159.5915 - acc: 0.7419 - val_loss: 152.7565 - val_acc: 0.7333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 855ms/step - loss: 152.7569 - acc: 0.8065 - val_loss: 146.1390 - val_acc: 0.8000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 859ms/step - loss: 146.1300 - acc: 0.8387 - val_loss: 139.7751 - val_acc: 0.8667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 139.7699 - acc: 0.8710 - val_loss: 133.6068 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 133.5849 - acc: 0.9355 - val_loss: 127.6508 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 652ms/step - loss: 127.6346 - acc: 0.9677 - val_loss: 121.9080 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 719ms/step - loss: 121.8773 - acc: 0.9355 - val_loss: 116.3655 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 116.3372 - acc: 0.9677 - val_loss: 111.0378 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 878ms/step - loss: 111.0095 - acc: 0.9677 - val_loss: 105.8855 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 105.8489 - acc: 1.0000 - val_loss: 100.9212 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 100.8747 - acc: 1.0000 - val_loss: 96.1313 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 96.0797 - acc: 1.0000 - val_loss: 91.5516 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 91.4965 - acc: 1.0000 - val_loss: 87.1451 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 87.0846 - acc: 1.0000 - val_loss: 82.9086 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 82.8465 - acc: 1.0000 - val_loss: 78.8455 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 78.7867 - acc: 1.0000 - val_loss: 74.9092 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 74.8531 - acc: 1.0000 - val_loss: 71.1681 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 71.1138 - acc: 1.0000 - val_loss: 67.5636 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 67.5120 - acc: 1.0000 - val_loss: 64.1112 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 64.0653 - acc: 1.0000 - val_loss: 60.8292 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 60.7881 - acc: 1.0000 - val_loss: 57.6561 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 57.6190 - acc: 1.0000 - val_loss: 54.6451 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 54.6121 - acc: 1.0000 - val_loss: 51.7604 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 51.7313 - acc: 1.0000 - val_loss: 48.9988 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 48.9731 - acc: 1.0000 - val_loss: 46.3820 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 46.3587 - acc: 1.0000 - val_loss: 43.8754 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 43.8548 - acc: 1.0000 - val_loss: 41.4769 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 41.4584 - acc: 1.0000 - val_loss: 39.2024 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 39.1861 - acc: 1.0000 - val_loss: 37.0393 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 37.0245 - acc: 1.0000 - val_loss: 34.9576 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 976ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9deec12003f748c9911dccc8c1c4ae85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 166.5005 - acc: 0.8387 - val_loss: 159.4554 - val_acc: 0.8667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159.4523 - acc: 0.8710 - val_loss: 152.6043 - val_acc: 0.8000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152.5951 - acc: 0.9032 - val_loss: 145.9756 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 998ms/step - loss: 145.9627 - acc: 0.9355 - val_loss: 139.5936 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 139.5791 - acc: 0.9355 - val_loss: 133.4089 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 751ms/step - loss: 133.3734 - acc: 1.0000 - val_loss: 127.4413 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 800ms/step - loss: 127.4066 - acc: 0.9677 - val_loss: 121.6902 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 671ms/step - loss: 121.6448 - acc: 1.0000 - val_loss: 116.1391 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 116.0947 - acc: 1.0000 - val_loss: 110.8085 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 712ms/step - loss: 110.7613 - acc: 1.0000 - val_loss: 105.6610 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 105.6127 - acc: 1.0000 - val_loss: 100.7050 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 498ms/step - loss: 100.6552 - acc: 1.0000 - val_loss: 95.9262 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 739ms/step - loss: 95.8802 - acc: 1.0000 - val_loss: 91.3592 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 91.3165 - acc: 1.0000 - val_loss: 86.9682 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 86.9305 - acc: 1.0000 - val_loss: 82.7479 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 82.7156 - acc: 1.0000 - val_loss: 78.7091 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 78.6823 - acc: 1.0000 - val_loss: 74.7962 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 74.7733 - acc: 1.0000 - val_loss: 71.0757 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 71.0562 - acc: 1.0000 - val_loss: 67.4946 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 67.4778 - acc: 1.0000 - val_loss: 64.0675 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 64.0535 - acc: 1.0000 - val_loss: 60.8036 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 60.7917 - acc: 1.0000 - val_loss: 57.6339 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 57.6238 - acc: 1.0000 - val_loss: 54.6274 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 54.6185 - acc: 1.0000 - val_loss: 51.7487 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 51.7410 - acc: 1.0000 - val_loss: 49.0006 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 48.9936 - acc: 1.0000 - val_loss: 46.4057 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 46.3995 - acc: 1.0000 - val_loss: 43.9140 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 972ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95cba840a914c54b0942f2708c0510e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 166.5737 - acc: 0.4516 - val_loss: 159.4884 - val_acc: 0.8667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159.4839 - acc: 0.8065 - val_loss: 152.6044 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152.5988 - acc: 0.8387 - val_loss: 145.9406 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 796ms/step - loss: 145.9249 - acc: 0.9355 - val_loss: 139.5273 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 798ms/step - loss: 139.5119 - acc: 0.9677 - val_loss: 133.3128 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 133.2767 - acc: 1.0000 - val_loss: 127.3137 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 600ms/step - loss: 127.2794 - acc: 1.0000 - val_loss: 121.5439 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 836ms/step - loss: 121.5018 - acc: 1.0000 - val_loss: 115.9880 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 115.9490 - acc: 1.0000 - val_loss: 110.6603 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 110.6286 - acc: 1.0000 - val_loss: 105.5195 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 105.4892 - acc: 1.0000 - val_loss: 100.5674 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 100.5400 - acc: 1.0000 - val_loss: 95.7955 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 95.7746 - acc: 1.0000 - val_loss: 91.2287 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 91.2116 - acc: 1.0000 - val_loss: 86.8422 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 566ms/step - loss: 86.8291 - acc: 1.0000 - val_loss: 82.6274 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 512ms/step - loss: 82.6164 - acc: 1.0000 - val_loss: 78.5907 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 78.5823 - acc: 1.0000 - val_loss: 74.6845 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 74.6777 - acc: 1.0000 - val_loss: 70.9756 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 70.9701 - acc: 1.0000 - val_loss: 67.4074 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 67.4029 - acc: 1.0000 - val_loss: 63.9878 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 63.9839 - acc: 1.0000 - val_loss: 60.7312 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 60.7279 - acc: 1.0000 - val_loss: 57.5727 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 57.5698 - acc: 1.0000 - val_loss: 54.5766 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 54.5741 - acc: 1.0000 - val_loss: 51.6990 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 994ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3696d10ef900427c8c3c7d705a1a5c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 166.5003 - acc: 0.2903 - val_loss: 159.3863 - val_acc: 0.8667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159.3744 - acc: 0.8710 - val_loss: 152.4845 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152.4624 - acc: 0.9355 - val_loss: 145.8052 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 989ms/step - loss: 145.7726 - acc: 0.9677 - val_loss: 139.3789 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 139.3449 - acc: 0.9677 - val_loss: 133.1615 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 934ms/step - loss: 133.1095 - acc: 1.0000 - val_loss: 127.1763 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 781ms/step - loss: 127.1334 - acc: 1.0000 - val_loss: 121.4260 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 738ms/step - loss: 121.3816 - acc: 1.0000 - val_loss: 115.8836 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 708ms/step - loss: 115.8503 - acc: 1.0000 - val_loss: 110.5639 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 110.5348 - acc: 1.0000 - val_loss: 105.4303 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 105.4069 - acc: 1.0000 - val_loss: 100.4899 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 481ms/step - loss: 100.4714 - acc: 1.0000 - val_loss: 95.7301 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 95.7164 - acc: 1.0000 - val_loss: 91.1835 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 91.1732 - acc: 1.0000 - val_loss: 86.8178 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 86.8103 - acc: 1.0000 - val_loss: 82.6243 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 82.6185 - acc: 1.0000 - val_loss: 78.6089 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 78.6045 - acc: 1.0000 - val_loss: 74.7183 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 74.7148 - acc: 1.0000 - val_loss: 71.0203 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 71.0174 - acc: 1.0000 - val_loss: 67.4523 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 67.4499 - acc: 1.0000 - val_loss: 64.0339 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 64.0320 - acc: 1.0000 - val_loss: 60.7829 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 60.7812 - acc: 1.0000 - val_loss: 57.6255 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 57.6240 - acc: 1.0000 - val_loss: 54.6325 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 54.6313 - acc: 1.0000 - val_loss: 51.7614 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441388bbd1034c028957859134a90453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 166.6842 - acc: 0.6129 - val_loss: 159.5477 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159.5426 - acc: 0.6774 - val_loss: 152.6166 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152.6019 - acc: 0.9355 - val_loss: 145.9094 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 767ms/step - loss: 145.8884 - acc: 0.9677 - val_loss: 139.4635 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 139.4463 - acc: 1.0000 - val_loss: 133.2351 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 736ms/step - loss: 133.2017 - acc: 1.0000 - val_loss: 127.2429 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 127.2135 - acc: 1.0000 - val_loss: 121.4907 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 121.4643 - acc: 1.0000 - val_loss: 115.9477 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 606ms/step - loss: 115.9303 - acc: 1.0000 - val_loss: 110.6297 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 527ms/step - loss: 110.6148 - acc: 1.0000 - val_loss: 105.5016 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 105.4904 - acc: 1.0000 - val_loss: 100.5683 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 100.5602 - acc: 1.0000 - val_loss: 95.8096 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 95.8039 - acc: 1.0000 - val_loss: 91.2578 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 91.2535 - acc: 1.0000 - val_loss: 86.8833 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 86.8800 - acc: 1.0000 - val_loss: 82.6839 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 82.6814 - acc: 1.0000 - val_loss: 78.6589 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 514ms/step - loss: 78.6569 - acc: 1.0000 - val_loss: 74.7558 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 74.7541 - acc: 1.0000 - val_loss: 71.0488 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 71.0474 - acc: 1.0000 - val_loss: 67.4789 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 67.4777 - acc: 1.0000 - val_loss: 64.0584 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 64.0573 - acc: 1.0000 - val_loss: 60.7962 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 60.7952 - acc: 1.0000 - val_loss: 57.6313 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 57.6305 - acc: 1.0000 - val_loss: 54.6298 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473800ba93ef480ba2a8e26c2b82fd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 166.5481 - acc: 0.6129 - val_loss: 159.5332 - val_acc: 0.8667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159.5334 - acc: 0.8387 - val_loss: 152.7069 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 753ms/step - loss: 152.7030 - acc: 0.7742 - val_loss: 146.0937 - val_acc: 0.8000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146.0859 - acc: 0.8710 - val_loss: 139.7300 - val_acc: 0.8667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 815ms/step - loss: 139.7211 - acc: 0.8710 - val_loss: 133.5666 - val_acc: 0.8667\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 515ms/step - loss: 133.5450 - acc: 0.9032 - val_loss: 127.6179 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 127.5993 - acc: 0.9032 - val_loss: 121.8882 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 121.8674 - acc: 0.8710 - val_loss: 116.3621 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 116.3465 - acc: 0.9032 - val_loss: 111.0470 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 483ms/step - loss: 111.0339 - acc: 0.9355 - val_loss: 105.9054 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 105.8768 - acc: 0.9677 - val_loss: 100.9445 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 784ms/step - loss: 100.9098 - acc: 1.0000 - val_loss: 96.1513 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 96.1153 - acc: 1.0000 - val_loss: 91.5661 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 91.5265 - acc: 1.0000 - val_loss: 87.1570 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 782ms/step - loss: 87.1023 - acc: 1.0000 - val_loss: 82.9141 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 82.8661 - acc: 1.0000 - val_loss: 78.8475 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 78.7970 - acc: 1.0000 - val_loss: 74.9065 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 74.8617 - acc: 1.0000 - val_loss: 71.1622 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 71.1179 - acc: 1.0000 - val_loss: 67.5634 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 67.5166 - acc: 1.0000 - val_loss: 64.1134 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 64.0707 - acc: 1.0000 - val_loss: 60.8230 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 60.7824 - acc: 1.0000 - val_loss: 57.6393 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 57.6040 - acc: 1.0000 - val_loss: 54.6276 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 54.5987 - acc: 1.0000 - val_loss: 51.7390 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 51.7101 - acc: 1.0000 - val_loss: 48.9813 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 48.9563 - acc: 1.0000 - val_loss: 46.3728 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 46.3504 - acc: 1.0000 - val_loss: 43.8801 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 43.8604 - acc: 1.0000 - val_loss: 41.4972 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 41.4789 - acc: 1.0000 - val_loss: 39.2293 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 39.2136 - acc: 1.0000 - val_loss: 37.0644 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 37.0509 - acc: 1.0000 - val_loss: 34.9832 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 34.9708 - acc: 1.0000 - val_loss: 33.0259 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d20e1a981c4563a4a2fcbb12e6dddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 166.5916 - acc: 0.5806 - val_loss: 159.5627 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159.5620 - acc: 0.6774 - val_loss: 152.7140 - val_acc: 0.6000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152.7132 - acc: 0.6452 - val_loss: 146.0866 - val_acc: 0.8000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 845ms/step - loss: 146.0808 - acc: 0.7097 - val_loss: 139.7117 - val_acc: 0.8000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 139.7009 - acc: 0.9032 - val_loss: 133.5343 - val_acc: 0.8667\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 806ms/step - loss: 133.5116 - acc: 0.9032 - val_loss: 127.5689 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 806ms/step - loss: 127.5451 - acc: 0.9677 - val_loss: 121.8137 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 803ms/step - loss: 121.7806 - acc: 0.9677 - val_loss: 116.2587 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 116.2388 - acc: 0.9677 - val_loss: 110.9229 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 110.9071 - acc: 1.0000 - val_loss: 105.7667 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 697ms/step - loss: 105.7335 - acc: 1.0000 - val_loss: 100.7969 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 100.7650 - acc: 1.0000 - val_loss: 95.9990 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 95.9661 - acc: 1.0000 - val_loss: 91.4138 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 645ms/step - loss: 91.3803 - acc: 1.0000 - val_loss: 87.0102 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 86.9781 - acc: 1.0000 - val_loss: 82.7808 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 82.7497 - acc: 1.0000 - val_loss: 78.7308 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 78.7087 - acc: 1.0000 - val_loss: 74.8109 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 74.7922 - acc: 1.0000 - val_loss: 71.0855 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 71.0695 - acc: 1.0000 - val_loss: 67.4979 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 67.4812 - acc: 1.0000 - val_loss: 64.0606 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 64.0463 - acc: 1.0000 - val_loss: 60.7853 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 60.7732 - acc: 1.0000 - val_loss: 57.6142 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 57.6041 - acc: 1.0000 - val_loss: 54.6072 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 54.5987 - acc: 1.0000 - val_loss: 51.7256 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 51.7188 - acc: 1.0000 - val_loss: 48.9722 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 48.9659 - acc: 1.0000 - val_loss: 46.3588 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 46.3534 - acc: 1.0000 - val_loss: 43.8568 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 43.8520 - acc: 1.0000 - val_loss: 41.4646 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 993ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b0ff2e938242488fd4c68207da3ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 166.5709 - acc: 0.5484 - val_loss: 159.4928 - val_acc: 1.0000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 159.4947 - acc: 0.8710 - val_loss: 152.6166 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152.6081 - acc: 0.9032 - val_loss: 145.9599 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 833ms/step - loss: 145.9338 - acc: 0.9677 - val_loss: 139.5550 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 967ms/step - loss: 139.5464 - acc: 0.9355 - val_loss: 133.3503 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 133.3169 - acc: 1.0000 - val_loss: 127.3621 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 127.3263 - acc: 0.9677 - val_loss: 121.5983 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 121.5624 - acc: 1.0000 - val_loss: 116.0420 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 116.0187 - acc: 1.0000 - val_loss: 110.7111 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 110.6891 - acc: 1.0000 - val_loss: 105.5638 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 105.5444 - acc: 1.0000 - val_loss: 100.6097 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 100.5913 - acc: 1.0000 - val_loss: 95.8317 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 95.8129 - acc: 1.0000 - val_loss: 91.2662 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 738ms/step - loss: 91.2494 - acc: 1.0000 - val_loss: 86.8867 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 86.8734 - acc: 1.0000 - val_loss: 82.6777 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 82.6671 - acc: 1.0000 - val_loss: 78.6475 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 78.6400 - acc: 1.0000 - val_loss: 74.7388 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 74.7327 - acc: 1.0000 - val_loss: 71.0218 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 71.0165 - acc: 1.0000 - val_loss: 67.4521 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 67.4475 - acc: 1.0000 - val_loss: 64.0326 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 64.0289 - acc: 1.0000 - val_loss: 60.7716 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78aa2e1d9a5e4315ada65d08401ab0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 166.4200 - acc: 0.6129 - val_loss: 159.3237 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 159.3117 - acc: 0.8710 - val_loss: 152.4280 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 961ms/step - loss: 152.4141 - acc: 0.9677 - val_loss: 145.7543 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145.7415 - acc: 0.9677 - val_loss: 139.3370 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 538ms/step - loss: 139.3191 - acc: 0.9677 - val_loss: 133.1275 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 133.1037 - acc: 1.0000 - val_loss: 127.1452 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 127.1191 - acc: 1.0000 - val_loss: 121.3903 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 121.3657 - acc: 1.0000 - val_loss: 115.8444 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 470ms/step - loss: 115.8267 - acc: 1.0000 - val_loss: 110.5230 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 110.5066 - acc: 1.0000 - val_loss: 105.3941 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 105.3811 - acc: 1.0000 - val_loss: 100.4604 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 100.4477 - acc: 1.0000 - val_loss: 95.7069 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 596ms/step - loss: 95.6968 - acc: 1.0000 - val_loss: 91.1622 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 702ms/step - loss: 91.1541 - acc: 1.0000 - val_loss: 86.7969 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 86.7901 - acc: 1.0000 - val_loss: 82.6020 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 82.5968 - acc: 1.0000 - val_loss: 78.5863 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 78.5832 - acc: 1.0000 - val_loss: 74.6896 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 74.6868 - acc: 1.0000 - val_loss: 70.9844 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 70.9823 - acc: 1.0000 - val_loss: 67.4159 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 67.4140 - acc: 1.0000 - val_loss: 64.0001 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 63.9984 - acc: 1.0000 - val_loss: 60.7494 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 60.7481 - acc: 1.0000 - val_loss: 57.5983 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 57.5971 - acc: 1.0000 - val_loss: 54.6103 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 54.6095 - acc: 1.0000 - val_loss: 51.7416 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4a870d9649416aadbef13f4a65489e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 166.4917 - acc: 0.4516 - val_loss: 159.3589 - val_acc: 0.8667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 159.3531 - acc: 0.8710 - val_loss: 152.4399 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152.4198 - acc: 0.9677 - val_loss: 145.7408 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 926ms/step - loss: 145.7240 - acc: 0.9677 - val_loss: 139.3020 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 139.2923 - acc: 0.9677 - val_loss: 133.0872 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 914ms/step - loss: 133.0636 - acc: 1.0000 - val_loss: 127.1065 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 127.0952 - acc: 1.0000 - val_loss: 121.3645 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 121.3467 - acc: 1.0000 - val_loss: 115.8377 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 115.8286 - acc: 1.0000 - val_loss: 110.5409 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 110.5307 - acc: 1.0000 - val_loss: 105.4318 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 105.4232 - acc: 1.0000 - val_loss: 100.5135 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 100.5061 - acc: 1.0000 - val_loss: 95.7623 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 95.7576 - acc: 1.0000 - val_loss: 91.2167 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 684ms/step - loss: 91.2126 - acc: 1.0000 - val_loss: 86.8498 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 86.8466 - acc: 1.0000 - val_loss: 82.6531 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 82.6506 - acc: 1.0000 - val_loss: 78.6321 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 78.6303 - acc: 1.0000 - val_loss: 74.7260 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 74.7245 - acc: 1.0000 - val_loss: 71.0159 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 71.0147 - acc: 1.0000 - val_loss: 67.4450 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 67.4439 - acc: 1.0000 - val_loss: 64.0224 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 64.0215 - acc: 1.0000 - val_loss: 60.7638 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 60.7630 - acc: 1.0000 - val_loss: 57.6048 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 57.6041 - acc: 1.0000 - val_loss: 54.6057 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68ca92b8c54478097916b41726d2f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 256.0942 - acc: 0.6452 - val_loss: 243.6849 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 243.6800 - acc: 0.6129 - val_loss: 231.6607 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 231.6550 - acc: 0.5806 - val_loss: 220.0818 - val_acc: 0.6000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 220.0732 - acc: 0.7419 - val_loss: 209.0195 - val_acc: 0.7333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 209.0043 - acc: 0.8065 - val_loss: 198.3658 - val_acc: 0.7333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 754ms/step - loss: 198.3386 - acc: 0.9032 - val_loss: 188.1463 - val_acc: 0.8667\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 557ms/step - loss: 188.1197 - acc: 0.9677 - val_loss: 178.3644 - val_acc: 0.8667\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 178.3293 - acc: 0.9677 - val_loss: 168.9904 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 168.9519 - acc: 1.0000 - val_loss: 160.0538 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 596ms/step - loss: 160.0129 - acc: 1.0000 - val_loss: 151.4673 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 151.4126 - acc: 1.0000 - val_loss: 143.2568 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 143.1989 - acc: 1.0000 - val_loss: 135.3866 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 135.3220 - acc: 1.0000 - val_loss: 127.9327 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 127.8625 - acc: 1.0000 - val_loss: 120.8324 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 120.7625 - acc: 1.0000 - val_loss: 114.0605 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 113.9879 - acc: 1.0000 - val_loss: 107.6307 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 637ms/step - loss: 107.5648 - acc: 1.0000 - val_loss: 101.4249 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 101.3648 - acc: 1.0000 - val_loss: 95.5982 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 95.5431 - acc: 1.0000 - val_loss: 90.0344 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 89.9842 - acc: 1.0000 - val_loss: 84.7528 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 84.7088 - acc: 1.0000 - val_loss: 79.7809 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 79.7435 - acc: 1.0000 - val_loss: 74.9807 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 74.9477 - acc: 1.0000 - val_loss: 70.4891 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 70.4611 - acc: 1.0000 - val_loss: 66.2159 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 66.1916 - acc: 1.0000 - val_loss: 62.1661 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 62.1450 - acc: 1.0000 - val_loss: 58.3893 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 58.3708 - acc: 1.0000 - val_loss: 54.8031 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 54.7864 - acc: 1.0000 - val_loss: 51.4042 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 51.3895 - acc: 1.0000 - val_loss: 48.2066 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a4a4e9809a4820810ce60114f4e54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 256.1627 - acc: 0.5484 - val_loss: 243.7153 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 243.7041 - acc: 0.9032 - val_loss: 231.6630 - val_acc: 0.7333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 231.6534 - acc: 0.9032 - val_loss: 220.0627 - val_acc: 0.8000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 220.0459 - acc: 0.9032 - val_loss: 208.9708 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 208.9484 - acc: 0.9677 - val_loss: 198.2833 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 937ms/step - loss: 198.2444 - acc: 0.9677 - val_loss: 188.0408 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 874ms/step - loss: 188.0002 - acc: 0.9677 - val_loss: 178.2458 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 699ms/step - loss: 178.2019 - acc: 1.0000 - val_loss: 168.8598 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 990ms/step - loss: 168.8166 - acc: 1.0000 - val_loss: 159.9119 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 159.8645 - acc: 1.0000 - val_loss: 151.3222 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 151.2740 - acc: 1.0000 - val_loss: 143.1022 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 143.0566 - acc: 1.0000 - val_loss: 135.2239 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 135.1791 - acc: 1.0000 - val_loss: 127.7677 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 127.7283 - acc: 1.0000 - val_loss: 120.6736 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 497ms/step - loss: 120.6407 - acc: 1.0000 - val_loss: 113.9127 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 113.8835 - acc: 1.0000 - val_loss: 107.5005 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 107.4766 - acc: 1.0000 - val_loss: 101.3156 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 101.2960 - acc: 1.0000 - val_loss: 95.5026 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 95.4857 - acc: 1.0000 - val_loss: 89.9573 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 89.9431 - acc: 1.0000 - val_loss: 84.6942 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 84.6825 - acc: 1.0000 - val_loss: 79.7399 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 79.7300 - acc: 1.0000 - val_loss: 74.9484 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 74.9399 - acc: 1.0000 - val_loss: 70.4621 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 70.4548 - acc: 1.0000 - val_loss: 66.1923 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 66.1859 - acc: 1.0000 - val_loss: 62.1487 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 62.1430 - acc: 1.0000 - val_loss: 58.3698 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 58.3648 - acc: 1.0000 - val_loss: 54.7797 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8822a22d515c413fb25585a119e91764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 255.9320 - acc: 0.4839 - val_loss: 243.4573 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 243.4473 - acc: 0.6774 - val_loss: 231.3815 - val_acc: 0.6667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 231.3727 - acc: 0.8065 - val_loss: 219.7545 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 219.7347 - acc: 0.9355 - val_loss: 208.6411 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 965ms/step - loss: 208.6116 - acc: 0.9677 - val_loss: 197.9376 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 197.8893 - acc: 1.0000 - val_loss: 187.6758 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 739ms/step - loss: 187.6319 - acc: 1.0000 - val_loss: 177.8653 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 177.8196 - acc: 1.0000 - val_loss: 168.4816 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 833ms/step - loss: 168.4450 - acc: 1.0000 - val_loss: 159.5539 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 159.5212 - acc: 1.0000 - val_loss: 150.9983 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 150.9687 - acc: 1.0000 - val_loss: 142.8212 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 570ms/step - loss: 142.7961 - acc: 1.0000 - val_loss: 134.9912 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 134.9722 - acc: 1.0000 - val_loss: 127.5730 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 127.5574 - acc: 1.0000 - val_loss: 120.5038 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 120.4917 - acc: 1.0000 - val_loss: 113.7687 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 113.7591 - acc: 1.0000 - val_loss: 107.3828 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 107.3755 - acc: 1.0000 - val_loss: 101.2173 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 101.2114 - acc: 1.0000 - val_loss: 95.4404 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 95.4357 - acc: 1.0000 - val_loss: 89.9222 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 89.9184 - acc: 1.0000 - val_loss: 84.6749 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 84.6718 - acc: 1.0000 - val_loss: 79.7337 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 79.7310 - acc: 1.0000 - val_loss: 74.9594 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 74.9572 - acc: 1.0000 - val_loss: 70.4888 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 70.4868 - acc: 1.0000 - val_loss: 66.2303 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 66.2286 - acc: 1.0000 - val_loss: 62.1945 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4e31e9eb704a17952728c0440a4285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 255.9539 - acc: 0.6774 - val_loss: 243.4376 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 243.4177 - acc: 0.8387 - val_loss: 231.3262 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 231.3102 - acc: 0.9032 - val_loss: 219.6710 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 964ms/step - loss: 219.6456 - acc: 0.9677 - val_loss: 208.5371 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 208.5072 - acc: 1.0000 - val_loss: 197.8331 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 197.7931 - acc: 1.0000 - val_loss: 187.5854 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 187.5542 - acc: 1.0000 - val_loss: 177.7942 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 929ms/step - loss: 177.7646 - acc: 1.0000 - val_loss: 168.4222 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 636ms/step - loss: 168.3996 - acc: 1.0000 - val_loss: 159.5028 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 706ms/step - loss: 159.4822 - acc: 1.0000 - val_loss: 150.9518 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 150.9368 - acc: 1.0000 - val_loss: 142.7790 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 142.7677 - acc: 1.0000 - val_loss: 134.9485 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 134.9402 - acc: 1.0000 - val_loss: 127.5416 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 127.5355 - acc: 1.0000 - val_loss: 120.4913 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 493ms/step - loss: 120.4865 - acc: 1.0000 - val_loss: 113.7637 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 113.7599 - acc: 1.0000 - val_loss: 107.3789 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 107.3759 - acc: 1.0000 - val_loss: 101.2163 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 101.2139 - acc: 1.0000 - val_loss: 95.4312 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 95.4291 - acc: 1.0000 - val_loss: 89.9086 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 89.9069 - acc: 1.0000 - val_loss: 84.6687 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 84.6672 - acc: 1.0000 - val_loss: 79.7309 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 79.7296 - acc: 1.0000 - val_loss: 74.9541 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 74.9529 - acc: 1.0000 - val_loss: 70.4832 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 70.4821 - acc: 1.0000 - val_loss: 66.2225 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8565e2a48db644c89f227518c7e01ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 256.0000 - acc: 0.3548 - val_loss: 243.4763 - val_acc: 0.6000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 243.4635 - acc: 0.8065 - val_loss: 231.3581 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 231.3258 - acc: 0.9677 - val_loss: 219.6962 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 219.6579 - acc: 0.9677 - val_loss: 208.5613 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 208.5182 - acc: 1.0000 - val_loss: 197.8559 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 591ms/step - loss: 197.8050 - acc: 1.0000 - val_loss: 187.6027 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 795ms/step - loss: 187.5700 - acc: 1.0000 - val_loss: 177.8184 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 476ms/step - loss: 177.7881 - acc: 1.0000 - val_loss: 168.4623 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 590ms/step - loss: 168.4397 - acc: 1.0000 - val_loss: 159.5486 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 795ms/step - loss: 159.5321 - acc: 1.0000 - val_loss: 150.9985 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 150.9867 - acc: 1.0000 - val_loss: 142.8316 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 142.8232 - acc: 1.0000 - val_loss: 135.0087 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 135.0028 - acc: 1.0000 - val_loss: 127.6003 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 127.5958 - acc: 1.0000 - val_loss: 120.5384 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 854ms/step - loss: 120.5351 - acc: 1.0000 - val_loss: 113.7982 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 113.7956 - acc: 1.0000 - val_loss: 107.3940 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 107.3921 - acc: 1.0000 - val_loss: 101.2200 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 101.2185 - acc: 1.0000 - val_loss: 95.4265 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 95.4252 - acc: 1.0000 - val_loss: 89.8899 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 89.8888 - acc: 1.0000 - val_loss: 84.6328 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 84.6319 - acc: 1.0000 - val_loss: 79.6850 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 79.6842 - acc: 1.0000 - val_loss: 74.8964 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 74.8957 - acc: 1.0000 - val_loss: 70.4157 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8c575f372d4b3b965cfca530c79e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 256.1476 - acc: 0.5161 - val_loss: 243.7273 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 243.7247 - acc: 0.5484 - val_loss: 231.7029 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 231.6980 - acc: 0.5806 - val_loss: 220.1310 - val_acc: 0.6000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 986ms/step - loss: 220.1208 - acc: 0.7419 - val_loss: 209.0673 - val_acc: 0.6000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 209.0529 - acc: 0.7742 - val_loss: 198.4079 - val_acc: 0.6667\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 703ms/step - loss: 198.3840 - acc: 0.8387 - val_loss: 188.1882 - val_acc: 0.8000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 916ms/step - loss: 188.1673 - acc: 0.8710 - val_loss: 178.4195 - val_acc: 0.8667\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 178.3866 - acc: 0.9355 - val_loss: 169.0486 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 783ms/step - loss: 169.0295 - acc: 0.9677 - val_loss: 160.1130 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 799ms/step - loss: 160.0847 - acc: 0.9677 - val_loss: 151.5302 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 151.4913 - acc: 0.9677 - val_loss: 143.3183 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 143.2801 - acc: 0.9677 - val_loss: 135.4480 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 135.3963 - acc: 1.0000 - val_loss: 127.9813 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 127.9296 - acc: 1.0000 - val_loss: 120.8698 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 120.8154 - acc: 1.0000 - val_loss: 114.0903 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 114.0229 - acc: 1.0000 - val_loss: 107.6550 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 107.6031 - acc: 1.0000 - val_loss: 101.4464 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 101.3967 - acc: 1.0000 - val_loss: 95.6195 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 95.5697 - acc: 1.0000 - val_loss: 90.0527 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 90.0090 - acc: 1.0000 - val_loss: 84.7704 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 84.7263 - acc: 1.0000 - val_loss: 79.7988 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 481ms/step - loss: 79.7598 - acc: 1.0000 - val_loss: 74.9984 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 74.9642 - acc: 1.0000 - val_loss: 70.4996 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 70.4731 - acc: 1.0000 - val_loss: 66.2163 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 66.1907 - acc: 1.0000 - val_loss: 62.1654 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 62.1427 - acc: 1.0000 - val_loss: 58.3838 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 58.3644 - acc: 1.0000 - val_loss: 54.8007 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 54.7838 - acc: 1.0000 - val_loss: 51.3998 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 51.3845 - acc: 1.0000 - val_loss: 48.1989 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 48.1859 - acc: 1.0000 - val_loss: 45.1822 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9653289d778d4929a344a1dbe5aac6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 255.9658 - acc: 0.4194 - val_loss: 243.5346 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 243.5340 - acc: 0.6452 - val_loss: 231.4888 - val_acc: 0.6667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 231.4890 - acc: 0.6774 - val_loss: 219.8956 - val_acc: 0.7333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 219.8865 - acc: 0.8387 - val_loss: 208.8215 - val_acc: 0.8667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 208.8143 - acc: 0.8710 - val_loss: 198.1539 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 739ms/step - loss: 198.1309 - acc: 0.9355 - val_loss: 187.9235 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 187.9031 - acc: 0.9677 - val_loss: 178.1324 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 798ms/step - loss: 178.0977 - acc: 0.9677 - val_loss: 168.7448 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 850ms/step - loss: 168.7211 - acc: 0.9677 - val_loss: 159.7973 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159.7745 - acc: 1.0000 - val_loss: 151.2137 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 621ms/step - loss: 151.1844 - acc: 1.0000 - val_loss: 143.0068 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 142.9737 - acc: 1.0000 - val_loss: 135.1498 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 135.1159 - acc: 1.0000 - val_loss: 127.7142 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 127.6831 - acc: 1.0000 - val_loss: 120.6296 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 120.5973 - acc: 1.0000 - val_loss: 113.8728 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 113.8391 - acc: 1.0000 - val_loss: 107.4639 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 107.4394 - acc: 1.0000 - val_loss: 101.2894 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 101.2667 - acc: 1.0000 - val_loss: 95.4944 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 95.4769 - acc: 1.0000 - val_loss: 89.9554 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 89.9377 - acc: 1.0000 - val_loss: 84.6931 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 84.6794 - acc: 1.0000 - val_loss: 79.7366 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 79.7246 - acc: 1.0000 - val_loss: 74.9592 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 74.9501 - acc: 1.0000 - val_loss: 70.4788 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 70.4711 - acc: 1.0000 - val_loss: 66.2164 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 66.2096 - acc: 1.0000 - val_loss: 62.1815 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 62.1753 - acc: 1.0000 - val_loss: 58.4134 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 58.4082 - acc: 1.0000 - val_loss: 54.8303 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55e864b9af74a52b5c55148b9872cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 256.1506 - acc: 0.5484 - val_loss: 243.6801 - val_acc: 0.8667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 243.6846 - acc: 0.7742 - val_loss: 231.6065 - val_acc: 0.8000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 742ms/step - loss: 231.6003 - acc: 0.8710 - val_loss: 219.9784 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 219.9667 - acc: 0.9355 - val_loss: 208.8546 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 208.8465 - acc: 0.9677 - val_loss: 198.1453 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 198.1152 - acc: 0.9677 - val_loss: 187.8774 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 775ms/step - loss: 187.8503 - acc: 1.0000 - val_loss: 178.0571 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 178.0270 - acc: 1.0000 - val_loss: 168.6576 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 898ms/step - loss: 168.6362 - acc: 1.0000 - val_loss: 159.7091 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 159.6842 - acc: 1.0000 - val_loss: 151.1341 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151.1164 - acc: 1.0000 - val_loss: 142.9478 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 725ms/step - loss: 142.9339 - acc: 1.0000 - val_loss: 135.1080 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 135.0939 - acc: 1.0000 - val_loss: 127.6841 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 604ms/step - loss: 127.6712 - acc: 1.0000 - val_loss: 120.6041 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 120.5942 - acc: 1.0000 - val_loss: 113.8527 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 113.8442 - acc: 1.0000 - val_loss: 107.4482 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 107.4426 - acc: 1.0000 - val_loss: 101.2789 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 101.2740 - acc: 1.0000 - val_loss: 95.4932 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 95.4895 - acc: 1.0000 - val_loss: 89.9644 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 89.9613 - acc: 1.0000 - val_loss: 84.7092 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 84.7066 - acc: 1.0000 - val_loss: 79.7593 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 79.7571 - acc: 1.0000 - val_loss: 74.9736 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 74.9718 - acc: 1.0000 - val_loss: 70.4847 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 70.4831 - acc: 1.0000 - val_loss: 66.2121 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 66.2107 - acc: 1.0000 - val_loss: 62.1584 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3954b84e06742feae4599d5cc6e999e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 256.1827 - acc: 0.7419 - val_loss: 243.6888 - val_acc: 0.7333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 243.6822 - acc: 0.8065 - val_loss: 231.5923 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 231.5816 - acc: 0.9032 - val_loss: 219.9429 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 219.9158 - acc: 0.9677 - val_loss: 208.8086 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 208.7793 - acc: 1.0000 - val_loss: 198.0962 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 704ms/step - loss: 198.0606 - acc: 1.0000 - val_loss: 187.8356 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 187.8067 - acc: 1.0000 - val_loss: 178.0357 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 178.0049 - acc: 1.0000 - val_loss: 168.6583 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 168.6359 - acc: 1.0000 - val_loss: 159.7360 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 159.7147 - acc: 1.0000 - val_loss: 151.1874 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 578ms/step - loss: 151.1709 - acc: 1.0000 - val_loss: 143.0221 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 617ms/step - loss: 143.0100 - acc: 1.0000 - val_loss: 135.1920 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 135.1829 - acc: 1.0000 - val_loss: 127.7634 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 127.7563 - acc: 1.0000 - val_loss: 120.6776 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 120.6719 - acc: 1.0000 - val_loss: 113.9178 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 113.9135 - acc: 1.0000 - val_loss: 107.5044 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 107.5014 - acc: 1.0000 - val_loss: 101.3133 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 101.3109 - acc: 1.0000 - val_loss: 95.5097 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 95.5078 - acc: 1.0000 - val_loss: 89.9612 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 89.9596 - acc: 1.0000 - val_loss: 84.6961 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 84.6947 - acc: 1.0000 - val_loss: 79.7430 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 79.7418 - acc: 1.0000 - val_loss: 74.9531 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 74.9521 - acc: 1.0000 - val_loss: 70.4693 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 70.4684 - acc: 1.0000 - val_loss: 66.1992 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d209a357cf74c7f9f600c5c41d96652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 255.8921 - acc: 0.6774 - val_loss: 243.3810 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 243.3759 - acc: 0.8065 - val_loss: 231.2737 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 231.2667 - acc: 0.9355 - val_loss: 219.6173 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 990ms/step - loss: 219.5992 - acc: 0.9677 - val_loss: 208.4808 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 208.4641 - acc: 1.0000 - val_loss: 197.7745 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 197.7538 - acc: 1.0000 - val_loss: 187.5219 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 945ms/step - loss: 187.5120 - acc: 1.0000 - val_loss: 177.7321 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 177.7125 - acc: 1.0000 - val_loss: 168.3628 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 168.3522 - acc: 1.0000 - val_loss: 159.4455 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 510ms/step - loss: 159.4347 - acc: 1.0000 - val_loss: 150.8937 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 150.8882 - acc: 1.0000 - val_loss: 142.7241 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 142.7200 - acc: 1.0000 - val_loss: 134.9005 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 700ms/step - loss: 134.8963 - acc: 1.0000 - val_loss: 127.4879 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 127.4847 - acc: 1.0000 - val_loss: 120.4306 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 947ms/step - loss: 120.4282 - acc: 1.0000 - val_loss: 113.6966 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 113.6948 - acc: 1.0000 - val_loss: 107.3023 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 107.3009 - acc: 1.0000 - val_loss: 101.1339 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 101.1328 - acc: 1.0000 - val_loss: 95.3479 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 95.3470 - acc: 1.0000 - val_loss: 89.8205 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 89.8198 - acc: 1.0000 - val_loss: 84.5749 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 84.5743 - acc: 1.0000 - val_loss: 79.6391 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 79.6385 - acc: 1.0000 - val_loss: 74.8697 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 74.8693 - acc: 1.0000 - val_loss: 70.4114 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e593ee8b7e741b88417ed3edca2d3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 357.5869 - acc: 0.4516 - val_loss: 338.2794 - val_acc: 0.7333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 338.2748 - acc: 0.7419 - val_loss: 319.6482 - val_acc: 0.6000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 319.6396 - acc: 0.7097 - val_loss: 301.8066 - val_acc: 0.6000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 790ms/step - loss: 301.7941 - acc: 0.7097 - val_loss: 284.8622 - val_acc: 0.6000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 284.8470 - acc: 0.8387 - val_loss: 268.6215 - val_acc: 0.6667\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 965ms/step - loss: 268.5940 - acc: 0.8710 - val_loss: 253.1310 - val_acc: 0.8000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 593ms/step - loss: 253.0983 - acc: 0.9677 - val_loss: 238.4063 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 798ms/step - loss: 238.3629 - acc: 1.0000 - val_loss: 224.3843 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 761ms/step - loss: 224.3380 - acc: 1.0000 - val_loss: 211.1219 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 211.0745 - acc: 1.0000 - val_loss: 198.4713 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 198.4148 - acc: 1.0000 - val_loss: 186.4523 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 515ms/step - loss: 186.3904 - acc: 1.0000 - val_loss: 175.0014 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 174.9308 - acc: 1.0000 - val_loss: 164.2429 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 470ms/step - loss: 164.1699 - acc: 1.0000 - val_loss: 154.0683 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 153.9934 - acc: 1.0000 - val_loss: 144.4516 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 144.3813 - acc: 1.0000 - val_loss: 135.4029 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 135.3374 - acc: 1.0000 - val_loss: 126.7071 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 126.6487 - acc: 1.0000 - val_loss: 118.6475 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 118.5950 - acc: 1.0000 - val_loss: 111.0201 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 110.9732 - acc: 1.0000 - val_loss: 103.8444 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 103.8049 - acc: 1.0000 - val_loss: 97.1599 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 97.1263 - acc: 1.0000 - val_loss: 90.7186 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 90.6900 - acc: 1.0000 - val_loss: 84.7593 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 84.7352 - acc: 1.0000 - val_loss: 79.1199 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 79.0992 - acc: 1.0000 - val_loss: 73.8118 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 73.7938 - acc: 1.0000 - val_loss: 68.9173 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 68.9019 - acc: 1.0000 - val_loss: 64.3085 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 64.2950 - acc: 1.0000 - val_loss: 59.9751 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 59.9631 - acc: 1.0000 - val_loss: 55.9369 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 55.9263 - acc: 1.0000 - val_loss: 52.1652 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcfa833b893499aad2caa2d9527454e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 515s 515s/step - loss: 357.6164 - acc: 0.6774 - val_loss: 338.2805 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 338.2831 - acc: 0.7419 - val_loss: 319.6272 - val_acc: 0.8000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 319.6199 - acc: 0.8065 - val_loss: 301.7581 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 301.7462 - acc: 0.9032 - val_loss: 284.7824 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 762ms/step - loss: 284.7697 - acc: 0.9355 - val_loss: 268.5139 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 268.4854 - acc: 0.9677 - val_loss: 252.9989 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 991ms/step - loss: 252.9732 - acc: 0.9677 - val_loss: 238.2492 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 572ms/step - loss: 238.2105 - acc: 1.0000 - val_loss: 224.2099 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 224.1721 - acc: 1.0000 - val_loss: 210.9326 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 210.8907 - acc: 1.0000 - val_loss: 198.2721 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 474ms/step - loss: 198.2302 - acc: 1.0000 - val_loss: 186.2527 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 859ms/step - loss: 186.2118 - acc: 1.0000 - val_loss: 174.8141 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 174.7754 - acc: 1.0000 - val_loss: 164.0740 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 164.0393 - acc: 1.0000 - val_loss: 153.9178 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 627ms/step - loss: 153.8892 - acc: 1.0000 - val_loss: 144.3186 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 144.2931 - acc: 1.0000 - val_loss: 135.2946 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 135.2737 - acc: 1.0000 - val_loss: 126.6285 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 126.6118 - acc: 1.0000 - val_loss: 118.5863 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 118.5727 - acc: 1.0000 - val_loss: 110.9609 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 110.9494 - acc: 1.0000 - val_loss: 103.7822 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 103.7728 - acc: 1.0000 - val_loss: 97.0941 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 97.0862 - acc: 1.0000 - val_loss: 90.6577 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 90.6510 - acc: 1.0000 - val_loss: 84.7084 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 84.7028 - acc: 1.0000 - val_loss: 79.0829 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 79.0781 - acc: 1.0000 - val_loss: 73.7836 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 73.7793 - acc: 1.0000 - val_loss: 68.8960 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 68.8922 - acc: 1.0000 - val_loss: 64.2885 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d57c899f654402bd36c457556a2a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 357.6219 - acc: 0.7419 - val_loss: 338.2591 - val_acc: 0.9333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 338.2529 - acc: 0.8387 - val_loss: 319.5697 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 319.5607 - acc: 0.9032 - val_loss: 301.6714 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 736ms/step - loss: 301.6439 - acc: 0.9677 - val_loss: 284.6680 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 284.6421 - acc: 1.0000 - val_loss: 268.3807 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 890ms/step - loss: 268.3391 - acc: 1.0000 - val_loss: 252.8481 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 921ms/step - loss: 252.8080 - acc: 1.0000 - val_loss: 238.0945 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 979ms/step - loss: 238.0480 - acc: 1.0000 - val_loss: 224.0593 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 605ms/step - loss: 224.0223 - acc: 1.0000 - val_loss: 210.7995 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 210.7686 - acc: 1.0000 - val_loss: 198.1715 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 198.1467 - acc: 1.0000 - val_loss: 186.1973 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 186.1780 - acc: 1.0000 - val_loss: 174.7933 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 174.7779 - acc: 1.0000 - val_loss: 164.0705 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 164.0588 - acc: 1.0000 - val_loss: 153.9287 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 153.9198 - acc: 1.0000 - val_loss: 144.3408 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 144.3340 - acc: 1.0000 - val_loss: 135.3150 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 135.3097 - acc: 1.0000 - val_loss: 126.6459 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 126.6418 - acc: 1.0000 - val_loss: 118.6119 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 118.6086 - acc: 1.0000 - val_loss: 110.9944 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 110.9917 - acc: 1.0000 - val_loss: 103.8093 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 103.8070 - acc: 1.0000 - val_loss: 97.1113 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 97.1094 - acc: 1.0000 - val_loss: 90.6565 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 90.6549 - acc: 1.0000 - val_loss: 84.6948 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 84.6934 - acc: 1.0000 - val_loss: 79.0538 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c238b5e42a143af9469441140ab5d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 357.5071 - acc: 0.5806 - val_loss: 338.0978 - val_acc: 0.8000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 338.0918 - acc: 0.8387 - val_loss: 319.3911 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 319.3771 - acc: 0.9355 - val_loss: 301.4700 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 301.4446 - acc: 0.9677 - val_loss: 284.4526 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 812ms/step - loss: 284.4240 - acc: 1.0000 - val_loss: 268.1568 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 804ms/step - loss: 268.1197 - acc: 1.0000 - val_loss: 252.6349 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 708ms/step - loss: 252.6053 - acc: 1.0000 - val_loss: 237.9067 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 624ms/step - loss: 237.8755 - acc: 1.0000 - val_loss: 223.8961 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 223.8739 - acc: 1.0000 - val_loss: 210.6487 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 210.6305 - acc: 1.0000 - val_loss: 198.0278 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 198.0142 - acc: 1.0000 - val_loss: 186.0467 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 186.0363 - acc: 1.0000 - val_loss: 174.6434 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 174.6355 - acc: 1.0000 - val_loss: 163.9462 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 163.9402 - acc: 1.0000 - val_loss: 153.8286 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 153.8241 - acc: 1.0000 - val_loss: 144.2572 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 144.2537 - acc: 1.0000 - val_loss: 135.2485 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 135.2457 - acc: 1.0000 - val_loss: 126.5889 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 126.5867 - acc: 1.0000 - val_loss: 118.5603 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 118.5585 - acc: 1.0000 - val_loss: 110.9538 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 110.9522 - acc: 1.0000 - val_loss: 103.7900 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 103.7886 - acc: 1.0000 - val_loss: 97.1109 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 97.1096 - acc: 1.0000 - val_loss: 90.6659 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 90.6648 - acc: 1.0000 - val_loss: 84.7145 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 84.7135 - acc: 1.0000 - val_loss: 79.0760 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e973d829afd450f95da06b9c8dc4b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 357.7492 - acc: 0.5806 - val_loss: 338.3144 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 338.2998 - acc: 0.8065 - val_loss: 319.5716 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 319.5486 - acc: 0.9677 - val_loss: 301.6165 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 301.5910 - acc: 1.0000 - val_loss: 284.5793 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 284.5564 - acc: 1.0000 - val_loss: 268.2816 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 268.2472 - acc: 1.0000 - val_loss: 252.7528 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 252.7299 - acc: 1.0000 - val_loss: 238.0142 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 237.9915 - acc: 1.0000 - val_loss: 223.9932 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 223.9805 - acc: 1.0000 - val_loss: 210.7402 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 210.7297 - acc: 1.0000 - val_loss: 198.1097 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 504ms/step - loss: 198.1031 - acc: 1.0000 - val_loss: 186.1186 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 186.1138 - acc: 1.0000 - val_loss: 174.7018 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 691ms/step - loss: 174.6983 - acc: 1.0000 - val_loss: 163.9809 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 163.9785 - acc: 1.0000 - val_loss: 153.8380 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 153.8362 - acc: 1.0000 - val_loss: 144.2385 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 144.2370 - acc: 1.0000 - val_loss: 135.2049 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 135.2038 - acc: 1.0000 - val_loss: 126.5240 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 126.5231 - acc: 1.0000 - val_loss: 118.4832 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 118.4824 - acc: 1.0000 - val_loss: 110.8619 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 459ms/step - loss: 110.8613 - acc: 1.0000 - val_loss: 103.6925 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 103.6919 - acc: 1.0000 - val_loss: 97.0158 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 97.0153 - acc: 1.0000 - val_loss: 90.5783 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 90.5779 - acc: 1.0000 - val_loss: 84.6312 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39de89bf0cc4c199167591242aee44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 8s 8s/step - loss: 357.5244 - acc: 0.4194 - val_loss: 338.2173 - val_acc: 0.7333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 338.2155 - acc: 0.7097 - val_loss: 319.5867 - val_acc: 0.7333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 319.5814 - acc: 0.7419 - val_loss: 301.7410 - val_acc: 0.7333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 892ms/step - loss: 301.7354 - acc: 0.7097 - val_loss: 284.7838 - val_acc: 0.7333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 658ms/step - loss: 284.7804 - acc: 0.8387 - val_loss: 268.5320 - val_acc: 0.8000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 268.5138 - acc: 0.8387 - val_loss: 253.0328 - val_acc: 0.8000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 253.0128 - acc: 0.9355 - val_loss: 238.3140 - val_acc: 0.8667\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 898ms/step - loss: 238.2893 - acc: 0.9677 - val_loss: 224.3053 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 502ms/step - loss: 224.2809 - acc: 0.9677 - val_loss: 211.0594 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 211.0280 - acc: 1.0000 - val_loss: 198.4266 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 198.3980 - acc: 0.9677 - val_loss: 186.4236 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 186.3826 - acc: 1.0000 - val_loss: 174.9762 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 174.9331 - acc: 1.0000 - val_loss: 164.2263 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 164.1780 - acc: 1.0000 - val_loss: 154.0595 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 154.0016 - acc: 1.0000 - val_loss: 144.4439 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 144.3865 - acc: 1.0000 - val_loss: 135.3883 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 135.3330 - acc: 1.0000 - val_loss: 126.6900 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 126.6425 - acc: 1.0000 - val_loss: 118.6341 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 118.5925 - acc: 1.0000 - val_loss: 110.9952 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 110.9515 - acc: 1.0000 - val_loss: 103.8064 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 103.7654 - acc: 1.0000 - val_loss: 97.1144 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 97.0778 - acc: 1.0000 - val_loss: 90.6772 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 90.6474 - acc: 1.0000 - val_loss: 84.7241 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 84.6991 - acc: 1.0000 - val_loss: 79.1023 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 79.0809 - acc: 1.0000 - val_loss: 73.8104 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 73.7913 - acc: 1.0000 - val_loss: 68.9427 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 68.9250 - acc: 1.0000 - val_loss: 64.3517 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 64.3359 - acc: 1.0000 - val_loss: 60.0211 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 60.0075 - acc: 1.0000 - val_loss: 55.9799 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 55.9677 - acc: 1.0000 - val_loss: 52.2005 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 52.1895 - acc: 1.0000 - val_loss: 48.6039 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6fb30a0e47486e88694a1cecf5070b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 357.5512 - acc: 0.4516 - val_loss: 338.2085 - val_acc: 0.7333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 338.2086 - acc: 0.7419 - val_loss: 319.5614 - val_acc: 0.7333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 319.5578 - acc: 0.8065 - val_loss: 301.6987 - val_acc: 0.8667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 301.6955 - acc: 0.8065 - val_loss: 284.7195 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 284.7095 - acc: 0.8387 - val_loss: 268.4499 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 268.4232 - acc: 0.9677 - val_loss: 252.9397 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 971ms/step - loss: 252.9237 - acc: 0.9677 - val_loss: 238.2073 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 238.1801 - acc: 1.0000 - val_loss: 224.1788 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 854ms/step - loss: 224.1505 - acc: 1.0000 - val_loss: 210.9113 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 210.8794 - acc: 1.0000 - val_loss: 198.2575 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 198.2351 - acc: 1.0000 - val_loss: 186.2428 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 186.2163 - acc: 1.0000 - val_loss: 174.8005 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 174.7730 - acc: 1.0000 - val_loss: 164.0560 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 164.0290 - acc: 1.0000 - val_loss: 153.9046 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 153.8816 - acc: 1.0000 - val_loss: 144.3052 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 144.2851 - acc: 1.0000 - val_loss: 135.2796 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 135.2611 - acc: 1.0000 - val_loss: 126.6227 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 126.6085 - acc: 1.0000 - val_loss: 118.5879 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 118.5754 - acc: 1.0000 - val_loss: 110.9748 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 110.9644 - acc: 1.0000 - val_loss: 103.8143 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 103.8053 - acc: 1.0000 - val_loss: 97.1371 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 97.1289 - acc: 1.0000 - val_loss: 90.7044 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 90.6978 - acc: 1.0000 - val_loss: 84.7520 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 84.7465 - acc: 1.0000 - val_loss: 79.1178 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 79.1131 - acc: 1.0000 - val_loss: 73.8272 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 73.8229 - acc: 1.0000 - val_loss: 68.9462 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 68.9425 - acc: 1.0000 - val_loss: 64.3422 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59db0fbbc26f4d61886bb2179e4fbfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 357.5934 - acc: 0.3548 - val_loss: 338.2161 - val_acc: 0.7333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 338.2161 - acc: 0.7742 - val_loss: 319.5407 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 319.5362 - acc: 0.8387 - val_loss: 301.6595 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 931ms/step - loss: 301.6355 - acc: 0.9677 - val_loss: 284.6671 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 284.6450 - acc: 0.9677 - val_loss: 268.3747 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 268.3280 - acc: 1.0000 - val_loss: 252.8369 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 593ms/step - loss: 252.8082 - acc: 1.0000 - val_loss: 238.0823 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 238.0436 - acc: 1.0000 - val_loss: 224.0535 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 910s 910s/step - loss: 224.0257 - acc: 1.0000 - val_loss: 210.7862 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 210.7603 - acc: 1.0000 - val_loss: 198.1420 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 869ms/step - loss: 198.1219 - acc: 1.0000 - val_loss: 186.1408 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 186.1225 - acc: 1.0000 - val_loss: 174.7118 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 174.6958 - acc: 1.0000 - val_loss: 163.9835 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 163.9702 - acc: 1.0000 - val_loss: 153.8471 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 153.8367 - acc: 1.0000 - val_loss: 144.2499 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 144.2412 - acc: 1.0000 - val_loss: 135.2275 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 135.2208 - acc: 1.0000 - val_loss: 126.5651 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 126.5605 - acc: 1.0000 - val_loss: 118.5366 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 604ms/step - loss: 118.5326 - acc: 1.0000 - val_loss: 110.9287 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 110.9253 - acc: 1.0000 - val_loss: 103.7642 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 103.7614 - acc: 1.0000 - val_loss: 97.0785 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 97.0762 - acc: 1.0000 - val_loss: 90.6312 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 90.6293 - acc: 1.0000 - val_loss: 84.6748 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 84.6731 - acc: 1.0000 - val_loss: 79.0329 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 79.0315 - acc: 1.0000 - val_loss: 73.7422 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4bc7adb6cd47ffaa23f5cbdea0f2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 8s 8s/step - loss: 357.5322 - acc: 0.5161 - val_loss: 338.1365 - val_acc: 0.6000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 338.1193 - acc: 0.7419 - val_loss: 319.4359 - val_acc: 0.8667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 319.4146 - acc: 0.9032 - val_loss: 301.5117 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 301.4879 - acc: 0.9677 - val_loss: 284.4863 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 284.4591 - acc: 1.0000 - val_loss: 268.1842 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 599ms/step - loss: 268.1471 - acc: 1.0000 - val_loss: 252.6568 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 792ms/step - loss: 252.6333 - acc: 1.0000 - val_loss: 237.9218 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 754ms/step - loss: 237.8960 - acc: 1.0000 - val_loss: 223.9152 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 223.9012 - acc: 1.0000 - val_loss: 210.6673 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 210.6507 - acc: 1.0000 - val_loss: 198.0425 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 198.0310 - acc: 1.0000 - val_loss: 186.0642 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 186.0553 - acc: 1.0000 - val_loss: 174.6606 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 553ms/step - loss: 174.6534 - acc: 1.0000 - val_loss: 163.9544 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 163.9486 - acc: 1.0000 - val_loss: 153.8314 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 153.8271 - acc: 1.0000 - val_loss: 144.2478 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1040s 1040s/step - loss: 144.2443 - acc: 1.0000 - val_loss: 135.2200 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 135.2174 - acc: 1.0000 - val_loss: 126.5588 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 740ms/step - loss: 126.5569 - acc: 1.0000 - val_loss: 118.5257 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 118.5242 - acc: 1.0000 - val_loss: 110.9060 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 110.9045 - acc: 1.0000 - val_loss: 103.7389 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 103.7376 - acc: 1.0000 - val_loss: 97.0609 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 97.0598 - acc: 1.0000 - val_loss: 90.6252 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 90.6243 - acc: 1.0000 - val_loss: 84.6716 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e432d2425e4b15973c3e2da84cf671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 989s 989s/step - loss: 357.4895 - acc: 0.6452 - val_loss: 338.0745 - val_acc: 0.6000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 336s 336s/step - loss: 338.0577 - acc: 0.7097 - val_loss: 319.3394 - val_acc: 0.9333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 319.3140 - acc: 0.9677 - val_loss: 301.4055 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 301.3809 - acc: 0.9677 - val_loss: 284.3891 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 907ms/step - loss: 284.3636 - acc: 1.0000 - val_loss: 268.0980 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 268.0643 - acc: 1.0000 - val_loss: 252.5803 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 799ms/step - loss: 252.5600 - acc: 1.0000 - val_loss: 237.8524 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 964ms/step - loss: 237.8323 - acc: 1.0000 - val_loss: 223.8480 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 917ms/step - loss: 223.8361 - acc: 1.0000 - val_loss: 210.6087 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 510ms/step - loss: 210.5969 - acc: 1.0000 - val_loss: 197.9949 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 802ms/step - loss: 197.9875 - acc: 1.0000 - val_loss: 186.0328 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 186.0275 - acc: 1.0000 - val_loss: 174.6481 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 174.6439 - acc: 1.0000 - val_loss: 163.9528 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 163.9495 - acc: 1.0000 - val_loss: 153.8275 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 873ms/step - loss: 153.8254 - acc: 1.0000 - val_loss: 144.2459 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 144.2441 - acc: 1.0000 - val_loss: 135.2341 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 135.2327 - acc: 1.0000 - val_loss: 126.5814 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 126.5804 - acc: 1.0000 - val_loss: 118.5617 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 118.5609 - acc: 1.0000 - val_loss: 110.9459 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 110.9451 - acc: 1.0000 - val_loss: 103.7644 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 103.7637 - acc: 1.0000 - val_loss: 97.0924 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 97.0919 - acc: 1.0000 - val_loss: 90.6581 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 90.6576 - acc: 1.0000 - val_loss: 84.7188 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[DeepeST:] Creating a model to test set\n",
      "[DeepeST:] Evaluation Config - bilstm-100-concat-0-0.5-50-64-1000-20-val_acc-ada-0.001-CCE-{}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa13d9ec602b4aa4bade804fe12cd371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Testing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6f3490170f495c943a405cbd58799b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 10s 10s/step - loss: 64.5883 - acc: 0.2903 - val_loss: 62.9937 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 62.9945 - acc: 0.5806 - val_loss: 61.4341 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 61.4188 - acc: 0.5484 - val_loss: 59.9017 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 59.8925 - acc: 0.5484 - val_loss: 58.3987 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 58.3827 - acc: 0.6129 - val_loss: 56.9187 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 56.8884 - acc: 0.5806 - val_loss: 55.4674 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 55.4487 - acc: 0.6452 - val_loss: 54.0450 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 54.0332 - acc: 0.7097 - val_loss: 52.6466 - val_acc: 0.5333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 52.6339 - acc: 0.6774 - val_loss: 51.2754 - val_acc: 0.6667\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 712ms/step - loss: 51.2643 - acc: 0.8065 - val_loss: 49.9246 - val_acc: 0.8667\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 746ms/step - loss: 49.8954 - acc: 0.9677 - val_loss: 48.5979 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 630ms/step - loss: 48.5577 - acc: 0.9677 - val_loss: 47.2946 - val_acc: 0.9333\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 47.2518 - acc: 1.0000 - val_loss: 46.0189 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 45.9735 - acc: 1.0000 - val_loss: 44.7661 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 44.7154 - acc: 1.0000 - val_loss: 43.5364 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 43.4778 - acc: 1.0000 - val_loss: 42.3295 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 42.2638 - acc: 1.0000 - val_loss: 41.1407 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 41.0848 - acc: 1.0000 - val_loss: 39.9806 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 39.9215 - acc: 1.0000 - val_loss: 38.8414 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 38.7665 - acc: 1.0000 - val_loss: 37.7234 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 37.6482 - acc: 1.0000 - val_loss: 36.6313 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 1s 522ms/step - loss: 36.5481 - acc: 1.0000 - val_loss: 35.5566 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 35.4704 - acc: 1.0000 - val_loss: 34.5077 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 34.4261 - acc: 1.0000 - val_loss: 33.4812 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 33.4028 - acc: 1.0000 - val_loss: 32.4794 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 32.3992 - acc: 1.0000 - val_loss: 31.5035 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 31.4272 - acc: 1.0000 - val_loss: 30.5499 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 30.4750 - acc: 1.0000 - val_loss: 29.6159 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 29.5448 - acc: 1.0000 - val_loss: 28.7044 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 28.6377 - acc: 1.0000 - val_loss: 27.8176 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 27.7604 - acc: 1.0000 - val_loss: 26.9509 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 26.8967 - acc: 1.0000 - val_loss: 26.1095 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 26.0572 - acc: 1.0000 - val_loss: 25.2885 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[DeepeST:] Processing time: 7162718.627 milliseconds. Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       " 0       1.0            1.0                1.0              1.0           1.0   \n",
       " \n",
       "    f1_macro      clstime  \n",
       " 0       1.0  7162709.584  ,\n",
       " array([[0.5078445 , 0.49215555],\n",
       "        [0.57725966, 0.42274037],\n",
       "        [0.5457003 , 0.4542997 ],\n",
       "        [0.5591361 , 0.44086394],\n",
       "        [0.54578936, 0.45421058],\n",
       "        [0.5282849 , 0.4717151 ],\n",
       "        [0.5798541 , 0.42014593],\n",
       "        [0.33536586, 0.66463417],\n",
       "        [0.38572234, 0.6142777 ],\n",
       "        [0.35664645, 0.6433535 ],\n",
       "        [0.32164967, 0.6783504 ],\n",
       "        [0.29013833, 0.70986164],\n",
       "        [0.3322852 , 0.66771483],\n",
       "        [0.3454447 , 0.6545553 ],\n",
       "        [0.3956218 , 0.6043782 ]], dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matclassification.methods import DeepeST\n",
    "\n",
    "model = DeepeST()\n",
    "model.prepare_input(train, test)\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>clstime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7162709.584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0       1.0            1.0                1.0              1.0           1.0   \n",
       "\n",
       "   f1_macro      clstime  \n",
       "0       1.0  7162709.584  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.3. Trajectory Random Forrest (TRF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-30 21:03:12.110291\n",
      "\n",
      "[TRF:] Building model\n",
      "\n",
      "###########    DATA PREPARATION      ###########\n",
      "Attribute 'space': 151 unique values\n",
      "Attribute 'time': 434 unique values\n",
      "Attribute 'day': 7 unique values\n",
      "Attribute 'poi': 137 unique values\n",
      "Attribute 'type': 85 unique values\n",
      "Attribute 'root_type': 10 unique values\n",
      "Attribute 'rating': 41 unique values\n",
      "Attribute 'weather': 5 unique values\n",
      "Attribute 'tid': 31 unique values\n",
      "Total of attribute/value pairs: 901\n",
      "\n",
      "\n",
      "###########      DATA ENCODING        ###########\n",
      "Checking sets split count (train, <validation>, test):\n",
      "   TIDs_0: 31\n",
      "   TIDs_1: 15\n",
      "Encoding string data to integer\n",
      "   Encoding: space\n",
      "   Encoding: time\n",
      "   Encoding: day\n",
      "   Encoding: poi\n",
      "   Encoding: type\n",
      "   Encoding: root_type\n",
      "   Encoding: rating\n",
      "   Encoding: weather\n",
      "Label encoding on label y\n",
      "[TRF:] Training hiperparameter model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ba6b6d77b3498880f9468f6fdca2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[TRF:] Model Training:   0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRF:] Creating a model to test set\n",
      "[TRF:] Evaluation Config - 200-20-2-1-sqrt-True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3710e734a29449794fc5dc919180512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Testing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRF:] Processing time: 638640.533 milliseconds. Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       " 0       1.0            1.0                1.0              1.0           1.0   \n",
       " \n",
       "    f1_macro     clstime  \n",
       " 0       1.0  638639.426  ,\n",
       " array([[0.88 , 0.12 ],\n",
       "        [0.785, 0.215],\n",
       "        [0.91 , 0.09 ],\n",
       "        [0.955, 0.045],\n",
       "        [0.94 , 0.06 ],\n",
       "        [0.925, 0.075],\n",
       "        [0.945, 0.055],\n",
       "        [0.065, 0.935],\n",
       "        [0.06 , 0.94 ],\n",
       "        [0.12 , 0.88 ],\n",
       "        [0.05 , 0.95 ],\n",
       "        [0.07 , 0.93 ],\n",
       "        [0.08 , 0.92 ],\n",
       "        [0.1  , 0.9  ],\n",
       "        [0.055, 0.945]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matclassification.methods import TRF\n",
    "\n",
    "model = TRF()\n",
    "model.prepare_input(train, test)\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>clstime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>638639.426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0       1.0            1.0                1.0              1.0           1.0   \n",
       "\n",
       "   f1_macro     clstime  \n",
       "0       1.0  638639.426  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2d99a7fd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.cm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.4. Trajectory XGBoost (TXGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-30 21:13:50.905706\n",
      "\n",
      "[TXGB:] Building model\n",
      "\n",
      "###########    DATA PREPARATION      ###########\n",
      "Attribute 'space': 151 unique values\n",
      "Attribute 'time': 434 unique values\n",
      "Attribute 'day': 7 unique values\n",
      "Attribute 'poi': 137 unique values\n",
      "Attribute 'type': 85 unique values\n",
      "Attribute 'root_type': 10 unique values\n",
      "Attribute 'rating': 41 unique values\n",
      "Attribute 'weather': 5 unique values\n",
      "Attribute 'tid': 31 unique values\n",
      "Total of attribute/value pairs: 901\n",
      "\n",
      "\n",
      "###########      DATA ENCODING        ###########\n",
      "Checking sets split count (train, <validation>, test):\n",
      "   TIDs_0: 31\n",
      "   TIDs_1: 15\n",
      "Encoding string data to integer\n",
      "   Encoding: space\n",
      "   Encoding: time\n",
      "   Encoding: day\n",
      "   Encoding: poi\n",
      "   Encoding: type\n",
      "   Encoding: root_type\n",
      "   Encoding: rating\n",
      "   Encoding: weather\n",
      "Label encoding on label y\n",
      "[TXGB:] Training hiperparameter model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd097249fb94238977e618a0579c28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[TXGB:] Model Training:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXGB:] Creating a model to test set\n",
      "[TXGB:] Evaluation Config - 2000-3-0.01-0.0-0.1-0.5-1.0-100-merror-20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0b360c12a8475caef6e8a02fb18eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Testing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXGB:] Processing time: 10199.032000000001 milliseconds. Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       " 0       1.0              0                1.0              1.0           1.0   \n",
       " \n",
       "    f1_macro    clstime  \n",
       " 0       1.0  10198.058  ,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matclassification.methods import TXGB\n",
    "\n",
    "model = TXGB()\n",
    "model.prepare_input(train, test)\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>clstime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10198.058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0       1.0            0.0                1.0              1.0           1.0   \n",
       "\n",
       "   f1_macro    clstime  \n",
       "0       1.0  10198.058  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.5. BITULER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-30 21:14:01.233938\n",
      "\n",
      "[Bituler:] Building model\n",
      "\n",
      "###########    DATA PREPARATION      ###########\n",
      "Attribute 'poi': 137 unique values\n",
      "Total of attribute/value pairs: 137\n",
      "\n",
      "\n",
      "###########      DATA ENCODING        ###########\n",
      "\n",
      "Input total: 2\n",
      "... tid_0: 31\n",
      "... tid_1: 15\n",
      "col_name: ['poi', 'tid', 'label']...\n",
      "... num_classes: 2\n",
      "... max_lenght: 38\n",
      "Removing column tid of attr\n",
      "Removing column label of attr\n",
      "\n",
      "\n",
      "#####   Encoding string data to integer   ######\n",
      "   Encoding: poi\n",
      "\n",
      "\n",
      "###########      Generating y_train and y_test     ###########\n",
      "Label encoding on label y\n",
      "Input total: 2\n",
      "[Bituler:] Training hiperparameter model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf58a06797804c4d87923812da519c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Bituler:] Model Training:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6977 - acc: 0.3548 - val_loss: 0.6903 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6853 - acc: 0.6129 - val_loss: 0.6836 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 871ms/step - loss: 0.6783 - acc: 0.5806 - val_loss: 0.6766 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 899ms/step - loss: 0.6716 - acc: 0.5484 - val_loss: 0.6686 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6591 - acc: 0.5484 - val_loss: 0.6587 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 630ms/step - loss: 0.6411 - acc: 0.5806 - val_loss: 0.6463 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 947ms/step - loss: 0.6383 - acc: 0.5806 - val_loss: 0.6297 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.6198 - acc: 0.7097 - val_loss: 0.6073 - val_acc: 0.6667\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.5917 - acc: 0.8387 - val_loss: 0.5770 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.5747 - acc: 0.8710 - val_loss: 0.5368 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 0.5168 - acc: 1.0000 - val_loss: 0.4861 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 0.4836 - acc: 1.0000 - val_loss: 0.4317 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4135 - acc: 0.9677 - val_loss: 0.3941 - val_acc: 0.8667\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3746 - acc: 0.9032 - val_loss: 0.3413 - val_acc: 0.9333\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3325 - acc: 0.9032 - val_loss: 0.2774 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.2574 - acc: 1.0000 - val_loss: 0.2386 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.2105 - acc: 1.0000 - val_loss: 0.2127 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 0.1812 - acc: 1.0000 - val_loss: 0.1683 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1391 - acc: 1.0000 - val_loss: 0.1001 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0800 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0580 - acc: 1.0000 - val_loss: 0.0353 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0395 - acc: 1.0000 - val_loss: 0.0276 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0476 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0304 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.3591 - val_acc: 0.8000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0832 - acc: 1.0000 - val_loss: 0.0566 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.1490e-04 - acc: 1.0000 - val_loss: 3.7746e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.1372e-04 - acc: 1.0000 - val_loss: 2.2421e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.3615e-04 - acc: 1.0000 - val_loss: 1.8220e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.0937e-04 - acc: 1.0000 - val_loss: 1.6127e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.9500e-04 - acc: 1.0000 - val_loss: 1.4716e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9853e-04 - acc: 1.0000 - val_loss: 1.3504e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.0848e-04 - acc: 1.0000 - val_loss: 1.2293e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.6772e-04 - acc: 1.0000 - val_loss: 1.0945e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0629e-04 - acc: 1.0000 - val_loss: 9.7382e-05 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.4216e-04 - acc: 1.0000 - val_loss: 8.6789e-05 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.4079e-04 - acc: 1.0000 - val_loss: 7.7738e-05 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 9.0562e-05 - acc: 1.0000 - val_loss: 7.0219e-05 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 9.9366e-05 - acc: 1.0000 - val_loss: 6.3961e-05 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.2838e-04 - acc: 1.0000 - val_loss: 5.8739e-05 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.2109e-05 - acc: 1.0000 - val_loss: 5.4431e-05 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.9636e-05 - acc: 1.0000 - val_loss: 5.0834e-05 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.3271e-05 - acc: 1.0000 - val_loss: 4.7820e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.7908e-05 - acc: 1.0000 - val_loss: 4.5309e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.2726e-05 - acc: 1.0000 - val_loss: 4.3155e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.4474e-05 - acc: 1.0000 - val_loss: 4.1320e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.1520e-05 - acc: 1.0000 - val_loss: 3.9745e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.8774e-05 - acc: 1.0000 - val_loss: 3.8379e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.5916e-05 - acc: 1.0000 - val_loss: 3.7189e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.4957e-05 - acc: 1.0000 - val_loss: 3.6137e-05 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.5175e-05 - acc: 1.0000 - val_loss: 3.5189e-05 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.8495e-05 - acc: 1.0000 - val_loss: 3.4368e-05 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 4.0688e-05 - acc: 1.0000 - val_loss: 3.3680e-05 - val_acc: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 4.8642e-05 - acc: 1.0000 - val_loss: 3.3008e-05 - val_acc: 1.0000\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.5249e-05 - acc: 1.0000 - val_loss: 3.2418e-05 - val_acc: 1.0000\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.3948e-05 - acc: 1.0000 - val_loss: 3.1884e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6907 - acc: 0.5484 - val_loss: 0.6807 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6702 - acc: 0.6774 - val_loss: 0.6687 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6579 - acc: 0.7097 - val_loss: 0.6549 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 700ms/step - loss: 0.6454 - acc: 0.6774 - val_loss: 0.6377 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6271 - acc: 0.7097 - val_loss: 0.6153 - val_acc: 0.6667\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 0.5885 - acc: 0.8710 - val_loss: 0.5859 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 928ms/step - loss: 0.5876 - acc: 0.8387 - val_loss: 0.5471 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.5340 - acc: 0.9677 - val_loss: 0.4963 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.4930 - acc: 1.0000 - val_loss: 0.4334 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 507ms/step - loss: 0.4210 - acc: 1.0000 - val_loss: 0.3674 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 0.3491 - acc: 0.9677 - val_loss: 0.3154 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.3085 - acc: 0.9032 - val_loss: 0.2476 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 680ms/step - loss: 0.2357 - acc: 0.9677 - val_loss: 0.1921 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.1591 - acc: 1.0000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 0.1304 - acc: 1.0000 - val_loss: 0.1297 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 0.0927 - acc: 1.0000 - val_loss: 0.0906 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0622 - acc: 1.0000 - val_loss: 0.0505 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0384 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0274 - acc: 1.0000 - val_loss: 0.0209 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0234 - acc: 1.0000 - val_loss: 0.0129 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0120 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0235 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0788 - acc: 0.9677 - val_loss: 0.0183 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0231 - acc: 1.0000 - val_loss: 0.2938 - val_acc: 0.8000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0950 - acc: 0.9355 - val_loss: 0.2885 - val_acc: 0.7333\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0696 - acc: 0.9677 - val_loss: 0.1620 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0445 - acc: 1.0000 - val_loss: 0.0759 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.0399 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.0260 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0201 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.0178 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0169 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0168 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.0169 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.0171 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.0173 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0174 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.0175 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.0174 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0169 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.0164 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.0159 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.0153 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0138 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0130 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0122 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0114 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 997ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6861 - acc: 0.6129 - val_loss: 0.6702 - val_acc: 0.6000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 913ms/step - loss: 0.6659 - acc: 0.7419 - val_loss: 0.6556 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 689ms/step - loss: 0.6515 - acc: 0.6774 - val_loss: 0.6389 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 0.6293 - acc: 0.6129 - val_loss: 0.6180 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 997ms/step - loss: 0.6147 - acc: 0.6452 - val_loss: 0.5918 - val_acc: 0.7333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5655 - acc: 0.8387 - val_loss: 0.5584 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 604ms/step - loss: 0.5436 - acc: 0.9032 - val_loss: 0.5154 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 0.4903 - acc: 0.9677 - val_loss: 0.4608 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.4381 - acc: 1.0000 - val_loss: 0.3942 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.3744 - acc: 1.0000 - val_loss: 0.3181 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 661ms/step - loss: 0.2946 - acc: 1.0000 - val_loss: 0.2407 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.2247 - acc: 1.0000 - val_loss: 0.1748 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.1605 - acc: 1.0000 - val_loss: 0.1224 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.1144 - acc: 1.0000 - val_loss: 0.0764 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.0725 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 0.0387 - acc: 1.0000 - val_loss: 0.0453 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0276 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0361 - acc: 1.0000 - val_loss: 0.0228 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.5523e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.9130e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 6.1936e-04 - acc: 1.0000 - val_loss: 9.1494e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 6.5971e-04 - acc: 1.0000 - val_loss: 8.2438e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.2783e-04 - acc: 1.0000 - val_loss: 7.4687e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.3753e-04 - acc: 1.0000 - val_loss: 6.8268e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 5.3507e-04 - acc: 1.0000 - val_loss: 6.2867e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.8628e-04 - acc: 1.0000 - val_loss: 5.8258e-04 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.9084e-04 - acc: 1.0000 - val_loss: 5.4281e-04 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 6.5807e-04 - acc: 1.0000 - val_loss: 5.0721e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.6415e-04 - acc: 1.0000 - val_loss: 4.7508e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 5.2946e-04 - acc: 1.0000 - val_loss: 4.4582e-04 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.8715e-04 - acc: 1.0000 - val_loss: 4.1972e-04 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.7071e-04 - acc: 1.0000 - val_loss: 3.9673e-04 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.7993e-04 - acc: 1.0000 - val_loss: 3.7576e-04 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.1592e-04 - acc: 1.0000 - val_loss: 3.5675e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6950 - acc: 0.3871 - val_loss: 0.6788 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6749 - acc: 0.5806 - val_loss: 0.6620 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6530 - acc: 0.5484 - val_loss: 0.6430 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6328 - acc: 0.6129 - val_loss: 0.6192 - val_acc: 0.6667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 763ms/step - loss: 0.6030 - acc: 0.7419 - val_loss: 0.5890 - val_acc: 0.8667\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5631 - acc: 0.9677 - val_loss: 0.5518 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 0.5408 - acc: 0.9355 - val_loss: 0.5051 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.4934 - acc: 0.9677 - val_loss: 0.4470 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.4376 - acc: 1.0000 - val_loss: 0.3776 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.3720 - acc: 1.0000 - val_loss: 0.3011 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.2871 - acc: 1.0000 - val_loss: 0.2254 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.2335 - acc: 1.0000 - val_loss: 0.1570 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 0.1499 - acc: 1.0000 - val_loss: 0.0975 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 794ms/step - loss: 0.0907 - acc: 1.0000 - val_loss: 0.0650 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0638 - acc: 1.0000 - val_loss: 0.0668 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0454 - acc: 1.0000 - val_loss: 0.0262 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0241 - acc: 1.0000 - val_loss: 0.0158 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0088 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0200 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.2964 - val_acc: 0.8667\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.2484 - acc: 0.9032 - val_loss: 0.1735 - val_acc: 0.9333\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0811 - acc: 0.9355 - val_loss: 0.7313 - val_acc: 0.6000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2005 - acc: 0.9032 - val_loss: 0.3817 - val_acc: 0.7333\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0979 - acc: 0.9355 - val_loss: 0.0891 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0405 - acc: 1.0000 - val_loss: 0.0226 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.0102 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.2602e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.5632e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.1852e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 986ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6934 - acc: 0.5484 - val_loss: 0.6890 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6855 - acc: 0.5484 - val_loss: 0.6818 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6784 - acc: 0.5484 - val_loss: 0.6728 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6703 - acc: 0.5484 - val_loss: 0.6593 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 685ms/step - loss: 0.6580 - acc: 0.5484 - val_loss: 0.6379 - val_acc: 0.7333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.6232 - acc: 0.7419 - val_loss: 0.6030 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 590ms/step - loss: 0.5943 - acc: 0.9355 - val_loss: 0.5478 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.5354 - acc: 1.0000 - val_loss: 0.4983 - val_acc: 0.7333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.4873 - acc: 0.7419 - val_loss: 0.4191 - val_acc: 0.8667\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.4079 - acc: 0.9032 - val_loss: 0.3541 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.3299 - acc: 1.0000 - val_loss: 0.2672 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 473ms/step - loss: 0.2300 - acc: 1.0000 - val_loss: 0.2652 - val_acc: 0.8667\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 468ms/step - loss: 0.2725 - acc: 0.8387 - val_loss: 0.3048 - val_acc: 0.9333\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 0.2194 - acc: 0.9355 - val_loss: 0.3093 - val_acc: 0.8667\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.2079 - acc: 0.9355 - val_loss: 0.1265 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0756 - acc: 1.0000 - val_loss: 0.0542 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0352 - acc: 1.0000 - val_loss: 0.0299 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.0112 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 7.7970e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.1420e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.4456e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5.3108e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.9860e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.5194e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.0600e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.6729e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 6.1465e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 4.1359e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.4367e-04 - acc: 1.0000 - val_loss: 6.7043e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.1721e-04 - acc: 1.0000 - val_loss: 3.8998e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.7049e-04 - acc: 1.0000 - val_loss: 2.1682e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.5721e-05 - acc: 1.0000 - val_loss: 1.2110e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.9748e-05 - acc: 1.0000 - val_loss: 7.0604e-05 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.1487e-05 - acc: 1.0000 - val_loss: 4.4609e-05 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.9429e-05 - acc: 1.0000 - val_loss: 3.1625e-05 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.2451e-05 - acc: 1.0000 - val_loss: 2.5902e-05 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.7414e-05 - acc: 1.0000 - val_loss: 2.4569e-05 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.1369e-05 - acc: 1.0000 - val_loss: 2.6497e-05 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.2474e-05 - acc: 1.0000 - val_loss: 3.0683e-05 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1355e-04 - acc: 1.0000 - val_loss: 3.3052e-05 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 6.1282e-05 - acc: 1.0000 - val_loss: 3.3647e-05 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 6.3758e-05 - acc: 1.0000 - val_loss: 3.2005e-05 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 1.8680e-05 - acc: 1.0000 - val_loss: 3.0088e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.1840e-05 - acc: 1.0000 - val_loss: 2.7064e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.2956e-05 - acc: 1.0000 - val_loss: 2.3781e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 9.2636e-05 - acc: 1.0000 - val_loss: 1.9054e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.6375e-05 - acc: 1.0000 - val_loss: 1.5006e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5381e-05 - acc: 1.0000 - val_loss: 1.2868e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0134e-05 - acc: 1.0000 - val_loss: 1.2003e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.0711e-05 - acc: 1.0000 - val_loss: 1.1992e-05 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.9529e-06 - acc: 1.0000 - val_loss: 1.2620e-05 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5.4402e-06 - acc: 1.0000 - val_loss: 1.3678e-05 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.9791e-06 - acc: 1.0000 - val_loss: 1.5121e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6894 - acc: 0.5484 - val_loss: 0.6763 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6782 - acc: 0.6774 - val_loss: 0.6592 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6522 - acc: 0.7419 - val_loss: 0.6366 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6409 - acc: 0.7097 - val_loss: 0.6032 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.6097 - acc: 0.9355 - val_loss: 0.5521 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.5433 - acc: 1.0000 - val_loss: 0.4810 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 834ms/step - loss: 0.4870 - acc: 0.9355 - val_loss: 0.4229 - val_acc: 0.8000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 806ms/step - loss: 0.4213 - acc: 0.7742 - val_loss: 0.3288 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.3288 - acc: 0.9677 - val_loss: 0.2913 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 877ms/step - loss: 0.2740 - acc: 1.0000 - val_loss: 0.1885 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.1688 - acc: 1.0000 - val_loss: 0.1507 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1718 - acc: 0.9032 - val_loss: 0.1082 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 616ms/step - loss: 0.0780 - acc: 1.0000 - val_loss: 0.2154 - val_acc: 0.9333\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.1343 - acc: 1.0000 - val_loss: 0.0623 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0408 - acc: 1.0000 - val_loss: 0.0271 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0300 - acc: 1.0000 - val_loss: 0.0367 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0514 - acc: 1.0000 - val_loss: 0.0204 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 645ms/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0144 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.0167 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.0195 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0219 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0229 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0222 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0198 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0167 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0132 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.3039e-04 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 5.0683e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.4032e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.3525e-04 - acc: 1.0000 - val_loss: 7.2701e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.7326e-04 - acc: 1.0000 - val_loss: 4.9784e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.6291e-04 - acc: 1.0000 - val_loss: 3.5251e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.7800e-05 - acc: 1.0000 - val_loss: 2.5815e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.2312e-05 - acc: 1.0000 - val_loss: 1.9484e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 4.3591e-05 - acc: 1.0000 - val_loss: 1.5122e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.9215e-05 - acc: 1.0000 - val_loss: 1.2028e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.3546e-05 - acc: 1.0000 - val_loss: 9.7789e-05 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.6682e-05 - acc: 1.0000 - val_loss: 8.1043e-05 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.5198e-05 - acc: 1.0000 - val_loss: 6.8346e-05 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0626e-05 - acc: 1.0000 - val_loss: 5.8502e-05 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7460e-05 - acc: 1.0000 - val_loss: 5.0795e-05 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7681e-05 - acc: 1.0000 - val_loss: 4.4648e-05 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.1212e-05 - acc: 1.0000 - val_loss: 3.9695e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.9479e-05 - acc: 1.0000 - val_loss: 3.5652e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.5220e-05 - acc: 1.0000 - val_loss: 3.2336e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.3594e-05 - acc: 1.0000 - val_loss: 2.9570e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.3375e-05 - acc: 1.0000 - val_loss: 2.7262e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.3335e-05 - acc: 1.0000 - val_loss: 2.5285e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.2354e-05 - acc: 1.0000 - val_loss: 2.3621e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 9.9849e-06 - acc: 1.0000 - val_loss: 2.2205e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6939 - acc: 0.4516 - val_loss: 0.6731 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6675 - acc: 0.5806 - val_loss: 0.6522 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6412 - acc: 0.5484 - val_loss: 0.6239 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 893ms/step - loss: 0.6165 - acc: 0.6452 - val_loss: 0.5809 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 918ms/step - loss: 0.5725 - acc: 0.8710 - val_loss: 0.5190 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 784ms/step - loss: 0.5039 - acc: 1.0000 - val_loss: 0.4341 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 683ms/step - loss: 0.4274 - acc: 1.0000 - val_loss: 0.3378 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 657ms/step - loss: 0.3399 - acc: 0.9677 - val_loss: 0.2278 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.2314 - acc: 0.9355 - val_loss: 0.1744 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1530 - acc: 1.0000 - val_loss: 0.0705 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0642 - acc: 1.0000 - val_loss: 0.2830 - val_acc: 0.8667\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.3070 - acc: 0.8710 - val_loss: 0.3268 - val_acc: 0.6667\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.1912 - acc: 0.9032 - val_loss: 0.3927 - val_acc: 0.6667\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 0.2322 - acc: 0.8710 - val_loss: 0.1721 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0924 - acc: 1.0000 - val_loss: 0.0909 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0486 - acc: 1.0000 - val_loss: 0.0648 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.0390 - acc: 1.0000 - val_loss: 0.0535 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0344 - acc: 1.0000 - val_loss: 0.0460 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 0.0284 - acc: 1.0000 - val_loss: 0.0390 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0248 - acc: 1.0000 - val_loss: 0.0315 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.0239 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0133 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.0102 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.0340e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.0185e-04 - acc: 1.0000 - val_loss: 9.8812e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5.7573e-04 - acc: 1.0000 - val_loss: 8.5483e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.9501e-04 - acc: 1.0000 - val_loss: 7.4638e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 4.3572e-04 - acc: 1.0000 - val_loss: 6.5660e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.3428e-04 - acc: 1.0000 - val_loss: 5.8195e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.1308e-04 - acc: 1.0000 - val_loss: 5.1982e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.4047e-04 - acc: 1.0000 - val_loss: 4.6790e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6903e-04 - acc: 1.0000 - val_loss: 4.2339e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.3157e-04 - acc: 1.0000 - val_loss: 3.8540e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.9924e-04 - acc: 1.0000 - val_loss: 3.5270e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.1005e-04 - acc: 1.0000 - val_loss: 3.2407e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.7472e-04 - acc: 1.0000 - val_loss: 2.9904e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.6623e-04 - acc: 1.0000 - val_loss: 2.7712e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.4936e-04 - acc: 1.0000 - val_loss: 2.5799e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.3276e-04 - acc: 1.0000 - val_loss: 2.4110e-04 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.2909e-04 - acc: 1.0000 - val_loss: 2.2612e-04 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.1397e-04 - acc: 1.0000 - val_loss: 2.1279e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0467e-04 - acc: 1.0000 - val_loss: 2.0086e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.0468e-04 - acc: 1.0000 - val_loss: 1.9020e-04 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.5610e-05 - acc: 1.0000 - val_loss: 1.8062e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.6856 - acc: 0.6129 - val_loss: 0.6703 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6592 - acc: 0.5484 - val_loss: 0.6454 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6335 - acc: 0.6129 - val_loss: 0.6090 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 830ms/step - loss: 0.6085 - acc: 0.8065 - val_loss: 0.5538 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 602ms/step - loss: 0.5432 - acc: 0.9677 - val_loss: 0.4761 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.4588 - acc: 1.0000 - val_loss: 0.3824 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 691ms/step - loss: 0.3654 - acc: 0.9032 - val_loss: 0.2837 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.2792 - acc: 0.9355 - val_loss: 0.1891 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 822ms/step - loss: 0.1727 - acc: 1.0000 - val_loss: 0.1724 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 733ms/step - loss: 0.1341 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0446 - acc: 1.0000 - val_loss: 0.1131 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1687 - acc: 0.9032 - val_loss: 0.0998 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0483 - acc: 1.0000 - val_loss: 0.3405 - val_acc: 0.7333\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.1579 - acc: 0.9355 - val_loss: 0.2784 - val_acc: 0.8000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 0.1158 - acc: 1.0000 - val_loss: 0.1728 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0731 - acc: 1.0000 - val_loss: 0.1114 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0441 - acc: 1.0000 - val_loss: 0.0812 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0363 - acc: 1.0000 - val_loss: 0.0654 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0284 - acc: 1.0000 - val_loss: 0.0564 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0243 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0233 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.0218 - acc: 1.0000 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.0219 - acc: 1.0000 - val_loss: 0.0399 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.0367 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.0335 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.0302 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0269 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.0236 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0203 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0088 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.8896e-04 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 6.4504e-04 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 6.0691e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 4.6684e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 3.7390e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0839e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7308e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.2556e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.1127e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.6094e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.6753e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.4470e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.4619e-04 - acc: 1.0000 - val_loss: 9.5850e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1746e-04 - acc: 1.0000 - val_loss: 9.0569e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.2314e-04 - acc: 1.0000 - val_loss: 8.5892e-04 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.7828e-05 - acc: 1.0000 - val_loss: 8.1752e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6947 - acc: 0.4194 - val_loss: 0.6859 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6853 - acc: 0.6452 - val_loss: 0.6776 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 814ms/step - loss: 0.6714 - acc: 0.5806 - val_loss: 0.6666 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 599ms/step - loss: 0.6636 - acc: 0.5484 - val_loss: 0.6479 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 986ms/step - loss: 0.6424 - acc: 0.5806 - val_loss: 0.6142 - val_acc: 0.8000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 728ms/step - loss: 0.5930 - acc: 0.8710 - val_loss: 0.5534 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 0.5475 - acc: 0.9355 - val_loss: 0.4882 - val_acc: 0.7333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 0.4792 - acc: 0.7419 - val_loss: 0.4043 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 0.3834 - acc: 0.9677 - val_loss: 0.4006 - val_acc: 0.8667\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.3563 - acc: 0.9032 - val_loss: 0.2805 - val_acc: 0.8667\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 0.2785 - acc: 0.8387 - val_loss: 0.2655 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 583ms/step - loss: 0.2095 - acc: 1.0000 - val_loss: 0.1948 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 519ms/step - loss: 0.1412 - acc: 1.0000 - val_loss: 0.1100 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 498ms/step - loss: 0.1073 - acc: 1.0000 - val_loss: 0.0525 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0491 - acc: 1.0000 - val_loss: 0.0811 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0243 - acc: 1.0000 - val_loss: 0.0908 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0289 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 5.0683e-04 - acc: 1.0000 - val_loss: 2.6391e-04 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 4.1234e-04 - acc: 1.0000 - val_loss: 1.2467e-04 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.4395e-04 - acc: 1.0000 - val_loss: 8.1489e-05 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.1879e-04 - acc: 1.0000 - val_loss: 6.0832e-05 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 3.5404e-04 - acc: 1.0000 - val_loss: 4.4097e-05 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6595e-04 - acc: 1.0000 - val_loss: 2.9625e-05 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 6.7481e-06 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 9.5637e-06 - acc: 1.0000 - val_loss: 1.3006e-05 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.6357e-06 - acc: 1.0000 - val_loss: 5.1991e-05 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2080e-05 - acc: 1.0000 - val_loss: 1.5379e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.5410e-05 - acc: 1.0000 - val_loss: 3.1761e-04 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.1257e-04 - acc: 1.0000 - val_loss: 5.0617e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.4035e-04 - acc: 1.0000 - val_loss: 6.7880e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.3761e-04 - acc: 1.0000 - val_loss: 8.1306e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.5438e-04 - acc: 1.0000 - val_loss: 9.0262e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.8413e-04 - acc: 1.0000 - val_loss: 9.4983e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.8124e-04 - acc: 1.0000 - val_loss: 9.6274e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 1.8341e-04 - acc: 1.0000 - val_loss: 9.5025e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.1150e-04 - acc: 1.0000 - val_loss: 9.1863e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.4212e-04 - acc: 1.0000 - val_loss: 8.7732e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.3445e-04 - acc: 1.0000 - val_loss: 8.2532e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.4793e-04 - acc: 1.0000 - val_loss: 7.7101e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.3538e-04 - acc: 1.0000 - val_loss: 7.1695e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 9.7785e-05 - acc: 1.0000 - val_loss: 6.6612e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0448e-04 - acc: 1.0000 - val_loss: 6.1827e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.8749e-05 - acc: 1.0000 - val_loss: 5.7417e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.9957e-05 - acc: 1.0000 - val_loss: 5.3310e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5.6393e-05 - acc: 1.0000 - val_loss: 4.9622e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.6065e-05 - acc: 1.0000 - val_loss: 4.6217e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.4338e-05 - acc: 1.0000 - val_loss: 4.3030e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 5.2062e-05 - acc: 1.0000 - val_loss: 4.0147e-04 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.3286e-05 - acc: 1.0000 - val_loss: 3.7491e-04 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.2847e-05 - acc: 1.0000 - val_loss: 3.5126e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.5206e-05 - acc: 1.0000 - val_loss: 3.2968e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.1387e-05 - acc: 1.0000 - val_loss: 3.1003e-04 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.2426e-05 - acc: 1.0000 - val_loss: 2.9119e-04 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5.4738e-05 - acc: 1.0000 - val_loss: 2.7361e-04 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.7467e-05 - acc: 1.0000 - val_loss: 2.5801e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6901 - acc: 0.5484 - val_loss: 0.6738 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6727 - acc: 0.6129 - val_loss: 0.6535 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6478 - acc: 0.6774 - val_loss: 0.6244 - val_acc: 0.6000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 605ms/step - loss: 0.6145 - acc: 0.7419 - val_loss: 0.5771 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 0.5689 - acc: 0.9032 - val_loss: 0.4984 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 0.4867 - acc: 1.0000 - val_loss: 0.4035 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 798ms/step - loss: 0.4029 - acc: 0.8710 - val_loss: 0.3016 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 840ms/step - loss: 0.3064 - acc: 0.9355 - val_loss: 0.2729 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 659ms/step - loss: 0.2360 - acc: 1.0000 - val_loss: 0.1693 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1398 - acc: 1.0000 - val_loss: 0.4124 - val_acc: 0.7333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 0.5060 - acc: 0.7742 - val_loss: 0.1800 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1186 - acc: 1.0000 - val_loss: 0.4017 - val_acc: 0.6000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.2508 - acc: 0.8710 - val_loss: 0.3384 - val_acc: 0.7333\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.2245 - acc: 0.9355 - val_loss: 0.2371 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1552 - acc: 1.0000 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0977 - acc: 1.0000 - val_loss: 0.0983 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 0.0665 - acc: 1.0000 - val_loss: 0.0510 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0339 - acc: 1.0000 - val_loss: 0.0249 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0140 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 7.7069e-04 - acc: 1.0000 - val_loss: 7.9115e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 6.2919e-04 - acc: 1.0000 - val_loss: 6.0111e-04 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5.4167e-04 - acc: 1.0000 - val_loss: 4.6588e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.7574e-04 - acc: 1.0000 - val_loss: 3.6924e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9948e-04 - acc: 1.0000 - val_loss: 2.9921e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6648e-04 - acc: 1.0000 - val_loss: 2.4771e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.6603e-04 - acc: 1.0000 - val_loss: 2.0940e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.5463e-04 - acc: 1.0000 - val_loss: 1.8030e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.2300e-04 - acc: 1.0000 - val_loss: 1.5779e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.1147e-04 - acc: 1.0000 - val_loss: 1.4007e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 9.4861e-05 - acc: 1.0000 - val_loss: 1.2587e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 9.7068e-05 - acc: 1.0000 - val_loss: 1.1433e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.5511e-05 - acc: 1.0000 - val_loss: 1.0476e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.1276e-05 - acc: 1.0000 - val_loss: 9.6792e-05 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 6.2169e-05 - acc: 1.0000 - val_loss: 9.0039e-05 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 6.8980e-05 - acc: 1.0000 - val_loss: 8.4249e-05 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.0754e-05 - acc: 1.0000 - val_loss: 7.9286e-05 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.1706e-05 - acc: 1.0000 - val_loss: 7.4951e-05 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.8608e-05 - acc: 1.0000 - val_loss: 7.1183e-05 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 5.8217e-05 - acc: 1.0000 - val_loss: 6.7806e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.5257e-05 - acc: 1.0000 - val_loss: 6.4842e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5.1372e-05 - acc: 1.0000 - val_loss: 6.2154e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.4228e-05 - acc: 1.0000 - val_loss: 5.9780e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.4273e-05 - acc: 1.0000 - val_loss: 5.7593e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.9473e-05 - acc: 1.0000 - val_loss: 5.5643e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.7687e-05 - acc: 1.0000 - val_loss: 5.3836e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.5939e-05 - acc: 1.0000 - val_loss: 5.2211e-05 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.7405e-05 - acc: 1.0000 - val_loss: 5.0713e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6928 - acc: 0.5161 - val_loss: 0.6712 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 907ms/step - loss: 0.6694 - acc: 0.5806 - val_loss: 0.6439 - val_acc: 0.6667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6377 - acc: 0.7742 - val_loss: 0.6046 - val_acc: 1.0000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6088 - acc: 0.8710 - val_loss: 0.5432 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 0.5446 - acc: 1.0000 - val_loss: 0.4458 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.4350 - acc: 1.0000 - val_loss: 0.3384 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 602ms/step - loss: 0.3555 - acc: 0.8387 - val_loss: 0.2220 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 683ms/step - loss: 0.2118 - acc: 1.0000 - val_loss: 0.2166 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 691ms/step - loss: 0.1670 - acc: 1.0000 - val_loss: 0.0815 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 0.1379 - acc: 0.9355 - val_loss: 0.1493 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.0957 - acc: 1.0000 - val_loss: 0.1279 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 0.0736 - acc: 1.0000 - val_loss: 0.0361 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.0116 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 7.6924e-04 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 6.7655e-04 - acc: 1.0000 - val_loss: 5.2869e-04 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 4.1187e-04 - acc: 1.0000 - val_loss: 3.9409e-04 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 3.1203e-04 - acc: 1.0000 - val_loss: 3.2066e-04 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.0871e-04 - acc: 1.0000 - val_loss: 2.8613e-04 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 1.4888e-04 - acc: 1.0000 - val_loss: 2.7789e-04 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 1.1258e-04 - acc: 1.0000 - val_loss: 2.8968e-04 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.0568e-04 - acc: 1.0000 - val_loss: 3.1750e-04 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.3809e-04 - acc: 1.0000 - val_loss: 3.5677e-04 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 9.6386e-05 - acc: 1.0000 - val_loss: 4.0749e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 9.4048e-05 - acc: 1.0000 - val_loss: 4.6709e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.8572e-05 - acc: 1.0000 - val_loss: 5.3218e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.0404e-04 - acc: 1.0000 - val_loss: 5.9850e-04 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.1758e-04 - acc: 1.0000 - val_loss: 6.5888e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 8.8686e-05 - acc: 1.0000 - val_loss: 7.1298e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.3026e-05 - acc: 1.0000 - val_loss: 7.5819e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.2391e-05 - acc: 1.0000 - val_loss: 7.9374e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.3690e-05 - acc: 1.0000 - val_loss: 8.1373e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 6.4163e-05 - acc: 1.0000 - val_loss: 8.2040e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.0980e-04 - acc: 1.0000 - val_loss: 8.0450e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.9892e-05 - acc: 1.0000 - val_loss: 7.7884e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 6.3369e-05 - acc: 1.0000 - val_loss: 7.4080e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4.6225e-05 - acc: 1.0000 - val_loss: 6.9658e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 5.4208e-05 - acc: 1.0000 - val_loss: 6.4614e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.8690e-05 - acc: 1.0000 - val_loss: 5.9169e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4.9178e-05 - acc: 1.0000 - val_loss: 5.3545e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 5.0492e-05 - acc: 1.0000 - val_loss: 4.7844e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 4.0566e-05 - acc: 1.0000 - val_loss: 4.2354e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6649e-05 - acc: 1.0000 - val_loss: 3.7377e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6231e-05 - acc: 1.0000 - val_loss: 3.2903e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9043e-05 - acc: 1.0000 - val_loss: 2.8838e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6937e-05 - acc: 1.0000 - val_loss: 2.5191e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.4082e-05 - acc: 1.0000 - val_loss: 2.2089e-04 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6052e-05 - acc: 1.0000 - val_loss: 1.9313e-04 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.6129e-05 - acc: 1.0000 - val_loss: 1.6924e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.0116e-05 - acc: 1.0000 - val_loss: 1.4826e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5708e-05 - acc: 1.0000 - val_loss: 1.2946e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6919 - acc: 0.5484 - val_loss: 0.6680 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6618 - acc: 0.7097 - val_loss: 0.6416 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6360 - acc: 0.7419 - val_loss: 0.6021 - val_acc: 0.8667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6000 - acc: 0.8065 - val_loss: 0.5409 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.5429 - acc: 0.9355 - val_loss: 0.4476 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 0.4253 - acc: 1.0000 - val_loss: 0.3316 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 856ms/step - loss: 0.3228 - acc: 0.9355 - val_loss: 0.2016 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 0.1936 - acc: 1.0000 - val_loss: 0.1981 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.1417 - acc: 1.0000 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0441 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.1210 - acc: 0.9355 - val_loss: 0.4638 - val_acc: 0.6000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.2404 - acc: 0.8710 - val_loss: 0.2759 - val_acc: 0.8000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.1135 - acc: 1.0000 - val_loss: 0.1182 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0575 - acc: 1.0000 - val_loss: 0.0735 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.0341 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0301 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 0.0326 - acc: 1.0000 - val_loss: 0.0641 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0360 - acc: 1.0000 - val_loss: 0.0653 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0366 - acc: 1.0000 - val_loss: 0.0645 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0349 - acc: 1.0000 - val_loss: 0.0619 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0350 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.0307 - acc: 1.0000 - val_loss: 0.0510 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0294 - acc: 1.0000 - val_loss: 0.0444 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0249 - acc: 1.0000 - val_loss: 0.0379 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.0318 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.0263 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0216 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0143 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.0116 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0093 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.4591e-04 - acc: 1.0000 - val_loss: 9.2089e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 6.8616e-04 - acc: 1.0000 - val_loss: 7.3774e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5.3588e-04 - acc: 1.0000 - val_loss: 5.9507e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 3.9865e-04 - acc: 1.0000 - val_loss: 4.8386e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.7589e-04 - acc: 1.0000 - val_loss: 3.9657e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.1363e-04 - acc: 1.0000 - val_loss: 3.2791e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.4344e-04 - acc: 1.0000 - val_loss: 2.7387e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.8245e-04 - acc: 1.0000 - val_loss: 2.3126e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.7599e-04 - acc: 1.0000 - val_loss: 1.9741e-04 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.3964e-04 - acc: 1.0000 - val_loss: 1.7056e-04 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.0277e-04 - acc: 1.0000 - val_loss: 1.4916e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.1073e-04 - acc: 1.0000 - val_loss: 1.3195e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.7388e-05 - acc: 1.0000 - val_loss: 1.1805e-04 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.5770e-05 - acc: 1.0000 - val_loss: 1.0678e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6936 - acc: 0.5484 - val_loss: 0.6830 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 887ms/step - loss: 0.6807 - acc: 0.6129 - val_loss: 0.6705 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 818ms/step - loss: 0.6645 - acc: 0.5484 - val_loss: 0.6517 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 787ms/step - loss: 0.6447 - acc: 0.5806 - val_loss: 0.6185 - val_acc: 0.6667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6153 - acc: 0.7742 - val_loss: 0.5574 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 841ms/step - loss: 0.5493 - acc: 0.9677 - val_loss: 0.4684 - val_acc: 0.8667\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 577ms/step - loss: 0.4671 - acc: 0.8387 - val_loss: 0.3872 - val_acc: 0.8000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.3931 - acc: 0.7742 - val_loss: 0.4060 - val_acc: 0.6000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 933ms/step - loss: 0.3656 - acc: 0.9355 - val_loss: 0.3201 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 0.2826 - acc: 1.0000 - val_loss: 0.1741 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.1768 - acc: 0.9677 - val_loss: 0.1228 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.1373 - acc: 0.9355 - val_loss: 0.0508 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 0.0353 - acc: 1.0000 - val_loss: 0.0324 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.0145 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 7.7528e-04 - acc: 1.0000 - val_loss: 8.1103e-04 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.3514e-04 - acc: 1.0000 - val_loss: 5.5432e-04 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 3.8061e-04 - acc: 1.0000 - val_loss: 4.2556e-04 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 2.9043e-04 - acc: 1.0000 - val_loss: 3.4962e-04 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0323e-04 - acc: 1.0000 - val_loss: 2.9437e-04 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 2.7978e-04 - acc: 1.0000 - val_loss: 2.4791e-04 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5562e-04 - acc: 1.0000 - val_loss: 2.0738e-04 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.8587e-04 - acc: 1.0000 - val_loss: 1.7270e-04 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.6581e-04 - acc: 1.0000 - val_loss: 1.4310e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.0594e-04 - acc: 1.0000 - val_loss: 1.1892e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 9.4923e-05 - acc: 1.0000 - val_loss: 9.9062e-05 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 9.5832e-05 - acc: 1.0000 - val_loss: 8.2541e-05 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 7.2575e-05 - acc: 1.0000 - val_loss: 6.9161e-05 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.8075e-05 - acc: 1.0000 - val_loss: 5.8320e-05 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 5.1619e-05 - acc: 1.0000 - val_loss: 4.9550e-05 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 4.2208e-05 - acc: 1.0000 - val_loss: 0.1917 - val_acc: 0.9333\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.3660e-05 - acc: 1.0000 - val_loss: 0.2325 - val_acc: 0.9333\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 4.4047e-05 - acc: 1.0000 - val_loss: 0.2411 - val_acc: 0.9333\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7420e-05 - acc: 1.0000 - val_loss: 0.2489 - val_acc: 0.9333\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.3664e-05 - acc: 1.0000 - val_loss: 0.2562 - val_acc: 0.9333\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.9658e-05 - acc: 1.0000 - val_loss: 0.2631 - val_acc: 0.9333\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5431e-05 - acc: 1.0000 - val_loss: 0.2693 - val_acc: 0.9333\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.6462e-05 - acc: 1.0000 - val_loss: 0.2752 - val_acc: 0.9333\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.5678e-05 - acc: 1.0000 - val_loss: 0.2806 - val_acc: 0.9333\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.1921e-05 - acc: 1.0000 - val_loss: 0.2857 - val_acc: 0.9333\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.4876e-05 - acc: 1.0000 - val_loss: 0.2903 - val_acc: 0.9333\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0657e-05 - acc: 1.0000 - val_loss: 0.2946 - val_acc: 0.9333\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.6003e-05 - acc: 1.0000 - val_loss: 0.2983 - val_acc: 0.9333\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.1810e-05 - acc: 1.0000 - val_loss: 0.3015 - val_acc: 0.9333\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.6995e-05 - acc: 1.0000 - val_loss: 0.3043 - val_acc: 0.9333\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.7290e-06 - acc: 1.0000 - val_loss: 0.3070 - val_acc: 0.9333\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.5110e-05 - acc: 1.0000 - val_loss: 0.3091 - val_acc: 0.9333\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6497e-05 - acc: 1.0000 - val_loss: 0.3107 - val_acc: 0.9333\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.0902e-05 - acc: 1.0000 - val_loss: 0.3122 - val_acc: 0.9333\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1283e-05 - acc: 1.0000 - val_loss: 0.3135 - val_acc: 0.9333\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.7272e-06 - acc: 1.0000 - val_loss: 0.3147 - val_acc: 0.9333\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.4224e-06 - acc: 1.0000 - val_loss: 0.3159 - val_acc: 0.9333\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.9431e-06 - acc: 1.0000 - val_loss: 0.3169 - val_acc: 0.9333\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6945 - acc: 0.4839 - val_loss: 0.6776 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6714 - acc: 0.7097 - val_loss: 0.6582 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 925ms/step - loss: 0.6538 - acc: 0.6452 - val_loss: 0.6280 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 0.6220 - acc: 0.7097 - val_loss: 0.5738 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 771ms/step - loss: 0.5586 - acc: 1.0000 - val_loss: 0.4766 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 0.4603 - acc: 1.0000 - val_loss: 0.3859 - val_acc: 0.8000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 0.3827 - acc: 0.7742 - val_loss: 0.2899 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 677ms/step - loss: 0.2561 - acc: 1.0000 - val_loss: 0.2527 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 577ms/step - loss: 0.2003 - acc: 1.0000 - val_loss: 0.3429 - val_acc: 0.8000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.3686 - acc: 0.7742 - val_loss: 0.1098 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 681ms/step - loss: 0.0930 - acc: 1.0000 - val_loss: 0.1847 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.1297 - acc: 1.0000 - val_loss: 0.1705 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1090 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0476 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 497ms/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.4851e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.4899e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 5.8080e-04 - acc: 1.0000 - val_loss: 8.7886e-04 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 5.5153e-04 - acc: 1.0000 - val_loss: 7.1033e-04 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.8233e-04 - acc: 1.0000 - val_loss: 5.6510e-04 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 3.9472e-04 - acc: 1.0000 - val_loss: 4.4495e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.1466e-04 - acc: 1.0000 - val_loss: 3.4969e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.3602e-04 - acc: 1.0000 - val_loss: 2.7603e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.3386e-04 - acc: 1.0000 - val_loss: 2.1937e-04 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7026e-04 - acc: 1.0000 - val_loss: 1.7654e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1297e-04 - acc: 1.0000 - val_loss: 1.4435e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.6487e-05 - acc: 1.0000 - val_loss: 1.1988e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.1804e-05 - acc: 1.0000 - val_loss: 1.0117e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.0852e-05 - acc: 1.0000 - val_loss: 8.6579e-05 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 7.8866e-05 - acc: 1.0000 - val_loss: 7.5143e-05 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.5340e-05 - acc: 1.0000 - val_loss: 6.6027e-05 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.9666e-05 - acc: 1.0000 - val_loss: 5.8733e-05 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.4865e-05 - acc: 1.0000 - val_loss: 5.2806e-05 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 4.6918e-05 - acc: 1.0000 - val_loss: 4.7925e-05 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.2413e-05 - acc: 1.0000 - val_loss: 4.3909e-05 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9864e-05 - acc: 1.0000 - val_loss: 4.0544e-05 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 3.1634e-05 - acc: 1.0000 - val_loss: 3.7701e-05 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.1458e-05 - acc: 1.0000 - val_loss: 3.5305e-05 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5663e-05 - acc: 1.0000 - val_loss: 3.3245e-05 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.2639e-05 - acc: 1.0000 - val_loss: 3.1476e-05 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.3184e-05 - acc: 1.0000 - val_loss: 2.9923e-05 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5389e-05 - acc: 1.0000 - val_loss: 2.8579e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.0980e-05 - acc: 1.0000 - val_loss: 2.7378e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5362e-05 - acc: 1.0000 - val_loss: 2.6342e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7606e-05 - acc: 1.0000 - val_loss: 2.5400e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.0292e-05 - acc: 1.0000 - val_loss: 2.4585e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.5921e-05 - acc: 1.0000 - val_loss: 2.3830e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.0271e-05 - acc: 1.0000 - val_loss: 2.3153e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.7979e-05 - acc: 1.0000 - val_loss: 2.2580e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6903 - acc: 0.6452 - val_loss: 0.6677 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 0.6612 - acc: 0.5806 - val_loss: 0.6383 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 795ms/step - loss: 0.6314 - acc: 0.6774 - val_loss: 0.5856 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 916ms/step - loss: 0.5753 - acc: 0.8710 - val_loss: 0.4951 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4930 - acc: 1.0000 - val_loss: 0.4250 - val_acc: 0.7333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 869ms/step - loss: 0.4144 - acc: 0.8065 - val_loss: 0.3208 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.3131 - acc: 1.0000 - val_loss: 0.2954 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 769ms/step - loss: 0.2666 - acc: 1.0000 - val_loss: 0.1524 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 671ms/step - loss: 0.1354 - acc: 1.0000 - val_loss: 0.1886 - val_acc: 0.8667\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 493ms/step - loss: 0.2338 - acc: 0.8710 - val_loss: 0.1256 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0785 - acc: 1.0000 - val_loss: 0.2525 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 0.1572 - acc: 0.9677 - val_loss: 0.1966 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.1129 - acc: 1.0000 - val_loss: 0.1302 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0728 - acc: 1.0000 - val_loss: 0.0901 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 0.0499 - acc: 1.0000 - val_loss: 0.0659 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0367 - acc: 1.0000 - val_loss: 0.0490 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 635ms/step - loss: 0.0297 - acc: 1.0000 - val_loss: 0.0351 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 0.0220 - acc: 1.0000 - val_loss: 0.0223 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0112 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 9.7407e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.5406e-04 - acc: 1.0000 - val_loss: 8.5735e-04 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 5.5286e-04 - acc: 1.0000 - val_loss: 6.2475e-04 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.1979e-04 - acc: 1.0000 - val_loss: 4.6535e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5521e-04 - acc: 1.0000 - val_loss: 3.5504e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.8677e-04 - acc: 1.0000 - val_loss: 2.7734e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.3062e-04 - acc: 1.0000 - val_loss: 2.2174e-04 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1261e-04 - acc: 1.0000 - val_loss: 1.8111e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.9324e-05 - acc: 1.0000 - val_loss: 1.5094e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 8.1270e-05 - acc: 1.0000 - val_loss: 1.2824e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.0072e-05 - acc: 1.0000 - val_loss: 1.1062e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 4.4545e-05 - acc: 1.0000 - val_loss: 9.6831e-05 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.6336e-05 - acc: 1.0000 - val_loss: 8.5869e-05 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9373e-05 - acc: 1.0000 - val_loss: 7.7077e-05 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8046e-05 - acc: 1.0000 - val_loss: 6.9894e-05 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.0260e-05 - acc: 1.0000 - val_loss: 6.3955e-05 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5213e-05 - acc: 1.0000 - val_loss: 5.8953e-05 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.7688e-05 - acc: 1.0000 - val_loss: 5.4706e-05 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.6646e-05 - acc: 1.0000 - val_loss: 5.1153e-05 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.4508e-05 - acc: 1.0000 - val_loss: 4.8041e-05 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.5188e-05 - acc: 1.0000 - val_loss: 4.5364e-05 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.0473e-05 - acc: 1.0000 - val_loss: 4.3067e-05 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.3498e-05 - acc: 1.0000 - val_loss: 4.1017e-05 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.7214e-06 - acc: 1.0000 - val_loss: 3.9210e-05 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0014e-05 - acc: 1.0000 - val_loss: 3.7641e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 6.8556e-06 - acc: 1.0000 - val_loss: 3.6236e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 8.1137e-06 - acc: 1.0000 - val_loss: 3.5013e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3354e-05 - acc: 1.0000 - val_loss: 3.3834e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.4389e-06 - acc: 1.0000 - val_loss: 3.2809e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5.1577e-06 - acc: 1.0000 - val_loss: 3.1917e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.0897e-06 - acc: 1.0000 - val_loss: 3.1052e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 6.5091e-06 - acc: 1.0000 - val_loss: 3.0281e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6870 - acc: 0.5484 - val_loss: 0.6636 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6531 - acc: 0.5806 - val_loss: 0.6282 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6270 - acc: 0.5806 - val_loss: 0.5656 - val_acc: 0.9333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 825ms/step - loss: 0.5538 - acc: 0.9032 - val_loss: 0.4574 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 503ms/step - loss: 0.4522 - acc: 1.0000 - val_loss: 0.3384 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 701ms/step - loss: 0.3363 - acc: 0.8710 - val_loss: 0.2069 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 710ms/step - loss: 0.2004 - acc: 1.0000 - val_loss: 0.1990 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.1472 - acc: 1.0000 - val_loss: 0.0663 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 604ms/step - loss: 0.0993 - acc: 0.9355 - val_loss: 0.1056 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 599ms/step - loss: 0.0632 - acc: 1.0000 - val_loss: 0.0682 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0364 - acc: 1.0000 - val_loss: 0.0173 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 817ms/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 8.3643e-04 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 6.6338e-04 - acc: 1.0000 - val_loss: 6.1818e-04 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 4.4775e-04 - acc: 1.0000 - val_loss: 5.7994e-04 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6804e-04 - acc: 1.0000 - val_loss: 6.8439e-04 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1142e-04 - acc: 1.0000 - val_loss: 9.4156e-04 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 3.1960e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 2.1512e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.5969e-04 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 5.3765e-04 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5124e-04 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5766e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.2336e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.4269e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.8719e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 9.1031e-05 - acc: 1.0000 - val_loss: 7.3784e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.8578e-05 - acc: 1.0000 - val_loss: 4.8848e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.3161e-05 - acc: 1.0000 - val_loss: 3.2273e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.2234e-05 - acc: 1.0000 - val_loss: 2.1140e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.4174e-05 - acc: 1.0000 - val_loss: 1.3912e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.0035e-05 - acc: 1.0000 - val_loss: 9.1846e-05 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.7048e-05 - acc: 1.0000 - val_loss: 6.2429e-05 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.5676e-05 - acc: 1.0000 - val_loss: 4.3871e-05 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 1.0302e-05 - acc: 1.0000 - val_loss: 3.1956e-05 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.1565e-05 - acc: 1.0000 - val_loss: 2.4089e-05 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2069e-05 - acc: 1.0000 - val_loss: 1.8702e-05 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1531e-05 - acc: 1.0000 - val_loss: 1.4945e-05 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.0972e-06 - acc: 1.0000 - val_loss: 1.2240e-05 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.1781e-06 - acc: 1.0000 - val_loss: 1.0318e-05 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.1914e-06 - acc: 1.0000 - val_loss: 8.8634e-06 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 6.5944e-06 - acc: 1.0000 - val_loss: 7.7451e-06 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 5.9280e-06 - acc: 1.0000 - val_loss: 6.9133e-06 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.7121e-06 - acc: 1.0000 - val_loss: 6.2413e-06 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 6.3598e-06 - acc: 1.0000 - val_loss: 5.7180e-06 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.5100e-06 - acc: 1.0000 - val_loss: 5.2828e-06 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.5793e-06 - acc: 1.0000 - val_loss: 4.9357e-06 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.9018e-06 - acc: 1.0000 - val_loss: 4.6273e-06 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 4.2061e-06 - acc: 1.0000 - val_loss: 4.3739e-06 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.7152e-06 - acc: 1.0000 - val_loss: 4.1755e-06 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[Bituler:] Creating a model to test set\n",
      "[Bituler:] Evaluation Config - bilstm-100-1-0.5-100-64-1000-20-val_acc-0.001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a73cbe3f184f6da406bac01559aff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Testing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6953 - acc: 0.4516 - val_loss: 0.6884 - val_acc: 0.7333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6889 - acc: 0.5806 - val_loss: 0.6809 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 901ms/step - loss: 0.6811 - acc: 0.6774 - val_loss: 0.6731 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 740ms/step - loss: 0.6702 - acc: 0.6452 - val_loss: 0.6643 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 941ms/step - loss: 0.6622 - acc: 0.5484 - val_loss: 0.6539 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 784ms/step - loss: 0.6412 - acc: 0.6452 - val_loss: 0.6410 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 747ms/step - loss: 0.6425 - acc: 0.6129 - val_loss: 0.6246 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 538ms/step - loss: 0.6121 - acc: 0.6452 - val_loss: 0.6034 - val_acc: 0.6667\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5930 - acc: 0.7419 - val_loss: 0.5756 - val_acc: 0.8667\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 502ms/step - loss: 0.5698 - acc: 0.7742 - val_loss: 0.5383 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 0.5248 - acc: 0.9677 - val_loss: 0.4893 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.4744 - acc: 1.0000 - val_loss: 0.4277 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.4120 - acc: 1.0000 - val_loss: 0.3589 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.3421 - acc: 1.0000 - val_loss: 0.2971 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.2796 - acc: 0.9677 - val_loss: 0.2324 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2218 - acc: 0.9677 - val_loss: 0.1546 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1393 - acc: 1.0000 - val_loss: 0.2100 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1527 - acc: 1.0000 - val_loss: 0.0939 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0778 - acc: 1.0000 - val_loss: 0.0738 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0928 - acc: 0.9677 - val_loss: 0.0378 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0499 - acc: 1.0000 - val_loss: 0.0537 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0346 - acc: 1.0000 - val_loss: 0.1084 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0574 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0109 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0215 - acc: 1.0000 - val_loss: 0.0086 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 9.8481e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.7852e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.3213e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.2157e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 9.1716e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5.4978e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5.3599e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 6.6050e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 4.2336e-04 - acc: 1.0000 - val_loss: 8.8556e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.1227e-04 - acc: 1.0000 - val_loss: 7.8093e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7773e-04 - acc: 1.0000 - val_loss: 6.9061e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.3524e-04 - acc: 1.0000 - val_loss: 6.1161e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.3858e-04 - acc: 1.0000 - val_loss: 5.4177e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.3663e-04 - acc: 1.0000 - val_loss: 4.8237e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1851e-04 - acc: 1.0000 - val_loss: 4.2955e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7841e-04 - acc: 1.0000 - val_loss: 3.8341e-04 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.9249e-04 - acc: 1.0000 - val_loss: 3.4379e-04 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.0746e-04 - acc: 1.0000 - val_loss: 3.0950e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.8270e-04 - acc: 1.0000 - val_loss: 2.7988e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.2816e-04 - acc: 1.0000 - val_loss: 2.5363e-04 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.5594e-04 - acc: 1.0000 - val_loss: 2.3086e-04 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.2815e-04 - acc: 1.0000 - val_loss: 2.1148e-04 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.0335e-04 - acc: 1.0000 - val_loss: 1.9484e-04 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.4222e-05 - acc: 1.0000 - val_loss: 1.8035e-04 - val_acc: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.3523e-05 - acc: 1.0000 - val_loss: 1.6791e-04 - val_acc: 1.0000\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.6602e-05 - acc: 1.0000 - val_loss: 1.5713e-04 - val_acc: 1.0000\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 9.4952e-05 - acc: 1.0000 - val_loss: 1.4760e-04 - val_acc: 1.0000\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.1408e-04 - acc: 1.0000 - val_loss: 1.3890e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 1s 989ms/step\n",
      "[Bituler:] Processing time: 335680.667 milliseconds. Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       " 0       1.0            1.0                1.0              1.0           1.0   \n",
       " \n",
       "    f1_macro     clstime  \n",
       " 0       1.0  335678.996  ,\n",
       " array([[0.5065809 , 0.49341917],\n",
       "        [0.5523806 , 0.44761944],\n",
       "        [0.5583106 , 0.44168934],\n",
       "        [0.57983446, 0.42016557],\n",
       "        [0.56162745, 0.4383726 ],\n",
       "        [0.56305957, 0.4369405 ],\n",
       "        [0.58497775, 0.4150222 ],\n",
       "        [0.3245508 , 0.67544925],\n",
       "        [0.35772285, 0.64227724],\n",
       "        [0.3435713 , 0.65642864],\n",
       "        [0.2637748 , 0.7362252 ],\n",
       "        [0.316659  , 0.683341  ],\n",
       "        [0.34295687, 0.65704316],\n",
       "        [0.3501528 , 0.64984727],\n",
       "        [0.36667636, 0.6333237 ]], dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matclassification.methods import Bituler\n",
    "\n",
    "model = Bituler()\n",
    "model.prepare_input(train, test, features=['poi'])\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>clstime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>335678.996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0       1.0            1.0                1.0              1.0           1.0   \n",
       "\n",
       "   f1_macro     clstime  \n",
       "0       1.0  335678.996  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.6. TULVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-30 21:19:36.931500\n",
      "\n",
      "[Tulvae:] Building model\n",
      "\n",
      "###########    DATA PREPARATION      ###########\n",
      "Attribute 'poi': 137 unique values\n",
      "Total of attribute/value pairs: 137\n",
      "\n",
      "\n",
      "###########      DATA ENCODING        ###########\n",
      "\n",
      "Input total: 2\n",
      "... tid_0: 31\n",
      "... tid_1: 15\n",
      "col_name: ['poi', 'tid', 'label']...\n",
      "... num_classes: 2\n",
      "... max_lenght: 38\n",
      "Removing column tid of attr\n",
      "Removing column label of attr\n",
      "\n",
      "\n",
      "#####   Encoding string data to integer   ######\n",
      "   Encoding: poi\n",
      "\n",
      "\n",
      "###########      Generating y_train and y_test     ###########\n",
      "Label encoding on label y\n",
      "Input total: 2\n",
      "[Tulvae:] Training hiperparameter model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87041a7faa8b4a878ab5c7c9564b3728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Tulvae:] Model Training:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.6999 - acc: 0.2903 - val_loss: 0.6916 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6885 - acc: 0.7097 - val_loss: 0.6858 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6827 - acc: 0.6774 - val_loss: 0.6802 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6804 - acc: 0.6452 - val_loss: 0.6728 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6704 - acc: 0.6452 - val_loss: 0.6622 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6463 - acc: 0.7097 - val_loss: 0.6468 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 843ms/step - loss: 0.6411 - acc: 0.6452 - val_loss: 0.6239 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 773ms/step - loss: 0.6052 - acc: 0.7419 - val_loss: 0.5902 - val_acc: 0.6667\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5666 - acc: 0.8065 - val_loss: 0.5405 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 0.5185 - acc: 0.9355 - val_loss: 0.4685 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.4314 - acc: 1.0000 - val_loss: 0.3707 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.3355 - acc: 1.0000 - val_loss: 0.2568 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 870ms/step - loss: 0.2205 - acc: 1.0000 - val_loss: 0.1546 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.1318 - acc: 1.0000 - val_loss: 0.0752 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0625 - acc: 1.0000 - val_loss: 0.0394 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 696ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0427 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 462ms/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 5.5234e-04 - acc: 1.0000 - val_loss: 1.6029 - val_acc: 0.6000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 1s 694ms/step - loss: 0.5189 - acc: 0.8065 - val_loss: 2.0435e-04 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 1.9338e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 1s 950ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.2136 - val_acc: 0.8667\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2593 - acc: 0.9032 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0212 - acc: 1.0000 - val_loss: 5.4061e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 4.2540e-04 - acc: 1.0000 - val_loss: 2.7230e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7725e-04 - acc: 1.0000 - val_loss: 2.2800e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 1.5070e-04 - acc: 1.0000 - val_loss: 2.3421e-04 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5284e-04 - acc: 1.0000 - val_loss: 2.6624e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.2577e-04 - acc: 1.0000 - val_loss: 3.1887e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6373e-04 - acc: 1.0000 - val_loss: 3.9060e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.9237e-04 - acc: 1.0000 - val_loss: 4.8013e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.3955e-04 - acc: 1.0000 - val_loss: 5.8551e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0463e-04 - acc: 1.0000 - val_loss: 7.0400e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9597e-04 - acc: 1.0000 - val_loss: 8.3282e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.6720e-04 - acc: 1.0000 - val_loss: 9.6837e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.1329e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.4932e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 4.0296e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.7266e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 4.5463e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.9883e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0530e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.4374e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.6929e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.4504e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.6684e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.8040e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.3600e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.8032e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.2259e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.0871e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.5638e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.8597e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.2406e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.1358e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.0058e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.6791e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 4.8682e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.6948 - acc: 0.5806 - val_loss: 0.6895 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6833 - acc: 0.6452 - val_loss: 0.6834 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6786 - acc: 0.5806 - val_loss: 0.6770 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6722 - acc: 0.5484 - val_loss: 0.6679 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 0.6608 - acc: 0.5806 - val_loss: 0.6547 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6319 - acc: 0.6129 - val_loss: 0.6358 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 721ms/step - loss: 0.6343 - acc: 0.6129 - val_loss: 0.6072 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 896ms/step - loss: 0.5904 - acc: 0.8065 - val_loss: 0.5644 - val_acc: 0.6667\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 761ms/step - loss: 0.5500 - acc: 0.8065 - val_loss: 0.5024 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4973 - acc: 0.9355 - val_loss: 0.4168 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 767ms/step - loss: 0.3965 - acc: 1.0000 - val_loss: 0.3092 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 545ms/step - loss: 0.2841 - acc: 1.0000 - val_loss: 0.1926 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 0.1775 - acc: 1.0000 - val_loss: 0.0952 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 0.0891 - acc: 1.0000 - val_loss: 0.0367 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0406 - acc: 1.0000 - val_loss: 0.0133 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0120 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0214 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 1s 574ms/step - loss: 5.5480e-04 - acc: 1.0000 - val_loss: 4.8017e-04 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.8973e-04 - acc: 1.0000 - val_loss: 1.0823e-04 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.4781e-04 - acc: 1.0000 - val_loss: 5.0503e-05 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 8.4210e-05 - acc: 1.0000 - val_loss: 3.2606e-05 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 6.9238e-05 - acc: 1.0000 - val_loss: 2.4635e-05 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.4619e-04 - acc: 1.0000 - val_loss: 4.1906 - val_acc: 0.6000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 2.1683 - acc: 0.6774 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 9.7266e-05 - acc: 1.0000 - val_loss: 2.8711e-05 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 7.3194e-05 - acc: 1.0000 - val_loss: 4.2753e-05 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.6608e-05 - acc: 1.0000 - val_loss: 7.5171e-05 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.9274e-04 - acc: 1.0000 - val_loss: 1.2652e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.1871e-04 - acc: 1.0000 - val_loss: 1.9942e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.7967e-04 - acc: 1.0000 - val_loss: 2.9547e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.5693e-04 - acc: 1.0000 - val_loss: 4.1438e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.6703e-04 - acc: 1.0000 - val_loss: 5.5301e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 7.0574e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 8.6555e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 9.6393e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 9.4360e-04 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 8.8295e-04 - val_acc: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 8.2658e-04 - val_acc: 1.0000\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 7.7357e-04 - val_acc: 1.0000\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 7.2442e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.6931 - acc: 0.5161 - val_loss: 0.6887 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6919 - acc: 0.5484 - val_loss: 0.6796 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6763 - acc: 0.7097 - val_loss: 0.6673 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6619 - acc: 0.6452 - val_loss: 0.6492 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6411 - acc: 0.6774 - val_loss: 0.6216 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 0.5947 - acc: 0.7742 - val_loss: 0.5800 - val_acc: 0.8000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5780 - acc: 0.7742 - val_loss: 0.5152 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 0.5007 - acc: 0.8710 - val_loss: 0.4201 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 610ms/step - loss: 0.4052 - acc: 1.0000 - val_loss: 0.2972 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 985ms/step - loss: 0.2961 - acc: 1.0000 - val_loss: 0.1813 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.1869 - acc: 0.9677 - val_loss: 0.0947 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1037 - acc: 1.0000 - val_loss: 0.0361 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 702ms/step - loss: 0.0353 - acc: 1.0000 - val_loss: 0.0205 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.0216 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 602ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0255 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 694ms/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 874ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 4.2255e-04 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.1630e-04 - acc: 1.0000 - val_loss: 9.8847e-05 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 8.1934e-05 - acc: 1.0000 - val_loss: 4.0257e-05 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.5396e-05 - acc: 1.0000 - val_loss: 2.4668e-05 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 4.9242e-05 - acc: 1.0000 - val_loss: 1.8961e-05 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.1700e-05 - acc: 1.0000 - val_loss: 1.6873e-05 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.4065e-05 - acc: 1.0000 - val_loss: 1.7771e-05 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8598e-04 - acc: 1.0000 - val_loss: 1.8151e-05 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 8.9956e-06 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.3539e-05 - acc: 1.0000 - val_loss: 9.4088e-06 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3274e-05 - acc: 1.0000 - val_loss: 6.7910e-05 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0112e-05 - acc: 1.0000 - val_loss: 0.1582 - val_acc: 0.8667\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 1.0599e-05 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.8205e-05 - acc: 1.0000 - val_loss: 6.9905e-06 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.0047e-05 - acc: 1.0000 - val_loss: 6.8472e-06 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.0537e-05 - acc: 1.0000 - val_loss: 6.9850e-06 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 1.6259e-05 - acc: 1.0000 - val_loss: 7.1337e-06 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9770e-05 - acc: 1.0000 - val_loss: 7.2659e-06 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.2558e-05 - acc: 1.0000 - val_loss: 7.3485e-06 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9487e-05 - acc: 1.0000 - val_loss: 7.4036e-06 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.9413e-05 - acc: 1.0000 - val_loss: 7.4091e-06 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2789e-05 - acc: 1.0000 - val_loss: 7.3981e-06 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 9.9342e-06 - acc: 1.0000 - val_loss: 7.3926e-06 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.7008e-05 - acc: 1.0000 - val_loss: 7.3706e-06 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 1.0806e-05 - acc: 1.0000 - val_loss: 7.2934e-06 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3287e-05 - acc: 1.0000 - val_loss: 7.2659e-06 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2336e-05 - acc: 1.0000 - val_loss: 7.2108e-06 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 9.8196e-06 - acc: 1.0000 - val_loss: 7.1667e-06 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4844e-05 - acc: 1.0000 - val_loss: 7.0896e-06 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9069e-05 - acc: 1.0000 - val_loss: 7.0400e-06 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.7176e-05 - acc: 1.0000 - val_loss: 6.9574e-06 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.1440e-05 - acc: 1.0000 - val_loss: 6.8638e-06 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.1027e-05 - acc: 1.0000 - val_loss: 6.7701e-06 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.7395e-05 - acc: 1.0000 - val_loss: 6.7095e-06 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3543e-05 - acc: 1.0000 - val_loss: 6.6214e-06 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1280e-05 - acc: 1.0000 - val_loss: 6.5443e-06 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 1.5492e-05 - acc: 1.0000 - val_loss: 6.4561e-06 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7358e-05 - acc: 1.0000 - val_loss: 6.3570e-06 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3455e-05 - acc: 1.0000 - val_loss: 6.2688e-06 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3183e-05 - acc: 1.0000 - val_loss: 6.1642e-06 - val_acc: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.6478e-05 - acc: 1.0000 - val_loss: 6.0650e-06 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.6917 - acc: 0.4839 - val_loss: 0.6932 - val_acc: 0.4667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6964 - acc: 0.5484 - val_loss: 0.6901 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6847 - acc: 0.5806 - val_loss: 0.6852 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6847 - acc: 0.5484 - val_loss: 0.6790 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6723 - acc: 0.5484 - val_loss: 0.6705 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6477 - acc: 0.6129 - val_loss: 0.6584 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6566 - acc: 0.5484 - val_loss: 0.6397 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 803ms/step - loss: 0.6295 - acc: 0.6452 - val_loss: 0.6109 - val_acc: 0.5333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 861ms/step - loss: 0.5890 - acc: 0.7742 - val_loss: 0.5675 - val_acc: 0.8667\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5560 - acc: 0.8710 - val_loss: 0.5038 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4708 - acc: 1.0000 - val_loss: 0.4158 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 0.3895 - acc: 1.0000 - val_loss: 0.3053 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.2659 - acc: 1.0000 - val_loss: 0.1883 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1625 - acc: 1.0000 - val_loss: 0.0954 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.0910 - acc: 1.0000 - val_loss: 0.0399 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 593ms/step - loss: 0.0329 - acc: 1.0000 - val_loss: 0.0145 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0141 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 891ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 9.0833e-04 - acc: 1.0000 - val_loss: 3.0381e-04 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.6673e-04 - acc: 1.0000 - val_loss: 1.4750e-04 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.0559e-04 - acc: 1.0000 - val_loss: 5.6182e-04 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2169 - acc: 0.9355 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.2634e-04 - acc: 1.0000 - val_loss: 3.1387 - val_acc: 0.6000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.4108 - acc: 0.7742 - val_loss: 0.7431 - val_acc: 0.6667\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1542 - acc: 0.9032 - val_loss: 6.3723e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.7560e-04 - acc: 1.0000 - val_loss: 1.4811e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 1.3950e-04 - acc: 1.0000 - val_loss: 1.2350e-04 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 1.4662e-04 - acc: 1.0000 - val_loss: 1.3918e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.5063e-04 - acc: 1.0000 - val_loss: 1.7258e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8970e-04 - acc: 1.0000 - val_loss: 2.1917e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 2.6767e-04 - acc: 1.0000 - val_loss: 2.7706e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.5747e-04 - acc: 1.0000 - val_loss: 3.4479e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.7435e-04 - acc: 1.0000 - val_loss: 4.2100e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 6.5788e-04 - acc: 1.0000 - val_loss: 5.0377e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 6.7327e-04 - acc: 1.0000 - val_loss: 5.9135e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.8987e-04 - acc: 1.0000 - val_loss: 6.8168e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 6.3541e-04 - acc: 1.0000 - val_loss: 7.7342e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.8849e-04 - acc: 1.0000 - val_loss: 8.6463e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.9416e-04 - acc: 1.0000 - val_loss: 9.5343e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.6977 - acc: 0.4839 - val_loss: 0.6926 - val_acc: 0.4667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6934 - acc: 0.5161 - val_loss: 0.6832 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6822 - acc: 0.6452 - val_loss: 0.6740 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6671 - acc: 0.5806 - val_loss: 0.6603 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6523 - acc: 0.5806 - val_loss: 0.6387 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6045 - acc: 0.8065 - val_loss: 0.6064 - val_acc: 0.6000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 830ms/step - loss: 0.5960 - acc: 0.7097 - val_loss: 0.5570 - val_acc: 0.8000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5418 - acc: 0.8710 - val_loss: 0.4842 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 697ms/step - loss: 0.4521 - acc: 0.9355 - val_loss: 0.3827 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 945ms/step - loss: 0.3634 - acc: 1.0000 - val_loss: 0.2592 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 0.2363 - acc: 1.0000 - val_loss: 0.1405 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 740ms/step - loss: 0.1216 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.0492 - acc: 1.0000 - val_loss: 0.0201 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 830ms/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0093 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 578ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.8338e-04 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 837ms/step - loss: 1.2507e-04 - acc: 1.0000 - val_loss: 6.0694e-05 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 7.2525e-05 - acc: 1.0000 - val_loss: 1.3235e-04 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0232 - acc: 0.9677 - val_loss: 9.6186e-05 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 6.4121e-05 - acc: 1.0000 - val_loss: 3.3277 - val_acc: 0.6000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.1735 - acc: 0.8065 - val_loss: 6.8406e-05 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 4.3559e-05 - acc: 1.0000 - val_loss: 2.3467e-05 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.7221e-05 - acc: 1.0000 - val_loss: 2.6039e-05 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.3088e-05 - acc: 1.0000 - val_loss: 3.7784e-05 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.3069e-05 - acc: 1.0000 - val_loss: 5.6629e-05 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 4.2346e-05 - acc: 1.0000 - val_loss: 8.0768e-05 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.0435e-04 - acc: 1.0000 - val_loss: 1.0867e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.6912e-04 - acc: 1.0000 - val_loss: 1.3925e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7537e-04 - acc: 1.0000 - val_loss: 1.7165e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.9309e-04 - acc: 1.0000 - val_loss: 2.0543e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 2.7057e-04 - acc: 1.0000 - val_loss: 2.3970e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5241e-04 - acc: 1.0000 - val_loss: 2.7406e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.4214e-04 - acc: 1.0000 - val_loss: 3.0710e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.1053e-04 - acc: 1.0000 - val_loss: 3.3829e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.9239e-04 - acc: 1.0000 - val_loss: 3.6674e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 3.0686e-04 - acc: 1.0000 - val_loss: 3.9284e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 4.4169e-04 - acc: 1.0000 - val_loss: 4.1568e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.8767e-04 - acc: 1.0000 - val_loss: 4.3416e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.9780e-04 - acc: 1.0000 - val_loss: 4.4943e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 5.8416e-04 - acc: 1.0000 - val_loss: 4.6037e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.1323e-04 - acc: 1.0000 - val_loss: 4.6714e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.3154e-04 - acc: 1.0000 - val_loss: 4.7069e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.1403e-04 - acc: 1.0000 - val_loss: 4.7104e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.9191e-04 - acc: 1.0000 - val_loss: 4.6753e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 6.2146e-04 - acc: 1.0000 - val_loss: 4.6086e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 4.4288e-04 - acc: 1.0000 - val_loss: 4.5235e-04 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.7145e-04 - acc: 1.0000 - val_loss: 4.4266e-04 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 6.2336e-04 - acc: 1.0000 - val_loss: 4.3073e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.9275e-04 - acc: 1.0000 - val_loss: 4.1752e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.0084e-04 - acc: 1.0000 - val_loss: 4.0365e-04 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.4763e-04 - acc: 1.0000 - val_loss: 3.8886e-04 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.1687e-04 - acc: 1.0000 - val_loss: 3.7343e-04 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.8152e-04 - acc: 1.0000 - val_loss: 3.5829e-04 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.6049e-04 - acc: 1.0000 - val_loss: 3.4319e-04 - val_acc: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.5435e-04 - acc: 1.0000 - val_loss: 3.2830e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.6954 - acc: 0.4839 - val_loss: 0.6907 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6892 - acc: 0.5806 - val_loss: 0.6810 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6751 - acc: 0.7097 - val_loss: 0.6696 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6669 - acc: 0.5484 - val_loss: 0.6517 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6442 - acc: 0.6452 - val_loss: 0.6222 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5842 - acc: 0.8387 - val_loss: 0.5750 - val_acc: 0.7333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 773ms/step - loss: 0.5649 - acc: 0.7742 - val_loss: 0.5012 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 0.4812 - acc: 0.9032 - val_loss: 0.3954 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 891ms/step - loss: 0.3736 - acc: 0.9677 - val_loss: 0.2641 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2529 - acc: 1.0000 - val_loss: 0.1382 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 0.1310 - acc: 1.0000 - val_loss: 0.0554 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 0.0579 - acc: 1.0000 - val_loss: 0.0164 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 992ms/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 901ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0119 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 9.4624e-04 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 573ms/step - loss: 5.4754e-04 - acc: 1.0000 - val_loss: 1.7373e-04 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8516e-04 - acc: 1.0000 - val_loss: 8.4321e-05 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 9.9641e-05 - acc: 1.0000 - val_loss: 5.8375e-05 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 8.8035e-05 - acc: 1.0000 - val_loss: 4.5033e-05 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 1.4550e-04 - acc: 1.0000 - val_loss: 4.3931e-05 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.1409 - acc: 0.9677 - val_loss: 3.3184e-05 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.0488e-05 - acc: 1.0000 - val_loss: 2.2179e-04 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 9.1833e-05 - acc: 1.0000 - val_loss: 0.2368 - val_acc: 0.8667\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0485 - acc: 0.9677 - val_loss: 0.2734 - val_acc: 0.8667\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.0560 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 8.0788e-04 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.1462e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 9.7708e-05 - acc: 1.0000 - val_loss: 8.8135e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 6.0658e-05 - acc: 1.0000 - val_loss: 4.7289e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 1.0680e-04 - acc: 1.0000 - val_loss: 3.1149e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 7.3079e-05 - acc: 1.0000 - val_loss: 2.3370e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 5.8747e-05 - acc: 1.0000 - val_loss: 1.9063e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 5.3157e-05 - acc: 1.0000 - val_loss: 1.6436e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 6.3332e-05 - acc: 1.0000 - val_loss: 1.4710e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.8507e-05 - acc: 1.0000 - val_loss: 1.3512e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 5.1022e-05 - acc: 1.0000 - val_loss: 1.2647e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.1864e-05 - acc: 1.0000 - val_loss: 1.1995e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 4.4079e-05 - acc: 1.0000 - val_loss: 1.1493e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.0777e-05 - acc: 1.0000 - val_loss: 1.1092e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.7908e-05 - acc: 1.0000 - val_loss: 1.0770e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.3325e-05 - acc: 1.0000 - val_loss: 1.0496e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 3.7055e-05 - acc: 1.0000 - val_loss: 1.0265e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.8167e-05 - acc: 1.0000 - val_loss: 1.0069e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.6214e-05 - acc: 1.0000 - val_loss: 9.8913e-05 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.4026e-05 - acc: 1.0000 - val_loss: 9.7327e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.3881e-05 - acc: 1.0000 - val_loss: 9.5966e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.2361e-05 - acc: 1.0000 - val_loss: 9.4688e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 4.1032e-05 - acc: 1.0000 - val_loss: 9.3493e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.9743e-05 - acc: 1.0000 - val_loss: 9.2385e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.2426e-05 - acc: 1.0000 - val_loss: 9.1311e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.0009e-05 - acc: 1.0000 - val_loss: 9.0292e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1692e-05 - acc: 1.0000 - val_loss: 8.9372e-05 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.0624e-05 - acc: 1.0000 - val_loss: 8.8447e-05 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1477e-05 - acc: 1.0000 - val_loss: 8.7604e-05 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.1708e-05 - acc: 1.0000 - val_loss: 8.6789e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.6908 - acc: 0.6452 - val_loss: 0.6818 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6926 - acc: 0.4839 - val_loss: 0.6844 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6872 - acc: 0.6774 - val_loss: 0.6755 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6717 - acc: 0.7742 - val_loss: 0.6632 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6581 - acc: 0.7742 - val_loss: 0.6452 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6219 - acc: 0.8387 - val_loss: 0.6189 - val_acc: 0.6000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6027 - acc: 0.7742 - val_loss: 0.5802 - val_acc: 0.7333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5643 - acc: 0.8065 - val_loss: 0.5230 - val_acc: 0.9333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4973 - acc: 0.9355 - val_loss: 0.4420 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 900ms/step - loss: 0.4230 - acc: 1.0000 - val_loss: 0.3359 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 459ms/step - loss: 0.3003 - acc: 1.0000 - val_loss: 0.2191 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 591ms/step - loss: 0.2000 - acc: 1.0000 - val_loss: 0.1216 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1092 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.0558 - acc: 1.0000 - val_loss: 0.0246 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.0249 - acc: 1.0000 - val_loss: 0.0113 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 935ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0093 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 6.4024e-04 - acc: 1.0000 - val_loss: 3.9916e-04 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 3.0125e-04 - acc: 1.0000 - val_loss: 2.1763e-04 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 2.5284e-04 - acc: 1.0000 - val_loss: 1.4568e-04 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 1.5061e-04 - acc: 1.0000 - val_loss: 1.0997e-04 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 1.4514e-04 - acc: 1.0000 - val_loss: 9.0617e-05 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 1.4538e-04 - acc: 1.0000 - val_loss: 8.4414e-05 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 2.3588e-04 - acc: 1.0000 - val_loss: 7.4592e-05 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 6.1581e-05 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.1850e-05 - acc: 1.0000 - val_loss: 1.0271 - val_acc: 0.6667\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.2257 - acc: 0.9032 - val_loss: 5.3808e-05 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.0161e-05 - acc: 1.0000 - val_loss: 5.5356e-05 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.3485e-05 - acc: 1.0000 - val_loss: 6.4054e-05 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.7230e-05 - acc: 1.0000 - val_loss: 8.5268e-05 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.2220e-04 - acc: 1.0000 - val_loss: 1.2844e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.3096e-04 - acc: 1.0000 - val_loss: 1.7823e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6235e-04 - acc: 1.0000 - val_loss: 2.0710e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.4133e-04 - acc: 1.0000 - val_loss: 1.9661e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5385e-04 - acc: 1.0000 - val_loss: 1.7738e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.2735e-04 - acc: 1.0000 - val_loss: 1.5871e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3025e-04 - acc: 1.0000 - val_loss: 1.4440e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.1703e-04 - acc: 1.0000 - val_loss: 1.3382e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3590e-04 - acc: 1.0000 - val_loss: 1.2590e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1208e-04 - acc: 1.0000 - val_loss: 1.1983e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7579e-04 - acc: 1.0000 - val_loss: 1.1485e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.0496e-04 - acc: 1.0000 - val_loss: 1.1075e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.2339e-04 - acc: 1.0000 - val_loss: 1.0717e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.8000e-04 - acc: 1.0000 - val_loss: 1.0384e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3015e-04 - acc: 1.0000 - val_loss: 1.0074e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.0504e-04 - acc: 1.0000 - val_loss: 9.7889e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 9.6607e-05 - acc: 1.0000 - val_loss: 9.5217e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7079e-04 - acc: 1.0000 - val_loss: 9.2485e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 9.7711e-05 - acc: 1.0000 - val_loss: 8.9907e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.1350e-05 - acc: 1.0000 - val_loss: 8.7483e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1424e-04 - acc: 1.0000 - val_loss: 8.5081e-05 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.1436e-04 - acc: 1.0000 - val_loss: 8.2712e-05 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.0140e-04 - acc: 1.0000 - val_loss: 8.0410e-05 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0562e-04 - acc: 1.0000 - val_loss: 7.8195e-05 - val_acc: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0577e-04 - acc: 1.0000 - val_loss: 7.5992e-05 - val_acc: 1.0000\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.1430e-04 - acc: 1.0000 - val_loss: 7.3838e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.6941 - acc: 0.4839 - val_loss: 0.6848 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6838 - acc: 0.6129 - val_loss: 0.6722 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6667 - acc: 0.6129 - val_loss: 0.6561 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6509 - acc: 0.6129 - val_loss: 0.6293 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6178 - acc: 0.7742 - val_loss: 0.5864 - val_acc: 0.8000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5315 - acc: 0.9677 - val_loss: 0.5223 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5038 - acc: 0.9032 - val_loss: 0.4300 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4005 - acc: 0.9355 - val_loss: 0.3095 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2824 - acc: 1.0000 - val_loss: 0.1797 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 0.1628 - acc: 1.0000 - val_loss: 0.0781 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 503ms/step - loss: 0.0678 - acc: 1.0000 - val_loss: 0.0259 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 0.0231 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 783ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 961ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 7.4033e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 4.5481e-04 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 6.4934e-04 - acc: 1.0000 - val_loss: 0.0259 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 9.4699e-05 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 1.0051e-04 - acc: 1.0000 - val_loss: 6.4886e-05 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 6.5868e-04 - acc: 1.0000 - val_loss: 0.0958 - val_acc: 0.9333\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.4322 - acc: 0.9032 - val_loss: 6.4407e-05 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 5.9643e-05 - acc: 1.0000 - val_loss: 2.2285e-04 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 1.5597e-04 - acc: 1.0000 - val_loss: 0.0178 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 8.2587e-04 - acc: 1.0000 - val_loss: 0.3467 - val_acc: 0.8667\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.5421 - val_acc: 0.8000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0346 - acc: 0.9677 - val_loss: 0.2254 - val_acc: 0.8667\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0703 - val_acc: 0.9333\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0212 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 7.2596e-04 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 5.6250e-04 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.3802e-04 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 4.9331e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0660e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.3576e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.5621e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9103e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.8804e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.6578e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8527e-04 - acc: 1.0000 - val_loss: 9.7421e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 3.2658e-04 - acc: 1.0000 - val_loss: 9.2223e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.7497e-04 - acc: 1.0000 - val_loss: 8.7645e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8031e-04 - acc: 1.0000 - val_loss: 8.3545e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7269e-04 - acc: 1.0000 - val_loss: 7.9816e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.0757e-04 - acc: 1.0000 - val_loss: 7.6381e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.2098e-04 - acc: 1.0000 - val_loss: 7.3151e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.1989e-04 - acc: 1.0000 - val_loss: 7.0115e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.0653e-04 - acc: 1.0000 - val_loss: 6.7293e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8163e-04 - acc: 1.0000 - val_loss: 6.4630e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8424e-04 - acc: 1.0000 - val_loss: 6.2105e-04 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5700e-04 - acc: 1.0000 - val_loss: 5.9711e-04 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.3331e-04 - acc: 1.0000 - val_loss: 5.7464e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.3431e-04 - acc: 1.0000 - val_loss: 5.5348e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9961e-04 - acc: 1.0000 - val_loss: 5.3313e-04 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.4951e-04 - acc: 1.0000 - val_loss: 5.1416e-04 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.7315e-05 - acc: 1.0000 - val_loss: 4.9677e-04 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3510e-04 - acc: 1.0000 - val_loss: 4.8036e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.6967 - acc: 0.4194 - val_loss: 0.6858 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6819 - acc: 0.7097 - val_loss: 0.6734 - val_acc: 0.6000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6673 - acc: 0.7419 - val_loss: 0.6567 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6453 - acc: 0.8065 - val_loss: 0.6295 - val_acc: 0.6000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6269 - acc: 0.7097 - val_loss: 0.5852 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5439 - acc: 0.8710 - val_loss: 0.5175 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5313 - acc: 0.8710 - val_loss: 0.4178 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4041 - acc: 0.9677 - val_loss: 0.2897 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 616ms/step - loss: 0.2877 - acc: 0.9677 - val_loss: 0.1609 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1628 - acc: 1.0000 - val_loss: 0.0680 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 570ms/step - loss: 0.0708 - acc: 1.0000 - val_loss: 0.0211 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 783ms/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 5.1666e-04 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 671ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 5.9486e-04 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 5.1681e-04 - acc: 1.0000 - val_loss: 1.2177e-04 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.0501e-04 - acc: 1.0000 - val_loss: 4.8570e-05 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 4.1269e-05 - acc: 1.0000 - val_loss: 2.9003e-05 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 3.2895e-05 - acc: 1.0000 - val_loss: 2.0867e-05 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 3.2577e-05 - acc: 1.0000 - val_loss: 1.6284e-05 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.3549e-05 - acc: 1.0000 - val_loss: 1.3331e-05 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.2129e-05 - acc: 1.0000 - val_loss: 1.1249e-05 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 1.3506e-05 - acc: 1.0000 - val_loss: 9.7448e-06 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 1.7171e-05 - acc: 1.0000 - val_loss: 8.5825e-06 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 1.0569e-05 - acc: 1.0000 - val_loss: 7.7066e-06 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 1.0792e-05 - acc: 1.0000 - val_loss: 7.1557e-06 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.2296e-04 - acc: 1.0000 - val_loss: 6.2909e-06 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.1294e-05 - acc: 1.0000 - val_loss: 5.7675e-06 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2549e-05 - acc: 1.0000 - val_loss: 5.3764e-06 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 7.4153e-06 - acc: 1.0000 - val_loss: 5.0569e-06 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 7.9031e-06 - acc: 1.0000 - val_loss: 4.7760e-06 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 8.4495e-06 - acc: 1.0000 - val_loss: 4.5722e-06 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.7245e-06 - acc: 1.0000 - val_loss: 4.4014e-06 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.9253e-06 - acc: 1.0000 - val_loss: 4.2306e-06 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.8312e-06 - acc: 1.0000 - val_loss: 4.1205e-06 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.8783e-06 - acc: 1.0000 - val_loss: 4.0323e-06 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 4.3820e-06 - acc: 1.0000 - val_loss: 3.9552e-06 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.9462e-06 - acc: 1.0000 - val_loss: 3.8560e-06 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.3656e-06 - acc: 1.0000 - val_loss: 3.8175e-06 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.6161e-06 - acc: 1.0000 - val_loss: 3.7844e-06 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.0377e-06 - acc: 1.0000 - val_loss: 3.7514e-06 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.8436e-06 - acc: 1.0000 - val_loss: 3.7238e-06 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 5.7867e-06 - acc: 1.0000 - val_loss: 3.7018e-06 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.3061e-06 - acc: 1.0000 - val_loss: 3.7018e-06 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 6.3038e-06 - acc: 1.0000 - val_loss: 3.6798e-06 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.5580e-06 - acc: 1.0000 - val_loss: 3.6743e-06 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 4.8618e-06 - acc: 1.0000 - val_loss: 3.6743e-06 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.9733e-06 - acc: 1.0000 - val_loss: 3.6798e-06 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 4.9711e-06 - acc: 1.0000 - val_loss: 3.6798e-06 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.6033e-06 - acc: 1.0000 - val_loss: 3.6908e-06 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.8294e-06 - acc: 1.0000 - val_loss: 3.6743e-06 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 4.8432e-06 - acc: 1.0000 - val_loss: 3.6357e-06 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.7205e-06 - acc: 1.0000 - val_loss: 3.6632e-06 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.6920 - acc: 0.6452 - val_loss: 0.6922 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6911 - acc: 0.6129 - val_loss: 0.6873 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6862 - acc: 0.5484 - val_loss: 0.6815 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6815 - acc: 0.5484 - val_loss: 0.6714 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6643 - acc: 0.5806 - val_loss: 0.6533 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6394 - acc: 0.6129 - val_loss: 0.6202 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 0.6031 - acc: 0.6774 - val_loss: 0.5585 - val_acc: 0.7333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 942ms/step - loss: 0.5430 - acc: 0.8387 - val_loss: 0.4484 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 910ms/step - loss: 0.4304 - acc: 0.9677 - val_loss: 0.2891 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2756 - acc: 1.0000 - val_loss: 0.1357 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 724ms/step - loss: 0.1304 - acc: 1.0000 - val_loss: 1.4224 - val_acc: 0.5333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 1.0542 - acc: 0.5806 - val_loss: 0.0647 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 805ms/step - loss: 0.0729 - acc: 1.0000 - val_loss: 0.3889 - val_acc: 0.8000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3960 - acc: 0.7419 - val_loss: 0.2699 - val_acc: 0.8000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.2873 - acc: 0.8387 - val_loss: 0.1220 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.1124 - acc: 0.9677 - val_loss: 0.0837 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0703 - acc: 1.0000 - val_loss: 0.0869 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0666 - acc: 1.0000 - val_loss: 0.1004 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 0.0680 - acc: 1.0000 - val_loss: 0.1118 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.0718 - acc: 1.0000 - val_loss: 0.1155 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0639 - acc: 1.0000 - val_loss: 0.1077 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0520 - acc: 1.0000 - val_loss: 0.0778 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0332 - acc: 1.0000 - val_loss: 0.0340 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 8.3940e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 6.7702e-04 - acc: 1.0000 - val_loss: 2.9694e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5789e-04 - acc: 1.0000 - val_loss: 1.2982e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.1708e-04 - acc: 1.0000 - val_loss: 7.2967e-05 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 1s 596ms/step - loss: 7.8799e-05 - acc: 1.0000 - val_loss: 5.1181e-05 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 6.3201e-05 - acc: 1.0000 - val_loss: 4.1667e-05 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.6719e-05 - acc: 1.0000 - val_loss: 3.6891e-05 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.2994e-05 - acc: 1.0000 - val_loss: 3.4203e-05 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.4860e-05 - acc: 1.0000 - val_loss: 3.2551e-05 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.7127e-05 - acc: 1.0000 - val_loss: 3.1416e-05 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 4.6592e-05 - acc: 1.0000 - val_loss: 3.0589e-05 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.6418e-05 - acc: 1.0000 - val_loss: 2.9923e-05 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.1136e-05 - acc: 1.0000 - val_loss: 2.9361e-05 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.4991e-05 - acc: 1.0000 - val_loss: 2.8887e-05 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8451e-05 - acc: 1.0000 - val_loss: 2.8430e-05 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1437e-05 - acc: 1.0000 - val_loss: 2.8011e-05 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8142e-05 - acc: 1.0000 - val_loss: 2.7587e-05 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 3.0661e-05 - acc: 1.0000 - val_loss: 2.7158e-05 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0328e-05 - acc: 1.0000 - val_loss: 2.6667e-05 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9931e-05 - acc: 1.0000 - val_loss: 2.6199e-05 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1149e-05 - acc: 1.0000 - val_loss: 2.5698e-05 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.4531e-05 - acc: 1.0000 - val_loss: 2.5174e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.4973e-05 - acc: 1.0000 - val_loss: 2.4651e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.3509e-05 - acc: 1.0000 - val_loss: 2.4128e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7633e-05 - acc: 1.0000 - val_loss: 2.3605e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1549e-05 - acc: 1.0000 - val_loss: 2.3065e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.4021e-05 - acc: 1.0000 - val_loss: 2.2497e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.3792e-05 - acc: 1.0000 - val_loss: 2.1968e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.1177e-05 - acc: 1.0000 - val_loss: 2.1423e-05 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7708e-05 - acc: 1.0000 - val_loss: 2.0889e-05 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.3083e-05 - acc: 1.0000 - val_loss: 2.0338e-05 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.1113e-05 - acc: 1.0000 - val_loss: 1.9826e-05 - val_acc: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.1121e-05 - acc: 1.0000 - val_loss: 1.9302e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.6931 - acc: 0.5484 - val_loss: 0.6875 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6861 - acc: 0.6129 - val_loss: 0.6796 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6790 - acc: 0.6452 - val_loss: 0.6660 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6673 - acc: 0.6774 - val_loss: 0.6414 - val_acc: 0.6000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6370 - acc: 0.7742 - val_loss: 0.5952 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5711 - acc: 0.9355 - val_loss: 0.5087 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5045 - acc: 1.0000 - val_loss: 0.3712 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3711 - acc: 1.0000 - val_loss: 0.2279 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2579 - acc: 0.9032 - val_loss: 0.1344 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1320 - acc: 1.0000 - val_loss: 0.1149 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 0.0810 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - acc: 1.0000 - val_loss: 1.1887e-04 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 8.5919e-05 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1483 - val_acc: 0.9333\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0482 - acc: 0.9677 - val_loss: 8.8689e-06 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 7.0955e-06 - acc: 1.0000 - val_loss: 7.9930e-06 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 1.2685e-05 - acc: 1.0000 - val_loss: 2.1120e-05 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 5.2918e-05 - acc: 1.0000 - val_loss: 2.6838e-05 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.7382e-05 - acc: 1.0000 - val_loss: 1.6295e-05 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 2.1406e-05 - acc: 1.0000 - val_loss: 7.2769e-06 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.4436e-05 - acc: 1.0000 - val_loss: 3.3878e-06 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 3.3025e-06 - acc: 1.0000 - val_loss: 1.8895e-06 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 2.2443e-06 - acc: 1.0000 - val_loss: 1.3055e-06 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9031e-06 - acc: 1.0000 - val_loss: 1.0356e-06 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.1035e-06 - acc: 1.0000 - val_loss: 9.0342e-07 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 7.1168e-07 - acc: 1.0000 - val_loss: 8.4833e-07 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 7.7832e-07 - acc: 1.0000 - val_loss: 8.5935e-07 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.8227e-07 - acc: 1.0000 - val_loss: 9.1443e-07 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.2767e-07 - acc: 1.0000 - val_loss: 1.0246e-06 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 8.2896e-07 - acc: 1.0000 - val_loss: 1.2229e-06 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.2905e-07 - acc: 1.0000 - val_loss: 1.5259e-06 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.8760e-07 - acc: 1.0000 - val_loss: 1.9721e-06 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.0102e-07 - acc: 1.0000 - val_loss: 2.6001e-06 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.8374e-07 - acc: 1.0000 - val_loss: 3.4319e-06 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 6.3172e-07 - acc: 1.0000 - val_loss: 4.4785e-06 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.2234e-07 - acc: 1.0000 - val_loss: 5.8006e-06 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.4358e-07 - acc: 1.0000 - val_loss: 7.3651e-06 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.5699e-07 - acc: 1.0000 - val_loss: 9.1719e-06 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 6.4771e-07 - acc: 1.0000 - val_loss: 1.1172e-05 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.1248e-06 - acc: 1.0000 - val_loss: 1.3331e-05 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.2448e-06 - acc: 1.0000 - val_loss: 1.5617e-05 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.9036e-07 - acc: 1.0000 - val_loss: 1.8002e-05 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 9.1159e-07 - acc: 1.0000 - val_loss: 2.0404e-05 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.7032e-07 - acc: 1.0000 - val_loss: 2.2828e-05 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 9.4891e-07 - acc: 1.0000 - val_loss: 2.5202e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.5029e-07 - acc: 1.0000 - val_loss: 2.7499e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.4100e-07 - acc: 1.0000 - val_loss: 2.9747e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.3087e-06 - acc: 1.0000 - val_loss: 3.1873e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.9027e-07 - acc: 1.0000 - val_loss: 3.3911e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.9431e-07 - acc: 1.0000 - val_loss: 3.5812e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 9.5957e-07 - acc: 1.0000 - val_loss: 3.7607e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.0773e-07 - acc: 1.0000 - val_loss: 3.9271e-05 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 9.0093e-07 - acc: 1.0000 - val_loss: 4.0836e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.6921 - acc: 0.5484 - val_loss: 0.6883 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6873 - acc: 0.5484 - val_loss: 0.6794 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6781 - acc: 0.5806 - val_loss: 0.6655 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6591 - acc: 0.5484 - val_loss: 0.6398 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6309 - acc: 0.5806 - val_loss: 0.5903 - val_acc: 0.8000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5533 - acc: 0.9355 - val_loss: 0.4975 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4846 - acc: 1.0000 - val_loss: 0.3580 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 0.3618 - acc: 0.9355 - val_loss: 0.2014 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 817ms/step - loss: 0.2142 - acc: 0.9355 - val_loss: 0.1379 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 938ms/step - loss: 0.1076 - acc: 1.0000 - val_loss: 0.0414 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0304 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0131 - acc: 1.0000 - val_loss: 5.3005e-04 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.2266e-04 - acc: 1.0000 - val_loss: 2.7447e-04 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 744ms/step - loss: 8.4498e-05 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 736ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 4.3252e-04 - acc: 1.0000 - val_loss: 4.3606e-05 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 829ms/step - loss: 1.7747e-05 - acc: 1.0000 - val_loss: 2.0712e-06 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.5726e-06 - acc: 1.0000 - val_loss: 4.8476e-07 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 8.3696e-07 - acc: 1.0000 - val_loss: 2.5891e-07 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 3.8383e-07 - acc: 1.0000 - val_loss: 2.3687e-07 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 7.4367e-07 - acc: 1.0000 - val_loss: 3.1950e-07 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0358 - acc: 0.9677 - val_loss: 6.6782 - val_acc: 0.5333\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.0212 - acc: 0.6452 - val_loss: 5.5026 - val_acc: 0.5333\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 3.9608 - acc: 0.6452 - val_loss: 1.8988 - val_acc: 0.6000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.3818 - acc: 0.9032 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.5118e-05 - acc: 1.0000 - val_loss: 1.5866e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 9.7146e-05 - acc: 1.0000 - val_loss: 3.6212e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.3029e-04 - acc: 1.0000 - val_loss: 9.8849e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.6681e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.0215 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.0285 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0352 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0223 - acc: 1.0000 - val_loss: 0.0409 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0269 - acc: 1.0000 - val_loss: 0.0453 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.0486 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0287 - acc: 1.0000 - val_loss: 0.0510 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0290 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0255 - acc: 1.0000 - val_loss: 0.0547 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.0566 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0617 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.0644 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.0670 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.0695 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.0723 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0759 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0766 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0754 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0759 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0765 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0793 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0929 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.6908 - acc: 0.7419 - val_loss: 0.6897 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6890 - acc: 0.6129 - val_loss: 0.6845 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6835 - acc: 0.5806 - val_loss: 0.6757 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6734 - acc: 0.5806 - val_loss: 0.6610 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6588 - acc: 0.5806 - val_loss: 0.6336 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6045 - acc: 0.7742 - val_loss: 0.5824 - val_acc: 0.9333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 820ms/step - loss: 0.5745 - acc: 0.8065 - val_loss: 0.4889 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4822 - acc: 1.0000 - val_loss: 0.3594 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 729ms/step - loss: 0.3617 - acc: 0.9355 - val_loss: 0.2228 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2373 - acc: 0.9032 - val_loss: 0.1276 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.1106 - acc: 1.0000 - val_loss: 0.1088 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0541 - acc: 1.0000 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.1912e-04 - acc: 1.0000 - val_loss: 0.0795 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 694ms/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 6.9113e-04 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 9.9886e-05 - acc: 1.0000 - val_loss: 1.2097e-04 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 4.7003e-05 - acc: 1.0000 - val_loss: 3.5905e-05 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 1.8632e-05 - acc: 1.0000 - val_loss: 1.5331e-05 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 6.2798e-06 - acc: 1.0000 - val_loss: 8.2960e-06 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 4.2141e-06 - acc: 1.0000 - val_loss: 5.2387e-06 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7241e-06 - acc: 1.0000 - val_loss: 3.6688e-06 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.2630e-06 - acc: 1.0000 - val_loss: 2.7708e-06 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 2.3989e-06 - acc: 1.0000 - val_loss: 2.1869e-06 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1804e-06 - acc: 1.0000 - val_loss: 1.8289e-06 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9271e-06 - acc: 1.0000 - val_loss: 1.5700e-06 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 8.5295e-07 - acc: 1.0000 - val_loss: 1.3827e-06 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 9.8089e-07 - acc: 1.0000 - val_loss: 1.2284e-06 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.1088e-06 - acc: 1.0000 - val_loss: 1.1403e-06 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.2767e-07 - acc: 1.0000 - val_loss: 1.0577e-06 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 9.1426e-07 - acc: 1.0000 - val_loss: 1.0136e-06 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 1.0902e-06 - acc: 1.0000 - val_loss: 9.6401e-07 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.5571e-07 - acc: 1.0000 - val_loss: 9.4198e-07 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.3576e-07 - acc: 1.0000 - val_loss: 9.2545e-07 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 6.9569e-07 - acc: 1.0000 - val_loss: 9.1994e-07 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.9036e-07 - acc: 1.0000 - val_loss: 9.3096e-07 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 5.8907e-07 - acc: 1.0000 - val_loss: 9.5299e-07 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.7445e-07 - acc: 1.0000 - val_loss: 9.4198e-07 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.7703e-07 - acc: 1.0000 - val_loss: 9.6952e-07 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.3172e-07 - acc: 1.0000 - val_loss: 9.9156e-07 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7721e-07 - acc: 1.0000 - val_loss: 1.0081e-06 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.6775e-07 - acc: 1.0000 - val_loss: 1.0411e-06 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.4780e-07 - acc: 1.0000 - val_loss: 1.0687e-06 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.4109e-07 - acc: 1.0000 - val_loss: 1.0907e-06 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.3181e-07 - acc: 1.0000 - val_loss: 1.1183e-06 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.7978e-07 - acc: 1.0000 - val_loss: 1.1293e-06 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.7850e-07 - acc: 1.0000 - val_loss: 1.1568e-06 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.4780e-07 - acc: 1.0000 - val_loss: 1.1844e-06 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.3989e-07 - acc: 1.0000 - val_loss: 1.2119e-06 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.1444e-07 - acc: 1.0000 - val_loss: 1.2450e-06 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.5580e-07 - acc: 1.0000 - val_loss: 1.2670e-06 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.8245e-07 - acc: 1.0000 - val_loss: 1.2945e-06 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9054e-07 - acc: 1.0000 - val_loss: 1.3111e-06 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.5975e-07 - acc: 1.0000 - val_loss: 1.3331e-06 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0919e-07 - acc: 1.0000 - val_loss: 1.3606e-06 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5055e-07 - acc: 1.0000 - val_loss: 1.3827e-06 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.6940 - acc: 0.5161 - val_loss: 0.6865 - val_acc: 0.8667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6840 - acc: 0.7419 - val_loss: 0.6740 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6699 - acc: 0.7097 - val_loss: 0.6538 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6536 - acc: 0.7419 - val_loss: 0.6166 - val_acc: 0.8667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6129 - acc: 0.7742 - val_loss: 0.5455 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5113 - acc: 0.9677 - val_loss: 0.4184 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 0.4121 - acc: 1.0000 - val_loss: 0.2541 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2502 - acc: 0.9677 - val_loss: 0.1014 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 739ms/step - loss: 0.1052 - acc: 1.0000 - val_loss: 0.0936 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0642 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 726ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0182 - acc: 1.0000 - val_loss: 2.3894e-04 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0891e-04 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 599ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0645 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 481ms/step - loss: 0.0258 - acc: 1.0000 - val_loss: 6.6296e-05 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 3.3883e-05 - acc: 1.0000 - val_loss: 6.3790e-06 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 5.7974e-06 - acc: 1.0000 - val_loss: 3.3437e-06 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 3.1666e-06 - acc: 1.0000 - val_loss: 2.3687e-06 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 3.1293e-06 - acc: 1.0000 - val_loss: 1.7793e-06 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.7539e-06 - acc: 1.0000 - val_loss: 1.3882e-06 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 1.4687e-06 - acc: 1.0000 - val_loss: 1.1348e-06 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 1.0955e-06 - acc: 1.0000 - val_loss: 9.3647e-07 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 8.2630e-07 - acc: 1.0000 - val_loss: 8.2079e-07 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 8.8227e-07 - acc: 1.0000 - val_loss: 7.6019e-07 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.1297e-07 - acc: 1.0000 - val_loss: 7.0511e-07 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 1.0129e-06 - acc: 1.0000 - val_loss: 6.8858e-07 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.0497e-07 - acc: 1.0000 - val_loss: 6.8307e-07 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.2114e-07 - acc: 1.0000 - val_loss: 7.0511e-07 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 5.7308e-07 - acc: 1.0000 - val_loss: 7.4367e-07 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 6.8236e-07 - acc: 1.0000 - val_loss: 7.9324e-07 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.7041e-07 - acc: 1.0000 - val_loss: 8.4833e-07 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.5975e-07 - acc: 1.0000 - val_loss: 9.3096e-07 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 4.8778e-07 - acc: 1.0000 - val_loss: 1.0136e-06 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 3.9982e-07 - acc: 1.0000 - val_loss: 1.1072e-06 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.1581e-07 - acc: 1.0000 - val_loss: 1.2229e-06 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.1581e-07 - acc: 1.0000 - val_loss: 1.3331e-06 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 4.1581e-07 - acc: 1.0000 - val_loss: 1.4653e-06 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.6250e-07 - acc: 1.0000 - val_loss: 1.5865e-06 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.1848e-07 - acc: 1.0000 - val_loss: 1.7077e-06 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.8512e-07 - acc: 1.0000 - val_loss: 1.8509e-06 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.3576e-07 - acc: 1.0000 - val_loss: 1.9831e-06 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.6783e-07 - acc: 1.0000 - val_loss: 2.1263e-06 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.5846e-07 - acc: 1.0000 - val_loss: 2.2696e-06 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.8649e-07 - acc: 1.0000 - val_loss: 2.4073e-06 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.9449e-07 - acc: 1.0000 - val_loss: 2.5285e-06 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.6517e-07 - acc: 1.0000 - val_loss: 2.6386e-06 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.6646e-07 - acc: 1.0000 - val_loss: 2.7653e-06 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.2648e-07 - acc: 1.0000 - val_loss: 2.8755e-06 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.2914e-07 - acc: 1.0000 - val_loss: 2.9857e-06 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0919e-07 - acc: 1.0000 - val_loss: 3.0903e-06 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.3318e-07 - acc: 1.0000 - val_loss: 3.1840e-06 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.7050e-07 - acc: 1.0000 - val_loss: 3.2776e-06 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.6783e-07 - acc: 1.0000 - val_loss: 3.3603e-06 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9320e-07 - acc: 1.0000 - val_loss: 3.4429e-06 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.4918e-07 - acc: 1.0000 - val_loss: 3.5145e-06 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.6949 - acc: 0.4839 - val_loss: 0.6881 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6861 - acc: 0.5484 - val_loss: 0.6765 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6720 - acc: 0.6452 - val_loss: 0.6561 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6449 - acc: 0.7097 - val_loss: 0.6155 - val_acc: 0.6667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6013 - acc: 0.8387 - val_loss: 0.5338 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4861 - acc: 0.9677 - val_loss: 0.3848 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3729 - acc: 1.0000 - val_loss: 0.2239 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2459 - acc: 0.9355 - val_loss: 0.0998 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1058 - acc: 0.9677 - val_loss: 0.1149 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 699ms/step - loss: 0.0734 - acc: 1.0000 - val_loss: 0.0213 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - acc: 1.0000 - val_loss: 8.6710e-04 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 600ms/step - loss: 7.9687e-04 - acc: 1.0000 - val_loss: 2.9936e-04 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 3.9351e-04 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0148 - acc: 1.0000 - val_loss: 1.6625e-05 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 1.0353e-05 - acc: 1.0000 - val_loss: 3.0346e-04 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 6.0466e-05 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 5.9267e-04 - acc: 1.0000 - val_loss: 0.0299 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0165 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 572ms/step - loss: 6.4210e-04 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 1.9106e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 4.7285e-05 - acc: 1.0000 - val_loss: 5.4751e-04 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 4.2568e-06 - acc: 1.0000 - val_loss: 2.0871e-04 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 5.8774e-06 - acc: 1.0000 - val_loss: 9.2385e-05 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 1s 516ms/step - loss: 3.3798e-06 - acc: 1.0000 - val_loss: 4.6928e-05 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 1.6473e-06 - acc: 1.0000 - val_loss: 2.6844e-05 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.0822e-06 - acc: 1.0000 - val_loss: 1.6956e-05 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0502e-06 - acc: 1.0000 - val_loss: 1.1640e-05 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.7308e-07 - acc: 1.0000 - val_loss: 8.5274e-06 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 6.2905e-07 - acc: 1.0000 - val_loss: 6.6049e-06 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.3300e-07 - acc: 1.0000 - val_loss: 5.3434e-06 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 5.3843e-07 - acc: 1.0000 - val_loss: 4.4950e-06 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 5.5442e-07 - acc: 1.0000 - val_loss: 3.8946e-06 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 6.1839e-07 - acc: 1.0000 - val_loss: 3.4484e-06 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 5.7574e-07 - acc: 1.0000 - val_loss: 3.1014e-06 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.5037e-07 - acc: 1.0000 - val_loss: 2.8369e-06 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.7179e-07 - acc: 1.0000 - val_loss: 2.6497e-06 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.9311e-07 - acc: 1.0000 - val_loss: 2.4899e-06 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 3.7583e-07 - acc: 1.0000 - val_loss: 2.3742e-06 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.2381e-07 - acc: 1.0000 - val_loss: 2.2475e-06 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.1710e-07 - acc: 1.0000 - val_loss: 2.1814e-06 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.1581e-07 - acc: 1.0000 - val_loss: 2.1043e-06 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.7050e-07 - acc: 1.0000 - val_loss: 2.0437e-06 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.9844e-07 - acc: 1.0000 - val_loss: 1.9886e-06 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.3714e-07 - acc: 1.0000 - val_loss: 1.9115e-06 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.3705e-07 - acc: 1.0000 - val_loss: 1.9005e-06 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.8236e-07 - acc: 1.0000 - val_loss: 1.8674e-06 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.9844e-07 - acc: 1.0000 - val_loss: 1.8454e-06 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.1848e-07 - acc: 1.0000 - val_loss: 1.8234e-06 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.8374e-07 - acc: 1.0000 - val_loss: 1.8068e-06 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.4385e-07 - acc: 1.0000 - val_loss: 1.7793e-06 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.3585e-07 - acc: 1.0000 - val_loss: 1.7573e-06 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.7050e-07 - acc: 1.0000 - val_loss: 1.7407e-06 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.8916e-07 - acc: 1.0000 - val_loss: 1.7352e-06 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6655e-07 - acc: 1.0000 - val_loss: 1.7242e-06 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.9578e-07 - acc: 1.0000 - val_loss: 1.7132e-06 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.6919 - acc: 0.5484 - val_loss: 0.6869 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6860 - acc: 0.7419 - val_loss: 0.6734 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6690 - acc: 0.7097 - val_loss: 0.6512 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6447 - acc: 0.7097 - val_loss: 0.6090 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5964 - acc: 0.9032 - val_loss: 0.5298 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 976ms/step - loss: 0.4965 - acc: 1.0000 - val_loss: 0.3963 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 760ms/step - loss: 0.3878 - acc: 0.9677 - val_loss: 0.2820 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 888ms/step - loss: 0.2924 - acc: 0.8710 - val_loss: 0.1396 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1482 - acc: 0.9677 - val_loss: 0.1263 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 557ms/step - loss: 0.0999 - acc: 1.0000 - val_loss: 0.0566 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 661ms/step - loss: 0.0374 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 3.3218e-04 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 9.1298e-04 - acc: 1.0000 - val_loss: 2.9420e-04 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 3.3818e-05 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 684ms/step - loss: 3.4241e-05 - acc: 1.0000 - val_loss: 9.9100e-05 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 2.6871e-05 - acc: 1.0000 - val_loss: 6.4366e-04 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 1.1814e-04 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.2350e-04 - acc: 1.0000 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 483ms/step - loss: 9.9474e-04 - acc: 1.0000 - val_loss: 0.0131 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 5.2661e-04 - acc: 1.0000 - val_loss: 0.0103 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 2.1139e-04 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 1.4964e-04 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 1s 514ms/step - loss: 4.6888e-05 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.0071e-05 - acc: 1.0000 - val_loss: 7.3338e-04 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 1.9607e-05 - acc: 1.0000 - val_loss: 3.6271e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 6.8822e-06 - acc: 1.0000 - val_loss: 1.8862e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.2812e-06 - acc: 1.0000 - val_loss: 1.0391e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 2.2390e-06 - acc: 1.0000 - val_loss: 6.0760e-05 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 2.6042e-06 - acc: 1.0000 - val_loss: 3.7635e-05 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 2.3776e-06 - acc: 1.0000 - val_loss: 2.4591e-05 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 1.6393e-06 - acc: 1.0000 - val_loss: 1.6901e-05 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.0871e-06 - acc: 1.0000 - val_loss: 1.2136e-05 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.5593e-06 - acc: 1.0000 - val_loss: 9.0893e-06 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.5424e-07 - acc: 1.0000 - val_loss: 7.0345e-06 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.8898e-07 - acc: 1.0000 - val_loss: 5.6243e-06 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.3567e-07 - acc: 1.0000 - val_loss: 4.6107e-06 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 1.0075e-06 - acc: 1.0000 - val_loss: 3.8946e-06 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.2639e-07 - acc: 1.0000 - val_loss: 3.3162e-06 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.0902e-07 - acc: 1.0000 - val_loss: 2.9251e-06 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 6.6903e-07 - acc: 1.0000 - val_loss: 2.6111e-06 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.2767e-07 - acc: 1.0000 - val_loss: 2.3357e-06 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.9826e-07 - acc: 1.0000 - val_loss: 2.1263e-06 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.0102e-07 - acc: 1.0000 - val_loss: 1.9721e-06 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.5975e-07 - acc: 1.0000 - val_loss: 1.8289e-06 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.1435e-07 - acc: 1.0000 - val_loss: 1.7132e-06 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.9698e-07 - acc: 1.0000 - val_loss: 1.6085e-06 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 6.8236e-07 - acc: 1.0000 - val_loss: 1.5369e-06 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.8640e-07 - acc: 1.0000 - val_loss: 1.4708e-06 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.1444e-07 - acc: 1.0000 - val_loss: 1.4102e-06 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.5975e-07 - acc: 1.0000 - val_loss: 1.3661e-06 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.9302e-07 - acc: 1.0000 - val_loss: 1.3166e-06 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.5442e-07 - acc: 1.0000 - val_loss: 1.2835e-06 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.5580e-07 - acc: 1.0000 - val_loss: 1.2505e-06 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 6.0773e-07 - acc: 1.0000 - val_loss: 1.2339e-06 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.6915 - acc: 0.4839 - val_loss: 0.6872 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6828 - acc: 0.5484 - val_loss: 0.6742 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6716 - acc: 0.6129 - val_loss: 0.6519 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6469 - acc: 0.6774 - val_loss: 0.6086 - val_acc: 0.6667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5872 - acc: 0.8065 - val_loss: 0.5262 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4745 - acc: 1.0000 - val_loss: 0.3814 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3655 - acc: 1.0000 - val_loss: 0.1931 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.1858 - acc: 1.0000 - val_loss: 0.0550 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 896ms/step - loss: 0.0691 - acc: 1.0000 - val_loss: 0.1527 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.0839 - acc: 0.9677 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1301 - acc: 0.9355 - val_loss: 3.9876e-04 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 704ms/step - loss: 2.3554e-04 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 4.5557e-04 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 873ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 643ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0189 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0192 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0178 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 5.9039e-04 - acc: 1.0000 - val_loss: 0.0137 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.2427e-04 - acc: 1.0000 - val_loss: 0.0120 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 1s 624ms/step - loss: 2.3171e-04 - acc: 1.0000 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6055e-04 - acc: 1.0000 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.8351e-04 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 1s 616ms/step - loss: 1.4947e-04 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.1057e-04 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1748e-04 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 9.0663e-05 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 5.6775e-05 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.0335e-05 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 5.6439e-05 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 5.0004e-05 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 5.6002e-05 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 4.7197e-05 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.2917e-05 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.3551e-05 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.0798e-05 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.1410e-05 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.4311e-05 - acc: 1.0000 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 2.8027e-05 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 2.7241e-05 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8800e-05 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 1.9972e-05 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.0607e-05 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.7639e-05 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.9119e-05 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 2.5455e-05 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.7312e-05 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 2.2318e-05 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9712e-05 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5215e-05 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.1312e-05 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.6227e-05 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.2845e-05 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.0569e-05 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6912 - acc: 0.6452 - val_loss: 0.6828 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6869 - acc: 0.5161 - val_loss: 0.6680 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6570 - acc: 0.6452 - val_loss: 0.6366 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6322 - acc: 0.7419 - val_loss: 0.5726 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5771 - acc: 0.8387 - val_loss: 0.4532 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4105 - acc: 0.9677 - val_loss: 0.2744 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2777 - acc: 0.9677 - val_loss: 0.1228 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1295 - acc: 0.9677 - val_loss: 0.0363 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 939ms/step - loss: 0.0258 - acc: 1.0000 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.0240 - acc: 1.0000 - val_loss: 0.0112 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0062 - acc: 1.0000 - val_loss: 5.1850e-04 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 635ms/step - loss: 1.7860e-04 - acc: 1.0000 - val_loss: 2.4502e-05 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 1.9839e-05 - acc: 1.0000 - val_loss: 6.0870e-06 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 7.9698e-06 - acc: 1.0000 - val_loss: 1.1166e-05 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 1.2820e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 863ms/step - loss: 0.2294 - acc: 0.9355 - val_loss: 2.6882e-06 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.7406e-06 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.3932 - val_acc: 0.8667\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.4387 - val_acc: 0.8667\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 0.0576 - acc: 0.9677 - val_loss: 0.1022 - val_acc: 0.9333\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0190 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 8.0313e-05 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 1s 631ms/step - loss: 5.5181e-05 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 3.3302e-05 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.4521e-05 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 1.3621e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.1678e-05 - acc: 1.0000 - val_loss: 9.9695e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.2414e-05 - acc: 1.0000 - val_loss: 9.4651e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6955e-05 - acc: 1.0000 - val_loss: 9.2420e-04 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0453e-05 - acc: 1.0000 - val_loss: 9.1847e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 5.2651e-05 - acc: 1.0000 - val_loss: 9.2260e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.5263e-05 - acc: 1.0000 - val_loss: 9.3263e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.2087e-05 - acc: 1.0000 - val_loss: 9.4672e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6018e-05 - acc: 1.0000 - val_loss: 9.6337e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.2517e-05 - acc: 1.0000 - val_loss: 9.8108e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.5410e-05 - acc: 1.0000 - val_loss: 9.9877e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.4680e-05 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.7270e-05 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 4.2424e-05 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 3.7556e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 1.1568e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 4.9527e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 3.8996e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.4883e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 6.0863e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.3285e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.3882e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 6.0759e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.8795e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.2597e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.5565e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.6627e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.1065e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.6775e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.4742e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.6931 - acc: 0.4839 - val_loss: 0.6915 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6890 - acc: 0.6129 - val_loss: 0.6867 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6844 - acc: 0.5806 - val_loss: 0.6781 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6736 - acc: 0.5806 - val_loss: 0.6600 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6548 - acc: 0.6129 - val_loss: 0.6192 - val_acc: 0.6000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5955 - acc: 0.8710 - val_loss: 0.5251 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 794ms/step - loss: 0.5115 - acc: 1.0000 - val_loss: 0.3758 - val_acc: 0.9333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3926 - acc: 0.8387 - val_loss: 0.3065 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 683ms/step - loss: 0.2550 - acc: 1.0000 - val_loss: 0.1161 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1400 - acc: 0.9677 - val_loss: 0.3858 - val_acc: 0.6000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 500ms/step - loss: 0.2184 - acc: 0.9032 - val_loss: 0.0535 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0921 - acc: 0.9677 - val_loss: 0.0311 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 0.0418 - acc: 1.0000 - val_loss: 0.0125 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 962ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 490ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0383 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 802ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.0435 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0422 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 499ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.0383 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0336 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0289 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0247 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 6.4543e-04 - acc: 1.0000 - val_loss: 0.0211 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 5.3250e-04 - acc: 1.0000 - val_loss: 0.0179 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7417e-04 - acc: 1.0000 - val_loss: 0.0152 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 1.9403e-04 - acc: 1.0000 - val_loss: 0.0129 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 1.1903e-04 - acc: 1.0000 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 9.8756e-05 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.7518e-05 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 4.1733e-05 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.6397e-05 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7417e-05 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9280e-05 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.1729e-05 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.3533e-05 - acc: 1.0000 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 1.8226e-05 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.4721e-05 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.3234e-05 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.3495e-05 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 7.9831e-06 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.1014e-05 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.0819e-05 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.1256e-05 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 9.1506e-06 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.2256e-05 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.7876e-06 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.0409e-05 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1131e-05 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.5437e-06 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.5597e-06 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.8818e-06 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.5037e-06 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.0553e-05 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.0995e-05 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.1323e-06 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.6935 - acc: 0.5484 - val_loss: 0.6901 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6920 - acc: 0.5161 - val_loss: 0.6824 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6833 - acc: 0.6452 - val_loss: 0.6685 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6661 - acc: 0.7097 - val_loss: 0.6384 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6340 - acc: 0.6774 - val_loss: 0.5694 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.5486 - acc: 1.0000 - val_loss: 0.4227 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 872ms/step - loss: 0.4240 - acc: 1.0000 - val_loss: 0.3665 - val_acc: 0.7333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4137 - acc: 0.7419 - val_loss: 0.2308 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.2030 - acc: 1.0000 - val_loss: 0.3464 - val_acc: 0.8000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2868 - acc: 0.9032 - val_loss: 0.1898 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1375 - acc: 1.0000 - val_loss: 0.0583 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0372 - acc: 1.0000 - val_loss: 0.0147 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 866ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 5.6848e-04 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 2.1358e-04 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 4.4881e-04 - acc: 1.0000 - val_loss: 7.8349e-05 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 617ms/step - loss: 1.8920e-04 - acc: 1.0000 - val_loss: 2.9692e-05 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 8.1737e-05 - acc: 1.0000 - val_loss: 1.2543e-05 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 711ms/step - loss: 1.6017e-05 - acc: 1.0000 - val_loss: 6.1972e-06 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 1s 716ms/step - loss: 1.7691e-05 - acc: 1.0000 - val_loss: 3.6357e-06 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 6.5864e-06 - acc: 1.0000 - val_loss: 2.4238e-06 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.5460e-06 - acc: 1.0000 - val_loss: 1.7738e-06 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 9.1959e-07 - acc: 1.0000 - val_loss: 1.3661e-06 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 8.6361e-07 - acc: 1.0000 - val_loss: 1.1072e-06 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 5.7041e-07 - acc: 1.0000 - val_loss: 9.2545e-07 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.6379e-07 - acc: 1.0000 - val_loss: 7.9875e-07 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.2519e-07 - acc: 1.0000 - val_loss: 7.2163e-07 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 2.7721e-07 - acc: 1.0000 - val_loss: 6.3900e-07 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 3.2252e-07 - acc: 1.0000 - val_loss: 5.8942e-07 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.8254e-07 - acc: 1.0000 - val_loss: 5.5637e-07 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.1324e-07 - acc: 1.0000 - val_loss: 5.2883e-07 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.9724e-07 - acc: 1.0000 - val_loss: 4.7925e-07 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5589e-07 - acc: 1.0000 - val_loss: 4.6823e-07 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 1.7859e-07 - acc: 1.0000 - val_loss: 4.5171e-07 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.8658e-07 - acc: 1.0000 - val_loss: 4.4069e-07 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.7326e-07 - acc: 1.0000 - val_loss: 4.1866e-07 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.7859e-07 - acc: 1.0000 - val_loss: 4.1866e-07 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 1.7859e-07 - acc: 1.0000 - val_loss: 3.9111e-07 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.6526e-07 - acc: 1.0000 - val_loss: 3.9111e-07 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5726e-07 - acc: 1.0000 - val_loss: 3.8010e-07 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.7592e-07 - acc: 1.0000 - val_loss: 3.8010e-07 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.6792e-07 - acc: 1.0000 - val_loss: 3.8010e-07 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.7592e-07 - acc: 1.0000 - val_loss: 3.8010e-07 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.2528e-07 - acc: 1.0000 - val_loss: 3.7459e-07 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3860e-07 - acc: 1.0000 - val_loss: 3.6357e-07 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.5993e-07 - acc: 1.0000 - val_loss: 3.5255e-07 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.1195e-07 - acc: 1.0000 - val_loss: 3.5255e-07 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.1195e-07 - acc: 1.0000 - val_loss: 3.5255e-07 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.6259e-07 - acc: 1.0000 - val_loss: 3.5255e-07 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.3860e-07 - acc: 1.0000 - val_loss: 3.5255e-07 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.4660e-07 - acc: 1.0000 - val_loss: 3.5255e-07 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.0395e-07 - acc: 1.0000 - val_loss: 3.5255e-07 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.0662e-07 - acc: 1.0000 - val_loss: 3.5255e-07 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.4394e-07 - acc: 1.0000 - val_loss: 3.5255e-07 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4394e-07 - acc: 1.0000 - val_loss: 3.5255e-07 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6936 - acc: 0.4839 - val_loss: 0.6883 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6858 - acc: 0.7419 - val_loss: 0.6792 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6797 - acc: 0.7097 - val_loss: 0.6594 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6575 - acc: 0.7097 - val_loss: 0.6146 - val_acc: 0.8000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6134 - acc: 0.7742 - val_loss: 0.5084 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4950 - acc: 1.0000 - val_loss: 0.3526 - val_acc: 0.8667\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3652 - acc: 0.8065 - val_loss: 0.1840 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1788 - acc: 1.0000 - val_loss: 0.0742 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0642 - acc: 1.0000 - val_loss: 0.0915 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1386 - acc: 0.9355 - val_loss: 0.4612 - val_acc: 0.6667\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 0.2743 - acc: 0.8710 - val_loss: 0.0309 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 629ms/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 6.0262e-04 - acc: 1.0000 - val_loss: 7.8360e-04 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 2.9095e-04 - acc: 1.0000 - val_loss: 5.4095e-04 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3384e-04 - acc: 1.0000 - val_loss: 4.2742e-04 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 739ms/step - loss: 1.0368e-04 - acc: 1.0000 - val_loss: 3.6536e-04 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 4.8128e-05 - acc: 1.0000 - val_loss: 3.2741e-04 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 6.7567e-05 - acc: 1.0000 - val_loss: 3.0132e-04 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 468ms/step - loss: 2.8950e-05 - acc: 1.0000 - val_loss: 2.8204e-04 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.7129e-05 - acc: 1.0000 - val_loss: 2.6672e-04 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.9573e-05 - acc: 1.0000 - val_loss: 2.5409e-04 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.2152e-05 - acc: 1.0000 - val_loss: 2.4338e-04 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 1.4876e-05 - acc: 1.0000 - val_loss: 2.3419e-04 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 1.5969e-05 - acc: 1.0000 - val_loss: 2.2625e-04 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.2192e-05 - acc: 1.0000 - val_loss: 2.1930e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.1699e-05 - acc: 1.0000 - val_loss: 2.1334e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 7.8871e-06 - acc: 1.0000 - val_loss: 2.0821e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.1771e-05 - acc: 1.0000 - val_loss: 2.0380e-04 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3594e-05 - acc: 1.0000 - val_loss: 1.9996e-04 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.0102e-06 - acc: 1.0000 - val_loss: 1.9671e-04 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.5913e-06 - acc: 1.0000 - val_loss: 1.9396e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 9.6250e-06 - acc: 1.0000 - val_loss: 1.9165e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 8.7694e-06 - acc: 1.0000 - val_loss: 1.8965e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 8.3109e-06 - acc: 1.0000 - val_loss: 1.8791e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.0129e-06 - acc: 1.0000 - val_loss: 1.8652e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 5.3363e-06 - acc: 1.0000 - val_loss: 1.8533e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.5708e-06 - acc: 1.0000 - val_loss: 1.8432e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.6628e-06 - acc: 1.0000 - val_loss: 1.8342e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.0577e-06 - acc: 1.0000 - val_loss: 1.8264e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 6.1972e-06 - acc: 1.0000 - val_loss: 1.8198e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.9911e-06 - acc: 1.0000 - val_loss: 1.8135e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.8973e-06 - acc: 1.0000 - val_loss: 1.8081e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.2372e-06 - acc: 1.0000 - val_loss: 1.8034e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 4.6699e-06 - acc: 1.0000 - val_loss: 1.7987e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 5.1497e-06 - acc: 1.0000 - val_loss: 1.7950e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.9382e-06 - acc: 1.0000 - val_loss: 1.7913e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.0879e-06 - acc: 1.0000 - val_loss: 1.7879e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.1421e-06 - acc: 1.0000 - val_loss: 1.7851e-04 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.7063e-06 - acc: 1.0000 - val_loss: 1.7818e-04 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.6028e-06 - acc: 1.0000 - val_loss: 1.7784e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.1683e-06 - acc: 1.0000 - val_loss: 1.7754e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 5.4562e-06 - acc: 1.0000 - val_loss: 1.7724e-04 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.2265e-06 - acc: 1.0000 - val_loss: 1.7692e-04 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.6934e-06 - acc: 1.0000 - val_loss: 1.7660e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.6915 - acc: 0.4839 - val_loss: 0.6887 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6860 - acc: 0.5484 - val_loss: 0.6787 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6727 - acc: 0.6774 - val_loss: 0.6595 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6549 - acc: 0.6452 - val_loss: 0.6186 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6077 - acc: 0.7419 - val_loss: 0.5250 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4887 - acc: 0.9677 - val_loss: 0.3527 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3423 - acc: 0.9355 - val_loss: 0.1510 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1389 - acc: 1.0000 - val_loss: 2.3772 - val_acc: 0.5333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0242 - acc: 0.5484 - val_loss: 0.0587 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 848ms/step - loss: 0.0541 - acc: 1.0000 - val_loss: 0.1676 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 954ms/step - loss: 0.2098 - acc: 0.8710 - val_loss: 0.1475 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 890ms/step - loss: 0.1686 - acc: 0.9032 - val_loss: 0.1351 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 946ms/step - loss: 0.1203 - acc: 1.0000 - val_loss: 0.1619 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 724ms/step - loss: 0.1340 - acc: 1.0000 - val_loss: 0.1916 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.1566 - acc: 1.0000 - val_loss: 0.2081 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.1556 - acc: 1.0000 - val_loss: 0.2090 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1593 - acc: 1.0000 - val_loss: 0.1953 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 504ms/step - loss: 0.1385 - acc: 1.0000 - val_loss: 0.1707 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 0.1164 - acc: 1.0000 - val_loss: 0.1392 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 557ms/step - loss: 0.0860 - acc: 1.0000 - val_loss: 0.1060 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0609 - acc: 1.0000 - val_loss: 0.0754 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 1s 512ms/step - loss: 0.0402 - acc: 1.0000 - val_loss: 0.0504 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.0321 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.0198 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.0121 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 8.7120e-04 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 9.3914e-04 - acc: 1.0000 - val_loss: 6.8269e-04 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 6.6775e-04 - acc: 1.0000 - val_loss: 5.4608e-04 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.5998e-04 - acc: 1.0000 - val_loss: 4.4455e-04 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 5.5314e-04 - acc: 1.0000 - val_loss: 3.6744e-04 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 3.8561e-04 - acc: 1.0000 - val_loss: 3.0801e-04 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.2214e-04 - acc: 1.0000 - val_loss: 2.6157e-04 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1352e-04 - acc: 1.0000 - val_loss: 2.2470e-04 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2897e-04 - acc: 1.0000 - val_loss: 1.9499e-04 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.1951e-04 - acc: 1.0000 - val_loss: 1.7090e-04 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.7178e-04 - acc: 1.0000 - val_loss: 1.5122e-04 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.7660e-04 - acc: 1.0000 - val_loss: 1.3495e-04 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3983e-04 - acc: 1.0000 - val_loss: 1.2135e-04 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8423e-04 - acc: 1.0000 - val_loss: 1.0989e-04 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.5878e-04 - acc: 1.0000 - val_loss: 1.0014e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.6028e-04 - acc: 1.0000 - val_loss: 9.1774e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.3674e-04 - acc: 1.0000 - val_loss: 8.4580e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 9.5333e-05 - acc: 1.0000 - val_loss: 7.8366e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.0862e-05 - acc: 1.0000 - val_loss: 7.2973e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.5582e-05 - acc: 1.0000 - val_loss: 6.8280e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.0300e-04 - acc: 1.0000 - val_loss: 6.4148e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.2583e-05 - acc: 1.0000 - val_loss: 6.0529e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 6.4496e-05 - acc: 1.0000 - val_loss: 5.7306e-05 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.9697e-05 - acc: 1.0000 - val_loss: 5.4458e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.6936 - acc: 0.5161 - val_loss: 0.6866 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6866 - acc: 0.7097 - val_loss: 0.6719 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6692 - acc: 0.6129 - val_loss: 0.6394 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6353 - acc: 0.7097 - val_loss: 0.5665 - val_acc: 0.8667\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5532 - acc: 0.8065 - val_loss: 0.4065 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3680 - acc: 1.0000 - val_loss: 0.2154 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2269 - acc: 0.9355 - val_loss: 0.1211 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0934 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0106 - acc: 1.0000 - val_loss: 1.4688e-04 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 895ms/step - loss: 2.8643e-04 - acc: 1.0000 - val_loss: 8.9135e-05 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 619ms/step - loss: 3.8745e-05 - acc: 1.0000 - val_loss: 3.0477e-04 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5800e-04 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.0164 - acc: 1.0000 - val_loss: 6.2798e-06 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0038 - acc: 1.0000 - val_loss: 2.7708e-06 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2756e-04 - acc: 1.0000 - val_loss: 2.1484e-07 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 2.8254e-07 - acc: 1.0000 - val_loss: 4.9578e-07 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 1.3860e-07 - acc: 1.0000 - val_loss: 1.1623e-05 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.8512e-07 - acc: 1.0000 - val_loss: 2.1068e-04 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 981ms/step - loss: 1.2021e-06 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 1s 779ms/step - loss: 2.0364e-06 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 5.8054e-06 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.9444e-06 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 5.5202e-06 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 9.7983e-06 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.4039e-05 - acc: 1.0000 - val_loss: 8.9008e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 3.2641e-05 - acc: 1.0000 - val_loss: 8.7214e-04 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 1.7808e-05 - acc: 1.0000 - val_loss: 9.6915e-04 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 9.5072e-05 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 2.3179e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.9117e-05 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 2.6582e-04 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.3866e-05 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 9.4936e-05 - acc: 1.0000 - val_loss: 0.0105 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6354e-04 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.4028e-04 - acc: 1.0000 - val_loss: 0.0178 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.9720e-04 - acc: 1.0000 - val_loss: 0.0187 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.4343e-04 - acc: 1.0000 - val_loss: 0.0185 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.6370e-05 - acc: 1.0000 - val_loss: 0.0180 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.8912e-05 - acc: 1.0000 - val_loss: 0.0167 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 2.8014e-05 - acc: 1.0000 - val_loss: 0.0153 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7763e-05 - acc: 1.0000 - val_loss: 0.0140 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 3.9172e-05 - acc: 1.0000 - val_loss: 0.0125 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.3765e-05 - acc: 1.0000 - val_loss: 0.0112 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.4844e-05 - acc: 1.0000 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.2000e-05 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.2416e-05 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.5819e-06 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.3511e-05 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 9.8516e-06 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.4753e-06 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.2514e-06 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 9.8276e-06 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.2834e-06 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.3229e-06 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.6956 - acc: 0.4516 - val_loss: 0.6854 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6805 - acc: 0.5806 - val_loss: 0.6687 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6656 - acc: 0.5806 - val_loss: 0.6285 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6159 - acc: 0.7742 - val_loss: 0.5292 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5125 - acc: 0.9677 - val_loss: 0.3475 - val_acc: 0.9333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3284 - acc: 0.9355 - val_loss: 0.1423 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1507 - acc: 1.0000 - val_loss: 0.4242 - val_acc: 0.6667\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3005 - acc: 0.8387 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.0694 - val_acc: 0.9333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1150 - acc: 0.9355 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 986ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0083 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0188 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0222 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 770ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0245 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0255 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0255 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0248 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0237 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 8.2095e-04 - acc: 1.0000 - val_loss: 0.0224 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 5.4654e-04 - acc: 1.0000 - val_loss: 0.0210 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 4.7830e-04 - acc: 1.0000 - val_loss: 0.0196 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 1s 924ms/step - loss: 3.3489e-04 - acc: 1.0000 - val_loss: 0.0182 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 2.8435e-04 - acc: 1.0000 - val_loss: 0.0169 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1875e-04 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 1.1947e-04 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 8.3267e-05 - acc: 1.0000 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.1711e-04 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 1.1045e-04 - acc: 1.0000 - val_loss: 0.0119 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.4105e-05 - acc: 1.0000 - val_loss: 0.0112 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 3.8106e-05 - acc: 1.0000 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4.8693e-05 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 1s 578ms/step - loss: 3.1874e-05 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.6000e-05 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.3797e-05 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.7832e-05 - acc: 1.0000 - val_loss: 0.0083 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.6449e-05 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.7040e-05 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5644e-05 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.5145e-05 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.2957e-05 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.6315e-05 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.1364e-05 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 1.6273e-05 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.0163e-05 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.0107e-05 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 9.6783e-06 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.1936e-05 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.2832e-05 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.6974e-06 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 9.3905e-06 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.4932e-05 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.9613e-06 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.6933 - acc: 0.4839 - val_loss: 0.6867 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6851 - acc: 0.5484 - val_loss: 0.6706 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6675 - acc: 0.6129 - val_loss: 0.6396 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6419 - acc: 0.6452 - val_loss: 0.5646 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5599 - acc: 0.9032 - val_loss: 0.4048 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3894 - acc: 1.0000 - val_loss: 0.3036 - val_acc: 0.8667\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3435 - acc: 0.7742 - val_loss: 0.1823 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1585 - acc: 1.0000 - val_loss: 0.2283 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1571 - acc: 1.0000 - val_loss: 0.0851 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 864ms/step - loss: 0.0510 - acc: 1.0000 - val_loss: 0.0153 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 5.4977e-04 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5665e-04 - acc: 1.0000 - val_loss: 1.7716e-04 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 624ms/step - loss: 1.2991e-04 - acc: 1.0000 - val_loss: 6.5635e-05 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 7.5974e-05 - acc: 1.0000 - val_loss: 2.4491e-05 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 820ms/step - loss: 2.6409e-05 - acc: 1.0000 - val_loss: 9.0342e-06 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 968ms/step - loss: 8.6068e-06 - acc: 1.0000 - val_loss: 3.3768e-06 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9858e-06 - acc: 1.0000 - val_loss: 1.3496e-06 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.3007e-06 - acc: 1.0000 - val_loss: 6.0044e-07 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 695ms/step - loss: 5.0111e-07 - acc: 1.0000 - val_loss: 3.2501e-07 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 4.6379e-07 - acc: 1.0000 - val_loss: 1.7628e-07 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 2.7454e-07 - acc: 1.0000 - val_loss: 7.1612e-08 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 1.0129e-07 - acc: 1.0000 - val_loss: 4.9578e-08 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 1s 761ms/step - loss: 7.9964e-08 - acc: 1.0000 - val_loss: 1.6526e-08 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.0644e-08 - acc: 1.0000 - val_loss: 1.1017e-08 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.9982e-08 - acc: 1.0000 - val_loss: 1.1017e-08 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.6655e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6655e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.0662e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5993e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 5.3309e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 5.3309e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 5.3309e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.3309e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 17s 17s/step - loss: 0.6956 - acc: 0.4516 - val_loss: 0.6874 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6854 - acc: 0.6129 - val_loss: 0.6759 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6735 - acc: 0.7097 - val_loss: 0.6492 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6458 - acc: 0.7097 - val_loss: 0.5854 - val_acc: 0.9333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5752 - acc: 0.8387 - val_loss: 0.4385 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3971 - acc: 1.0000 - val_loss: 0.2929 - val_acc: 0.8667\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3075 - acc: 0.8065 - val_loss: 0.1769 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1542 - acc: 1.0000 - val_loss: 0.1026 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0624 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 488ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 570ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 943ms/step - loss: 0.0198 - acc: 1.0000 - val_loss: 4.1414e-05 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 4.6118e-05 - acc: 1.0000 - val_loss: 7.2350e-05 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 4.4295e-05 - acc: 1.0000 - val_loss: 1.7024e-04 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.2939e-05 - acc: 1.0000 - val_loss: 2.8397e-04 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 970ms/step - loss: 4.1968e-05 - acc: 1.0000 - val_loss: 3.2693e-04 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 862ms/step - loss: 8.9187e-05 - acc: 1.0000 - val_loss: 2.7223e-04 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 4.4175e-05 - acc: 1.0000 - val_loss: 1.8004e-04 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 754ms/step - loss: 3.5603e-05 - acc: 1.0000 - val_loss: 1.0322e-04 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 577ms/step - loss: 2.0172e-05 - acc: 1.0000 - val_loss: 5.5224e-05 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 1s 735ms/step - loss: 5.7787e-06 - acc: 1.0000 - val_loss: 2.9119e-05 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.8623e-06 - acc: 1.0000 - val_loss: 1.5661e-05 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 8.7161e-07 - acc: 1.0000 - val_loss: 8.7753e-06 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 1s 515ms/step - loss: 6.3172e-07 - acc: 1.0000 - val_loss: 5.1671e-06 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 9.3025e-07 - acc: 1.0000 - val_loss: 3.2005e-06 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 3.2252e-07 - acc: 1.0000 - val_loss: 2.0933e-06 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 1s 596ms/step - loss: 4.1048e-07 - acc: 1.0000 - val_loss: 1.4433e-06 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 1.9724e-07 - acc: 1.0000 - val_loss: 1.0301e-06 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1.2794e-07 - acc: 1.0000 - val_loss: 7.5468e-07 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.1195e-07 - acc: 1.0000 - val_loss: 5.7841e-07 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0928e-07 - acc: 1.0000 - val_loss: 4.6273e-07 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.0644e-08 - acc: 1.0000 - val_loss: 3.8010e-07 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.3309e-08 - acc: 1.0000 - val_loss: 3.0848e-07 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 9.0626e-08 - acc: 1.0000 - val_loss: 2.7543e-07 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 5.0644e-08 - acc: 1.0000 - val_loss: 2.4238e-07 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4.5313e-08 - acc: 1.0000 - val_loss: 2.1484e-07 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9320e-08 - acc: 1.0000 - val_loss: 1.9831e-07 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9320e-08 - acc: 1.0000 - val_loss: 1.6526e-07 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 5.0644e-08 - acc: 1.0000 - val_loss: 1.5975e-07 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 3.9982e-08 - acc: 1.0000 - val_loss: 1.3772e-07 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5993e-08 - acc: 1.0000 - val_loss: 1.2670e-07 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.5313e-08 - acc: 1.0000 - val_loss: 1.2670e-07 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.8640e-08 - acc: 1.0000 - val_loss: 1.2119e-07 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6655e-08 - acc: 1.0000 - val_loss: 1.1017e-07 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6655e-08 - acc: 1.0000 - val_loss: 9.9156e-08 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5993e-08 - acc: 1.0000 - val_loss: 9.9156e-08 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1986e-08 - acc: 1.0000 - val_loss: 9.9156e-08 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 5.3309e-09 - acc: 1.0000 - val_loss: 9.3647e-08 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3327e-08 - acc: 1.0000 - val_loss: 9.3647e-08 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.4651e-08 - acc: 1.0000 - val_loss: 9.3647e-08 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6655e-08 - acc: 1.0000 - val_loss: 9.3647e-08 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.3327e-08 - acc: 1.0000 - val_loss: 9.3647e-08 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.1986e-08 - acc: 1.0000 - val_loss: 9.3647e-08 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.3989e-08 - acc: 1.0000 - val_loss: 9.3647e-08 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9320e-08 - acc: 1.0000 - val_loss: 9.3647e-08 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 17s 17s/step - loss: 0.6906 - acc: 0.5484 - val_loss: 0.6861 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6799 - acc: 0.5484 - val_loss: 0.6591 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6570 - acc: 0.7097 - val_loss: 0.6012 - val_acc: 0.7333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.5917 - acc: 0.8387 - val_loss: 0.4655 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4564 - acc: 0.9677 - val_loss: 0.2516 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2347 - acc: 1.0000 - val_loss: 0.0949 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0727 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.8402e-04 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0023 - acc: 1.0000 - val_loss: 3.4714 - val_acc: 0.6000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9134 - acc: 0.6774 - val_loss: 3.3933 - val_acc: 0.6667\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.2028 - acc: 0.7419 - val_loss: 1.2163 - val_acc: 0.7333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 929ms/step - loss: 1.4359 - acc: 0.7419 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0108 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0582 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 578ms/step - loss: 0.0248 - acc: 1.0000 - val_loss: 0.1277 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0656 - acc: 1.0000 - val_loss: 0.1866 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 0.1169 - acc: 1.0000 - val_loss: 0.2205 - val_acc: 0.9333\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.1418 - acc: 1.0000 - val_loss: 0.2325 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1621 - acc: 1.0000 - val_loss: 0.2286 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 779ms/step - loss: 0.1593 - acc: 1.0000 - val_loss: 0.2149 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1473 - acc: 1.0000 - val_loss: 0.1950 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1314 - acc: 1.0000 - val_loss: 0.1716 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1186 - acc: 1.0000 - val_loss: 0.1467 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 1s 592ms/step - loss: 0.1001 - acc: 1.0000 - val_loss: 0.1222 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0797 - acc: 1.0000 - val_loss: 0.0992 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0624 - acc: 1.0000 - val_loss: 0.0791 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.0504 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0330 - acc: 1.0000 - val_loss: 0.0504 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0235 - acc: 1.0000 - val_loss: 0.0406 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.0328 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0264 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0213 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0089 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.2938e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.1266e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.7040e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.2508e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.3970e-04 - acc: 1.0000 - val_loss: 9.1988e-04 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.2527e-04 - acc: 1.0000 - val_loss: 7.3842e-04 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.2980e-04 - acc: 1.0000 - val_loss: 5.9757e-04 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.1770e-04 - acc: 1.0000 - val_loss: 4.8715e-04 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.1196e-04 - acc: 1.0000 - val_loss: 4.0065e-04 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.0999e-04 - acc: 1.0000 - val_loss: 3.3404e-04 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.5515e-04 - acc: 1.0000 - val_loss: 2.8293e-04 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.2585e-04 - acc: 1.0000 - val_loss: 2.4349e-04 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3039e-04 - acc: 1.0000 - val_loss: 2.1322e-04 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "[Tulvae:] Creating a model to test set\n",
      "[Tulvae:] Evaluation Config - bilstm-100-1-0.5-100-100-64-1000-20-val_acc-0.001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb39ff23e85c4cb595f8979e8341e422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Testing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.6920 - acc: 0.5161 - val_loss: 0.6921 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6892 - acc: 0.6774 - val_loss: 0.6904 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6862 - acc: 0.5806 - val_loss: 0.6870 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6806 - acc: 0.5806 - val_loss: 0.6826 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6790 - acc: 0.5806 - val_loss: 0.6764 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6518 - acc: 0.5806 - val_loss: 0.6674 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6628 - acc: 0.5806 - val_loss: 0.6536 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6308 - acc: 0.6129 - val_loss: 0.6326 - val_acc: 0.5333\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6147 - acc: 0.6452 - val_loss: 0.6001 - val_acc: 0.5333\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5932 - acc: 0.7419 - val_loss: 0.5505 - val_acc: 0.9333\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5214 - acc: 0.9677 - val_loss: 0.4792 - val_acc: 0.9333\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4437 - acc: 0.9677 - val_loss: 0.3827 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3385 - acc: 1.0000 - val_loss: 0.2705 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 793ms/step - loss: 0.2315 - acc: 1.0000 - val_loss: 0.1707 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.1523 - acc: 1.0000 - val_loss: 0.0919 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0787 - acc: 1.0000 - val_loss: 0.0412 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.0306 - acc: 1.0000 - val_loss: 0.0375 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 870ms/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.0622 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 437ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 948ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.0130 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 7.6688e-04 - acc: 1.0000 - val_loss: 4.0687e-04 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 4.7492e-04 - acc: 1.0000 - val_loss: 1.9969e-04 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6830e-04 - acc: 1.0000 - val_loss: 2.0183e-04 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 1.0281e-04 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.6912e-04 - acc: 1.0000 - val_loss: 3.9705e-04 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.0786e-04 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 0.9333\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.7569e-04 - acc: 1.0000 - val_loss: 1.9509e-04 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 6.7985e-05 - acc: 1.0000 - val_loss: 7.0571e-05 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 6.1250e-05 - acc: 1.0000 - val_loss: 4.6041e-05 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.7786e-05 - acc: 1.0000 - val_loss: 3.6908e-05 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.1507e-05 - acc: 1.0000 - val_loss: 3.2314e-05 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.6369e-05 - acc: 1.0000 - val_loss: 2.9521e-05 - val_acc: 1.0000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.1963e-05 - acc: 1.0000 - val_loss: 2.7637e-05 - val_acc: 1.0000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.4233e-05 - acc: 1.0000 - val_loss: 2.6144e-05 - val_acc: 1.0000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.5914e-05 - acc: 1.0000 - val_loss: 2.4921e-05 - val_acc: 1.0000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.4093e-05 - acc: 1.0000 - val_loss: 2.3847e-05 - val_acc: 1.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.8281e-05 - acc: 1.0000 - val_loss: 2.2899e-05 - val_acc: 1.0000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.7389e-05 - acc: 1.0000 - val_loss: 2.2035e-05 - val_acc: 1.0000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.9654e-05 - acc: 1.0000 - val_loss: 2.1214e-05 - val_acc: 1.0000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.5139e-05 - acc: 1.0000 - val_loss: 2.0448e-05 - val_acc: 1.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.7151e-05 - acc: 1.0000 - val_loss: 1.9710e-05 - val_acc: 1.0000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2780e-05 - acc: 1.0000 - val_loss: 1.9021e-05 - val_acc: 1.0000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9144e-05 - acc: 1.0000 - val_loss: 1.8360e-05 - val_acc: 1.0000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8720e-05 - acc: 1.0000 - val_loss: 1.7782e-05 - val_acc: 1.0000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.6337e-05 - acc: 1.0000 - val_loss: 1.7165e-05 - val_acc: 1.0000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.5641e-05 - acc: 1.0000 - val_loss: 1.6575e-05 - val_acc: 1.0000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6167e-05 - acc: 1.0000 - val_loss: 1.6003e-05 - val_acc: 1.0000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8214e-05 - acc: 1.0000 - val_loss: 1.5474e-05 - val_acc: 1.0000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.8695e-05 - acc: 1.0000 - val_loss: 1.4939e-05 - val_acc: 1.0000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.3833e-05 - acc: 1.0000 - val_loss: 1.4427e-05 - val_acc: 1.0000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6711e-05 - acc: 1.0000 - val_loss: 1.3948e-05 - val_acc: 1.0000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.8617e-05 - acc: 1.0000 - val_loss: 1.3491e-05 - val_acc: 1.0000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.7343e-05 - acc: 1.0000 - val_loss: 1.3039e-05 - val_acc: 1.0000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5327e-05 - acc: 1.0000 - val_loss: 1.2609e-05 - val_acc: 1.0000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8760e-05 - acc: 1.0000 - val_loss: 1.2202e-05 - val_acc: 1.0000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.3867e-05 - acc: 1.0000 - val_loss: 1.1805e-05 - val_acc: 1.0000\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9568e-05 - acc: 1.0000 - val_loss: 1.1474e-05 - val_acc: 1.0000\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.0135e-05 - acc: 1.0000 - val_loss: 1.1111e-05 - val_acc: 1.0000\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.1801e-05 - acc: 1.0000 - val_loss: 1.0802e-05 - val_acc: 1.0000\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7217e-05 - acc: 1.0000 - val_loss: 1.0483e-05 - val_acc: 1.0000\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "[Tulvae:] Processing time: 1200624.949 milliseconds. Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       " 0       1.0            1.0                1.0              1.0           1.0   \n",
       " \n",
       "    f1_macro      clstime  \n",
       " 0       1.0  1200623.688  ,\n",
       " array([[0.52313113, 0.47686893],\n",
       "        [0.5784474 , 0.42155263],\n",
       "        [0.5728779 , 0.42712215],\n",
       "        [0.59499395, 0.4050061 ],\n",
       "        [0.5869492 , 0.41305083],\n",
       "        [0.5763522 , 0.42364785],\n",
       "        [0.59864753, 0.4013525 ],\n",
       "        [0.31533653, 0.6846635 ],\n",
       "        [0.36025256, 0.6397475 ],\n",
       "        [0.3657776 , 0.6342224 ],\n",
       "        [0.3062105 , 0.69378954],\n",
       "        [0.33576033, 0.66423965],\n",
       "        [0.37066722, 0.62933284],\n",
       "        [0.37626898, 0.6237311 ],\n",
       "        [0.3960342 , 0.60396576]], dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matclassification.methods import Tulvae\n",
    "\n",
    "model = Tulvae()\n",
    "model.prepare_input(train, test, features=['poi'])\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>clstime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1200623.688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0       1.0            1.0                1.0              1.0           1.0   \n",
       "\n",
       "   f1_macro      clstime  \n",
       "0       1.0  1200623.688  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.2. Movelet Based Methods\n",
    "\n",
    "Movelet-based methods, for classifiers that previously use a movelet extraction method.\n",
    "\n",
    "Let's start by extracting movelets from the train and test data, so we save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CSV: sample/data/FoursquareNYC/train.csv\n",
      "Done.\n",
      " --------------------------------------------------------------------------------\n",
      "Writing CSV: sample/data/FoursquareNYC/test.csv\n",
      "Done.\n",
      " --------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>label</th>\n",
       "      <th>space</th>\n",
       "      <th>time</th>\n",
       "      <th>day</th>\n",
       "      <th>poi</th>\n",
       "      <th>type</th>\n",
       "      <th>root_type</th>\n",
       "      <th>rating</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11035</td>\n",
       "      <td>390</td>\n",
       "      <td>40.7024350000000 -73.9870730000000</td>\n",
       "      <td>1082</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>reBar</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Nightlife Spot</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11035</td>\n",
       "      <td>390</td>\n",
       "      <td>40.7509455224884 -74.0056339217607</td>\n",
       "      <td>449</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Harry Fox Agency</td>\n",
       "      <td>Office</td>\n",
       "      <td>Professional &amp; Other Places</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11035</td>\n",
       "      <td>390</td>\n",
       "      <td>40.7509455224884 -74.0056339217607</td>\n",
       "      <td>480</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Harry Fox Agency</td>\n",
       "      <td>Office</td>\n",
       "      <td>Professional &amp; Other Places</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11035</td>\n",
       "      <td>390</td>\n",
       "      <td>40.7452086435588 -74.0023612976074</td>\n",
       "      <td>620</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Billy's Bakery</td>\n",
       "      <td>Cupcake Shop</td>\n",
       "      <td>Food</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11035</td>\n",
       "      <td>390</td>\n",
       "      <td>40.7504159978144 -74.0030630705807</td>\n",
       "      <td>812</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>High Line</td>\n",
       "      <td>Park</td>\n",
       "      <td>Outdoors &amp; Recreation</td>\n",
       "      <td>9.7</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>24455</td>\n",
       "      <td>881</td>\n",
       "      <td>40.7478827722694 -73.9871379446347</td>\n",
       "      <td>297</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Space Billiards</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Nightlife Spot</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>24455</td>\n",
       "      <td>881</td>\n",
       "      <td>40.8340978041072 -73.9452672225881</td>\n",
       "      <td>466</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Galaxy Gourmet Deli</td>\n",
       "      <td>Deli / Bodega</td>\n",
       "      <td>Food</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>24455</td>\n",
       "      <td>881</td>\n",
       "      <td>40.8079950658704 -73.9638555049896</td>\n",
       "      <td>1371</td>\n",
       "      <td>Friday</td>\n",
       "      <td>MTA Bus Stop - 116th &amp; Broadway (M4/M60/M104)</td>\n",
       "      <td>Bus Station</td>\n",
       "      <td>Travel &amp; Transport</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>24455</td>\n",
       "      <td>881</td>\n",
       "      <td>40.8331652006224 -73.9418603427692</td>\n",
       "      <td>349</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Wayne Manor ( Bat Cave )</td>\n",
       "      <td>Home (private)</td>\n",
       "      <td>Residence</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>24455</td>\n",
       "      <td>881</td>\n",
       "      <td>40.8079950658704 -73.9638555049896</td>\n",
       "      <td>1418</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>MTA Bus Stop - 116th &amp; Broadway (M4/M60/M104)</td>\n",
       "      <td>Bus Station</td>\n",
       "      <td>Travel &amp; Transport</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tid  label                               space  time        day  \\\n",
       "0    11035    390  40.7024350000000 -73.9870730000000  1082    Tuesday   \n",
       "1    11035    390  40.7509455224884 -74.0056339217607   449  Wednesday   \n",
       "2    11035    390  40.7509455224884 -74.0056339217607   480   Thursday   \n",
       "3    11035    390  40.7452086435588 -74.0023612976074   620   Thursday   \n",
       "4    11035    390  40.7504159978144 -74.0030630705807   812   Thursday   \n",
       "..     ...    ...                                 ...   ...        ...   \n",
       "156  24455    881  40.7478827722694 -73.9871379446347   297     Friday   \n",
       "157  24455    881  40.8340978041072 -73.9452672225881   466     Friday   \n",
       "158  24455    881  40.8079950658704 -73.9638555049896  1371     Friday   \n",
       "159  24455    881  40.8331652006224 -73.9418603427692   349   Saturday   \n",
       "160  24455    881  40.8079950658704 -73.9638555049896  1418   Saturday   \n",
       "\n",
       "                                               poi            type  \\\n",
       "0                                            reBar             Bar   \n",
       "1                                 Harry Fox Agency          Office   \n",
       "2                                 Harry Fox Agency          Office   \n",
       "3                                   Billy's Bakery    Cupcake Shop   \n",
       "4                                        High Line            Park   \n",
       "..                                             ...             ...   \n",
       "156                                Space Billiards             Bar   \n",
       "157                            Galaxy Gourmet Deli   Deli / Bodega   \n",
       "158  MTA Bus Stop - 116th & Broadway (M4/M60/M104)     Bus Station   \n",
       "159                       Wayne Manor ( Bat Cave )  Home (private)   \n",
       "160  MTA Bus Stop - 116th & Broadway (M4/M60/M104)     Bus Station   \n",
       "\n",
       "                       root_type  rating weather  \n",
       "0                 Nightlife Spot    -1.0  Clouds  \n",
       "1    Professional & Other Places    -1.0  Clouds  \n",
       "2    Professional & Other Places    -1.0  Clouds  \n",
       "3                           Food     9.0  Clouds  \n",
       "4          Outdoors & Recreation     9.7  Clouds  \n",
       "..                           ...     ...     ...  \n",
       "156               Nightlife Spot     6.2   Clear  \n",
       "157                         Food     8.2  Clouds  \n",
       "158           Travel & Transport    -1.0    Rain  \n",
       "159                    Residence    -1.0  Clouds  \n",
       "160           Travel & Transport    -1.0  Clouds  \n",
       "\n",
       "[250 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matdata.converter import df2csv\n",
    "\n",
    "data_path = 'sample/data/FoursquareNYC'\n",
    "\n",
    "df2csv(train, data_path, 'train')\n",
    "df2csv(test, data_path, 'test')\n",
    "#test.to_csv('sample/data/FoursquareNYC/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Parquet: sample/data/FoursquareNYC/train.parquet\n",
      "Done.\n",
      " --------------------------------------------------------------------------------\n",
      "Writing Parquet: sample/data/FoursquareNYC/test.parquet\n",
      "Done.\n",
      " --------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>label</th>\n",
       "      <th>space</th>\n",
       "      <th>time</th>\n",
       "      <th>day</th>\n",
       "      <th>poi</th>\n",
       "      <th>type</th>\n",
       "      <th>root_type</th>\n",
       "      <th>rating</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11035</td>\n",
       "      <td>390</td>\n",
       "      <td>40.7024350000000 -73.9870730000000</td>\n",
       "      <td>1082</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>reBar</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Nightlife Spot</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11035</td>\n",
       "      <td>390</td>\n",
       "      <td>40.7509455224884 -74.0056339217607</td>\n",
       "      <td>449</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Harry Fox Agency</td>\n",
       "      <td>Office</td>\n",
       "      <td>Professional &amp; Other Places</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11035</td>\n",
       "      <td>390</td>\n",
       "      <td>40.7509455224884 -74.0056339217607</td>\n",
       "      <td>480</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Harry Fox Agency</td>\n",
       "      <td>Office</td>\n",
       "      <td>Professional &amp; Other Places</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11035</td>\n",
       "      <td>390</td>\n",
       "      <td>40.7452086435588 -74.0023612976074</td>\n",
       "      <td>620</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Billy's Bakery</td>\n",
       "      <td>Cupcake Shop</td>\n",
       "      <td>Food</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11035</td>\n",
       "      <td>390</td>\n",
       "      <td>40.7504159978144 -74.0030630705807</td>\n",
       "      <td>812</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>High Line</td>\n",
       "      <td>Park</td>\n",
       "      <td>Outdoors &amp; Recreation</td>\n",
       "      <td>9.7</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>24455</td>\n",
       "      <td>881</td>\n",
       "      <td>40.7478827722694 -73.9871379446347</td>\n",
       "      <td>297</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Space Billiards</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Nightlife Spot</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>24455</td>\n",
       "      <td>881</td>\n",
       "      <td>40.8340978041072 -73.9452672225881</td>\n",
       "      <td>466</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Galaxy Gourmet Deli</td>\n",
       "      <td>Deli / Bodega</td>\n",
       "      <td>Food</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>24455</td>\n",
       "      <td>881</td>\n",
       "      <td>40.8079950658704 -73.9638555049896</td>\n",
       "      <td>1371</td>\n",
       "      <td>Friday</td>\n",
       "      <td>MTA Bus Stop - 116th &amp; Broadway (M4/M60/M104)</td>\n",
       "      <td>Bus Station</td>\n",
       "      <td>Travel &amp; Transport</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>24455</td>\n",
       "      <td>881</td>\n",
       "      <td>40.8331652006224 -73.9418603427692</td>\n",
       "      <td>349</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Wayne Manor ( Bat Cave )</td>\n",
       "      <td>Home (private)</td>\n",
       "      <td>Residence</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>24455</td>\n",
       "      <td>881</td>\n",
       "      <td>40.8079950658704 -73.9638555049896</td>\n",
       "      <td>1418</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>MTA Bus Stop - 116th &amp; Broadway (M4/M60/M104)</td>\n",
       "      <td>Bus Station</td>\n",
       "      <td>Travel &amp; Transport</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tid  label                               space  time        day  \\\n",
       "0    11035    390  40.7024350000000 -73.9870730000000  1082    Tuesday   \n",
       "1    11035    390  40.7509455224884 -74.0056339217607   449  Wednesday   \n",
       "2    11035    390  40.7509455224884 -74.0056339217607   480   Thursday   \n",
       "3    11035    390  40.7452086435588 -74.0023612976074   620   Thursday   \n",
       "4    11035    390  40.7504159978144 -74.0030630705807   812   Thursday   \n",
       "..     ...    ...                                 ...   ...        ...   \n",
       "156  24455    881  40.7478827722694 -73.9871379446347   297     Friday   \n",
       "157  24455    881  40.8340978041072 -73.9452672225881   466     Friday   \n",
       "158  24455    881  40.8079950658704 -73.9638555049896  1371     Friday   \n",
       "159  24455    881  40.8331652006224 -73.9418603427692   349   Saturday   \n",
       "160  24455    881  40.8079950658704 -73.9638555049896  1418   Saturday   \n",
       "\n",
       "                                               poi            type  \\\n",
       "0                                            reBar             Bar   \n",
       "1                                 Harry Fox Agency          Office   \n",
       "2                                 Harry Fox Agency          Office   \n",
       "3                                   Billy's Bakery    Cupcake Shop   \n",
       "4                                        High Line            Park   \n",
       "..                                             ...             ...   \n",
       "156                                Space Billiards             Bar   \n",
       "157                            Galaxy Gourmet Deli   Deli / Bodega   \n",
       "158  MTA Bus Stop - 116th & Broadway (M4/M60/M104)     Bus Station   \n",
       "159                       Wayne Manor ( Bat Cave )  Home (private)   \n",
       "160  MTA Bus Stop - 116th & Broadway (M4/M60/M104)     Bus Station   \n",
       "\n",
       "                       root_type  rating weather  \n",
       "0                 Nightlife Spot    -1.0  Clouds  \n",
       "1    Professional & Other Places    -1.0  Clouds  \n",
       "2    Professional & Other Places    -1.0  Clouds  \n",
       "3                           Food     9.0  Clouds  \n",
       "4          Outdoors & Recreation     9.7  Clouds  \n",
       "..                           ...     ...     ...  \n",
       "156               Nightlife Spot     6.2   Clear  \n",
       "157                         Food     8.2  Clouds  \n",
       "158           Travel & Transport    -1.0    Rain  \n",
       "159                    Residence    -1.0  Clouds  \n",
       "160           Travel & Transport    -1.0  Clouds  \n",
       "\n",
       "[250 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matdata.converter import df2parquet\n",
    "\n",
    "data_path = 'sample/data/FoursquareNYC'\n",
    "\n",
    "df2parquet(train, data_path, 'train')\n",
    "df2parquet(test, data_path, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) First, we can apply a method for movelets extraction like [HiPerMovelets](https://github.com/bigdata-ufsc/HiPerMovelets), for example configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 77.1M  100 77.1M    0     0  4043k      0  0:00:19  0:00:19 --:--:-- 6455k\n"
     ]
    }
   ],
   "source": [
    "# To download the excutable:\n",
    "!curl -o sample/programs/MoveletDiscovery.jar https://raw.githubusercontent.com/mat-analysis/mat-tools/main/jarfiles/MoveletDiscovery.jar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 30 23:04:52 BRT 2024\n",
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "Starting Movelets +Log extractor \n",
      "Configurations:\n",
      "   -curpath\t\tDatasets directory:\t./sample/data/FoursquareNYC\n",
      "   -respath\t\tResults directory: \t./sample/results/hiper/Movelets/HIPER_Log_FoursquareNYC_LSP_ED\n",
      "   -descfile \t\tDescription file : \t./sample/data/FoursquareNYC/FoursquareNYC.json\n",
      "+-------------+--------------------+---------------------+----------------------------------------------+\n",
      "| Option      | Description        | Value               | Help                                         |\n",
      "+-------------+--------------------+---------------------+----------------------------------------------+\n",
      "| -nt         | Allowed Threads    | 1                   |                                              |\n",
      "| -ms         | Min size           | -1                  | Any positive | -1 | Log: -2                  |\n",
      "| -Ms         | Max size           | -3                  | Any | All sizes: -1 | Log: -3 or -4          |\n",
      "| -mnf        | Max. Dimensions    | -1                  | Any | Explore dim.: -1 | Log: -2 | Other: -3 |\n",
      "| -samples    | Samples            | 1                   |                                              |\n",
      "| -sampleSize | Sample Size        | 0.5                 |                                              |\n",
      "| -q          | Quality Measure    | LSP                 |                                              |\n",
      "| -medium     | Medium             | none                |                                              |\n",
      "| -mpt        | Movelets Per Traj. | -1                  | Any | Auto: -1                               |\n",
      "| -output     | Output             | discrete (CSV,JSON) |                                              |\n",
      "|             |                    |                     |                                              |\n",
      "| -version    | Version Impl.      | hiper               | master, super, hiper[-pivots], random, ultra |\n",
      "|             | -- Last Prunning   | false               |                                              |\n",
      "| -TC         | Time Contract      | 86400000 ms         | Use: w(eeks), d, h, m, s(econds)             |\n",
      "+-------------+--------------------+---------------------+----------------------------------------------+\n",
      "\n",
      "[1] >> Load Input: 3138 milliseconds\n",
      "\n",
      "Attributes and Features:\n",
      "+---+---------------+---------+-----------------+\n",
      "| # | Attribute     | Type    | Comparator      |\n",
      "+---+---------------+---------+-----------------+\n",
      "| 1 | 1 - space     | space2d | euclidean/-1.0  |\n",
      "| 2 | 2 - time      | time    | difference/-1.0 |\n",
      "| 3 | 3 - day       | nominal | equals/-1.0     |\n",
      "| 4 | 4 - poi       | nominal | equals/-1.0     |\n",
      "| 5 | 5 - type      | nominal | equals/-1.0     |\n",
      "| 6 | 6 - root_type | nominal | equals/-1.0     |\n",
      "| 7 | 7 - rating    | numeric | diffnotneg/-1.0 |\n",
      "| 8 | 8 - weather   | nominal | equals/-1.0     |\n",
      "+---+---------------+---------+-----------------+\n",
      "\n",
      "\n",
      "[Memory Usage (MiB)] Memory Total: 245.5; Memory Free: 232.60610961914062; Memory Used: 12.893890380859375;\n",
      "[2] >> Movelet Discovery: [0%] Hiper Movelets Discovery for Class: 390.\n",
      "[2] >> Movelet Discovery: [3%] Class: 390. Trajectory: 11031. Trajectory Size: 15. Number of Candidates: 13770. TAU: 0.9. Bucket Size: 1377. Scored Candidates: 339. Total of Movelets: 8. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [6%] Class: 390. Trajectory: 11032. Trajectory Size: 15. Number of Candidates: 13770. TAU: 0.9. Bucket Size: 2721. Scored Candidates: 278. Total of Movelets: 9. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [9%] Class: 390. Trajectory: 11033. Trajectory Size: 11. Number of Candidates: 9690. TAU: 0.9. Bucket Size: 3662. Scored Candidates: 172. Total of Movelets: 4. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [12%] Class: 390. Trajectory: 11034. Trajectory Size: 20. Number of Candidates: 18870. TAU: 0.9. Bucket Size: 5532. Scored Candidates: 385. Total of Movelets: 13. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [16%] Class: 390. Trajectory: 11036. Trajectory Size: 21. Number of Candidates: 24225. TAU: 0.9. Bucket Size: 7916. Scored Candidates: 426. Total of Movelets: 9. Max Size: 5. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [19%] Class: 390. Trajectory: 11039. Trajectory Size: 10. Number of Candidates: 8670. TAU: 0.9. Bucket Size: 8670. Scored Candidates: 194. Total of Movelets: 7. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [22%] Class: 390. Trajectory: 11040. Trajectory Size: 12. Number of Candidates: 10710. TAU: 0.9. Bucket Size: 9792. Scored Candidates: 230. Total of Movelets: 9. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [25%] Class: 390. Trajectory: 11045. Trajectory Size: 15. Number of Candidates: 13770. TAU: 0.9. Bucket Size: 11146. Scored Candidates: 250. Total of Movelets: 7. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [29%] Class: 390. Trajectory: 11047. Trajectory Size: 13. Number of Candidates: 11730. TAU: 0.9. Bucket Size: 11730. Scored Candidates: 201. Total of Movelets: 5. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [32%] Class: 390. Trajectory: 11049. Trajectory Size: 12. Number of Candidates: 10710. TAU: 0.9. Bucket Size: 10710. Scored Candidates: 219. Total of Movelets: 5. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [35%] Class: 390. Trajectory: 11058. Trajectory Size: 10. Number of Candidates: 8670. TAU: 0.9. Bucket Size: 8670. Scored Candidates: 167. Total of Movelets: 4. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [38%] Class: 390. Trajectory: 11062. Trajectory Size: 20. Number of Candidates: 18870. TAU: 0.9. Bucket Size: 16060. Scored Candidates: 335. Total of Movelets: 7. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [41%] Class: 390. Trajectory: 11063. Trajectory Size: 15. Number of Candidates: 13770. TAU: 0.9. Bucket Size: 13770. Scored Candidates: 242. Total of Movelets: 6. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [45%] Class: 390. Trajectory: 11070. Trajectory Size: 13. Number of Candidates: 11730. TAU: 0.9. Bucket Size: 11730. Scored Candidates: 214. Total of Movelets: 7. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [45%] Class: 390. Trajs. Looked: 14. Trajs. Ignored: 0.\n",
      "[2] >> Movelet Discovery: [45%] Hiper Movelets Discovery for Class: 881.\n",
      "[2] >> Movelet Discovery: [48%] Class: 881. Trajectory: 24428. Trajectory Size: 15. Number of Candidates: 13770. TAU: 0.9000000000000002. Bucket Size: 1377. Scored Candidates: 310. Total of Movelets: 6. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [51%] Class: 881. Trajectory: 24429. Trajectory Size: 21. Number of Candidates: 24225. TAU: 0.9. Bucket Size: 3769. Scored Candidates: 542. Total of Movelets: 6. Max Size: 5. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [54%] Class: 881. Trajectory: 24430. Trajectory Size: 14. Number of Candidates: 12750. TAU: 0.9000000000000002. Bucket Size: 4990. Scored Candidates: 277. Total of Movelets: 5. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [58%] Class: 881. Trajectory: 24431. Trajectory Size: 16. Number of Candidates: 14790. TAU: 0.9. Bucket Size: 6441. Scored Candidates: 459. Total of Movelets: 6. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [61%] Class: 881. Trajectory: 24434. Trajectory Size: 21. Number of Candidates: 24225. TAU: 0.9000000000000005. Bucket Size: 8818. Scored Candidates: 512. Total of Movelets: 5. Max Size: 5. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [64%] Class: 881. Trajectory: 24435. Trajectory Size: 16. Number of Candidates: 14790. TAU: 0.9. Bucket Size: 10245. Scored Candidates: 355. Total of Movelets: 6. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [67%] Class: 881. Trajectory: 24436. Trajectory Size: 12. Number of Candidates: 10710. TAU: 0.9000000000000005. Bucket Size: 10710. Scored Candidates: 267. Total of Movelets: 5. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [70%] Class: 881. Trajectory: 24442. Trajectory Size: 14. Number of Candidates: 12750. TAU: 0.9000000000000005. Bucket Size: 12529. Scored Candidates: 372. Total of Movelets: 5. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [74%] Class: 881. Trajectory: 24446. Trajectory Size: 20. Number of Candidates: 18870. TAU: 0.9000000000000002. Bucket Size: 14379. Scored Candidates: 497. Total of Movelets: 7. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [77%] Class: 881. Trajectory: 24447. Trajectory Size: 21. Number of Candidates: 24225. TAU: 0.9. Bucket Size: 16752. Scored Candidates: 564. Total of Movelets: 7. Max Size: 5. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [80%] Class: 881. Trajectory: 24448. Trajectory Size: 29. Number of Candidates: 34425. TAU: 0.9. Bucket Size: 20138. Scored Candidates: 699. Total of Movelets: 10. Max Size: 5. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [83%] Class: 881. Trajectory: 24450. Trajectory Size: 24. Number of Candidates: 28050. TAU: 0.9. Bucket Size: 22873. Scored Candidates: 611. Total of Movelets: 8. Max Size: 5. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [87%] Class: 881. Trajectory: 24451. Trajectory Size: 17. Number of Candidates: 15810. TAU: 0.9. Bucket Size: 15810. Scored Candidates: 396. Total of Movelets: 6. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [90%] Class: 881. Trajectory: 24452. Trajectory Size: 31. Number of Candidates: 36975. TAU: 0.9. Bucket Size: 28051. Scored Candidates: 837. Total of Movelets: 9. Max Size: 5. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [93%] Class: 881. Trajectory: 24456. Trajectory Size: 20. Number of Candidates: 18870. TAU: 0.9. Bucket Size: 18870. Scored Candidates: 542. Total of Movelets: 7. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [96%] Class: 881. Trajectory: 24457. Trajectory Size: 28. Number of Candidates: 33150. TAU: 0.9000000000000002. Bucket Size: 33115. Scored Candidates: 772. Total of Movelets: 8. Max Size: 5. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [100%] Class: 881. Trajectory: 24458. Trajectory Size: 17. Number of Candidates: 15810. TAU: 0.9. Bucket Size: 15810. Scored Candidates: 457. Total of Movelets: 7. Max Size: 4. Used Features: 8.\n",
      "[2] >> Movelet Discovery: [100%] Class: 881. Trajs. Looked: 17. Trajs. Ignored: 0.\n",
      "\n",
      "[3] >> Processing time: 6309 milliseconds\n",
      "Sun Jun 30 23:05:02 BRT 2024\n"
     ]
    }
   ],
   "source": [
    "# Running the movelet extractor HiPerMovelets:\n",
    "!java -Xmx7G -jar \"./sample/programs/MoveletDiscovery.jar\" \\\n",
    "-curpath \"./sample/data/FoursquareNYC\" \\\n",
    "-respath \"./sample/results/hiper\" \\\n",
    "-descfile \"./sample/data/FoursquareNYC/FoursquareNYC.json\" \\\n",
    "-nt 1 -version hiper -ms -1 -Ms -3 -TC 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Concatenate the feature files (created by label) containing the movelets matrix into one train.csv and one test.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files - train.csv\n",
      "Writing train.csv file\n",
      "Done.\n",
      "Loading files - test.csv\n",
      "Writing test.csv file\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!python3 scripts/helpers/MAT-MergeDatasets.py ./sample/results/hiper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "movelets_train = pd.read_csv('./sample/results/hiper/train.csv')\n",
    "movelets_test = pd.read_csv('./sample/results/hiper/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the movelets for classification ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.1. Movelet Multi-Layer Perceptron (MMLP)\n",
    "\n",
    "*In this case we don´t need `model.train()` to train models for finding the best configuration parameters. Because the model use movelets, those are already the most discriminant patterns in data. Thus, the neural network was already tunned to this type of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-30 23:05:10.871841\n",
      "\n",
      "[MMLP:] Building model\n",
      "[MMLP:] Training hiperparameter model\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'in_top_k/InTopKV2' defined at (most recent call last):\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/dr/fz9zzkjx71s814x_62jv3zwr0000gn/T/ipykernel_20979/2565433262.py\", line 5, in <cell line: 5>\n      model.train()\n    File \"/Users/tarlisportela/workdir/programs/mat-classification-pkg/matclassification/methods/core/HSClassifier.py\", line 71, in train\n      self.fit(X_train, y_train, X_val, y_val) #, config)\n    File \"/Users/tarlisportela/workdir/programs/mat-classification-pkg/matclassification/methods/feature/MoveletMLP.py\", line 104, in fit\n      history = self.model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epoch, batch_size=par_batch_size, verbose=verbose)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1055, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1149, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/base_metric.py\", line 691, in update_state\n      matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/accuracy_metrics.py\", line 494, in top_k_categorical_accuracy\n      return metrics_utils.sparse_top_k_categorical_matches(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 1006, in sparse_top_k_categorical_matches\n      tf.math.in_top_k(\nNode: 'in_top_k/InTopKV2'\nDetected at node 'in_top_k/InTopKV2' defined at (most recent call last):\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/dr/fz9zzkjx71s814x_62jv3zwr0000gn/T/ipykernel_20979/2565433262.py\", line 5, in <cell line: 5>\n      model.train()\n    File \"/Users/tarlisportela/workdir/programs/mat-classification-pkg/matclassification/methods/core/HSClassifier.py\", line 71, in train\n      self.fit(X_train, y_train, X_val, y_val) #, config)\n    File \"/Users/tarlisportela/workdir/programs/mat-classification-pkg/matclassification/methods/feature/MoveletMLP.py\", line 104, in fit\n      history = self.model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epoch, batch_size=par_batch_size, verbose=verbose)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1055, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1149, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/base_metric.py\", line 691, in update_state\n      matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/accuracy_metrics.py\", line 494, in top_k_categorical_accuracy\n      return metrics_utils.sparse_top_k_categorical_matches(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 1006, in sparse_top_k_categorical_matches\n      tf.math.in_top_k(\nNode: 'in_top_k/InTopKV2'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  input must have at least k columns. Had 2, needed 5\n\t [[{{node in_top_k/InTopKV2}}]]\n\t [[clip_by_value_1/_28]]\n  (1) INVALID_ARGUMENT:  input must have at least k columns. Had 2, needed 5\n\t [[{{node in_top_k/InTopKV2}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1856870]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m MMLP()\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mprepare_input(movelets_train, movelets_test)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtest()\n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/HSClassifier.py:71\u001b[0m, in \u001b[0;36mHSClassifier.train\u001b[0;34m(self, dir_validation)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate() \u001b[38;5;66;03m# pass the config dict()\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, config)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     validation_report, y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X_val, y_val)\n\u001b[1;32m     74\u001b[0m     validation_report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclstime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration()\n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/feature/MoveletMLP.py:104\u001b[0m, in \u001b[0;36mMMLP.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    102\u001b[0m     adam \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39madam, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_k_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,f1])\n\u001b[0;32m--> 104\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpar_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(history\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'in_top_k/InTopKV2' defined at (most recent call last):\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/dr/fz9zzkjx71s814x_62jv3zwr0000gn/T/ipykernel_20979/2565433262.py\", line 5, in <cell line: 5>\n      model.train()\n    File \"/Users/tarlisportela/workdir/programs/mat-classification-pkg/matclassification/methods/core/HSClassifier.py\", line 71, in train\n      self.fit(X_train, y_train, X_val, y_val) #, config)\n    File \"/Users/tarlisportela/workdir/programs/mat-classification-pkg/matclassification/methods/feature/MoveletMLP.py\", line 104, in fit\n      history = self.model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epoch, batch_size=par_batch_size, verbose=verbose)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1055, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1149, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/base_metric.py\", line 691, in update_state\n      matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/accuracy_metrics.py\", line 494, in top_k_categorical_accuracy\n      return metrics_utils.sparse_top_k_categorical_matches(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 1006, in sparse_top_k_categorical_matches\n      tf.math.in_top_k(\nNode: 'in_top_k/InTopKV2'\nDetected at node 'in_top_k/InTopKV2' defined at (most recent call last):\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/dr/fz9zzkjx71s814x_62jv3zwr0000gn/T/ipykernel_20979/2565433262.py\", line 5, in <cell line: 5>\n      model.train()\n    File \"/Users/tarlisportela/workdir/programs/mat-classification-pkg/matclassification/methods/core/HSClassifier.py\", line 71, in train\n      self.fit(X_train, y_train, X_val, y_val) #, config)\n    File \"/Users/tarlisportela/workdir/programs/mat-classification-pkg/matclassification/methods/feature/MoveletMLP.py\", line 104, in fit\n      history = self.model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epoch, batch_size=par_batch_size, verbose=verbose)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1055, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1149, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/base_metric.py\", line 691, in update_state\n      matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/metrics/accuracy_metrics.py\", line 494, in top_k_categorical_accuracy\n      return metrics_utils.sparse_top_k_categorical_matches(\n    File \"/Users/tarlisportela/miniforge3/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 1006, in sparse_top_k_categorical_matches\n      tf.math.in_top_k(\nNode: 'in_top_k/InTopKV2'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  input must have at least k columns. Had 2, needed 5\n\t [[{{node in_top_k/InTopKV2}}]]\n\t [[clip_by_value_1/_28]]\n  (1) INVALID_ARGUMENT:  input must have at least k columns. Had 2, needed 5\n\t [[{{node in_top_k/InTopKV2}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1856870]"
     ]
    }
   ],
   "source": [
    "from matclassification.methods import MMLP\n",
    "\n",
    "model = MMLP()\n",
    "model.prepare_input(movelets_train, movelets_test)\n",
    "model.train() \n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is another version, without hiper-parameter search:\n",
    "from matclassification.methods.feature.MoveletMLP import MMLP1\n",
    "\n",
    "model = MMLP1()\n",
    "model.prepare_input(movelets_train, movelets_test)\n",
    "model.train()\n",
    "model.test()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.2. Movelet Random Forrest (MRF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matclassification.methods import MRF\n",
    "\n",
    "model = MRF()\n",
    "model.prepare_input(movelets_train, movelets_test)\n",
    "model.train() \n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.3. Movelet Random Forrest with HiperParameter Search (MRFHP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matclassification.methods import MRFHP\n",
    "\n",
    "model = MRFHP()\n",
    "model.prepare_input(movelets_train, movelets_test)\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.4. Movelet Support Vector Machine (MSCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matclassification.methods import MSVC\n",
    "\n",
    "model = MSVC()\n",
    "model.prepare_input(movelets_train, movelets_test)\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.5. Movelet Decision Tree (MDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matclassification.methods import MDT\n",
    "\n",
    "model = MDT()\n",
    "model.prepare_input(movelets_train, movelets_test)\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = model.plot_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.graph_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.3. Feature-Based Methods\n",
    "\n",
    "Feature-based methods input extracted features from trajectory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.3.1. POI-Sequences (extention of POI-Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matclassification.methods import POIS\n",
    "\n",
    "sequences = [1] # Sequence sizes to use, example, for 1, 2 or 3 points use: [1, 2, 3] \n",
    "features = ['poi'] # Features to build frequency matrix combined with sequence sizes, \n",
    "                   # by default selects the feature with higher variance\n",
    "# method='npoi' # you can pass the extract method\n",
    "\n",
    "model = POIS('npoi', sequences, features)\n",
    "model.prepare_input(train, test)\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix:\n",
    "model.cm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use just the feature extractor, you can check `poifreq` submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matclassification.methods.feature.feature_extraction.pois import pois\n",
    "\n",
    "sequences = [1,2,3]\n",
    "features = ['poi']\n",
    "method='npoi' # default: 'npoi', or, 'poi' and 'wnpoi'\n",
    "\n",
    "x_train, x_test, y_train, y_test, _ = pois(train, test, sequences, features, method, \n",
    "                                           result_dir='./sample/results/pois', save_all=True) # And we save the results\n",
    "\n",
    "display(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matclassification.methods.feature.feature_extraction.pois import pois\n",
    "\n",
    "sequences = [1]\n",
    "features = ['poi']\n",
    "method='npoi' # default: 'npoi', or, 'poi' and 'wnpoi'\n",
    "\n",
    "x_train, x_test, y_train, y_test, _ = pois(train, test, sequences, features, method, \n",
    "                                           result_dir='./sample/results/pois', save_all=True) # And we save the results\n",
    "\n",
    "display(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you can create the classifier (another way of using the classifier classes in the classical way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matclassification.methods.feature.POIS import POIS, prepareData\n",
    "\n",
    "sequences = [1]\n",
    "features = ['poi']\n",
    "\n",
    "# POIS method have a method for data transformation:\n",
    "num_features, num_classes, labels, X, y, one_hot_y = prepareData(x_train, x_test, y_train, y_test)\n",
    "x_train, x_test = X\n",
    "y_train, y_test = y\n",
    "\n",
    "# Create the classifier:\n",
    "model = POIS('npoi', sequences, features)\n",
    "\n",
    "# Model Label Encoder:\n",
    "model.le = one_hot_y\n",
    "\n",
    "# You can add method variables with this:\n",
    "model.add_config(num_features=num_features,\n",
    "                num_classes=num_classes, \n",
    "                labels=labels)\n",
    "\n",
    "# Run the classifier:\n",
    "model.fit(x_train, y_train, x_test, y_test)\n",
    "\n",
    "summary, y_pred = model.predict(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next classifiers use Hiperparameter Optimization (class derrived from `HPSClassifier`), thus we first call `model.train()` to test model configurations to look for the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.4. Similarity-Based Methods (#TODO)\n",
    "\n",
    "Similarity-based methods input a inverse similarity matrix (distance) from trajectory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: Attributes: [0], Weight: 1\n",
      "Feature: Attributes: [1], Weight: 1\n",
      "Feature: Attributes: [2], Weight: 0.25\n",
      "Feature: Attributes: [3, 4, 5], Weight: 0.25\n",
      "Feature: Attributes: [3, 6], Weight: 0.25\n",
      "Feature: Attributes: [7], Weight: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Dataset Descriptor\n",
    "from matmodel.descriptor import readDescriptor\n",
    "dataset_descriptor = readDescriptor('./sample/data/FoursquareNYC/FoursquareNYC.json')\n",
    "\n",
    "# importing all functions refering similarity measure method:\n",
    "from matsimilarity.methods.mat import *\n",
    "\n",
    "# Create the MUITAS object \n",
    "muitas = MUITAS(dataset_descriptor)\n",
    "\n",
    "# Add features to the MUITAS object\n",
    "muitas.add_feature([dataset_descriptor.attributes[0]], 1)\n",
    "muitas.add_feature([dataset_descriptor.attributes[1]], 1)\n",
    "muitas.add_feature([dataset_descriptor.attributes[2]], 0.25)\n",
    "muitas.add_feature([dataset_descriptor.attributes[3], dataset_descriptor.attributes[4], dataset_descriptor.attributes[5]], 0.25)\n",
    "muitas.add_feature([dataset_descriptor.attributes[3], dataset_descriptor.attributes[6]], 0.25)\n",
    "muitas.add_feature([dataset_descriptor.attributes[7]], 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Descriptor\n",
    "from matmodel.descriptor import readDescriptor\n",
    "dataset_descriptor = readDescriptor('./sample/data/FoursquareNYC/FoursquareNYC.json')\n",
    "\n",
    "# importing all functions refering similarity measure method:\n",
    "from matsimilarity.methods.mat import MSM\n",
    "\n",
    "# Create the MUITAS object \n",
    "msm = MSM(dataset_descriptor, weights=[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-23 21:03:42.967062\n",
      "\n",
      "[TSVC:] Building model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12923072f0e44c42abb3223fda852b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting Trajectories:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668a56f710c94712a0827d1a5f72719d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting Trajectories:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TSVC:] Metric provided - MUITAS.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3b2bfcbbdc429dace20215f9f39735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarity matrix:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (112,112,108,108,8) (8,108,108,112,112) (112,112,108,108,8) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatclassification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSVC\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m TSVC()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmuitas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtest()\n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/SimilarityClassifier.py:108\u001b[0m, in \u001b[0;36mSimilarityClassifier.prepare_input\u001b[0;34m(self, train, test, tid_col, class_col, space_geohash, geo_precision, validate, metric, dataset_descriptor, inverse)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_input\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m                   train, test,\n\u001b[1;32m     98\u001b[0m                   tid_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtid\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Load Data - Tarlis:\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_geohash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeo_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_descriptor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;241m0\u001b[39m] \n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/SimilarityClassifier.py:88\u001b[0m, in \u001b[0;36mSimilarityClassifier.xy\u001b[0;34m(self, train, test, tid_col, class_col, space_geohash, geo_precision, validate, metric, dataset_descriptor, inverse)\u001b[0m\n\u001b[1;32m     86\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inverse: \u001b[38;5;66;03m# Use inverse of similarity (distance metric):\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43msimilarity_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_jobs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m )\n\u001b[1;32m     89\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m similarity_matrix(test, train,  measure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m]) )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/SimilarityClassifier.py:180\u001b[0m, in \u001b[0;36msimilarity_matrix\u001b[0;34m(A, B, measure, n_jobs)\u001b[0m\n\u001b[1;32m    177\u001b[0m similarity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack(similarity)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m upper:\n\u001b[0;32m--> 180\u001b[0m     similarity \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m similarity\u001b[38;5;241m.\u001b[39mtranspose() \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;28mlen\u001b[39m(A))\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m similarity\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (112,112,108,108,8) (8,108,108,112,112) (112,112,108,108,8) "
     ]
    }
   ],
   "source": [
    "from matclassification.methods import TSVC\n",
    "\n",
    "model = TSVC()\n",
    "model.prepare_input(train, test, metric=muitas)\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-23 21:02:18.924979\n",
      "\n",
      "[TDT:] Building model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c8ac9b920e49b0ac42220e6c61ec6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting Trajectories:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495d9bf6728345c180da89932cddeb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting Trajectories:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TDT:] Default metric set to MUITAS.\n",
      "Feature: Attributes: [0], Weight: 1\n",
      "Feature: Attributes: [1], Weight: 1\n",
      "Feature: Attributes: [2], Weight: 1\n",
      "Feature: Attributes: [3], Weight: 1\n",
      "Feature: Attributes: [4], Weight: 1\n",
      "Feature: Attributes: [5], Weight: 1\n",
      "Feature: Attributes: [6], Weight: 1\n",
      "Feature: Attributes: [7], Weight: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9ab9c818f74e8cbe3d6dc787b4d56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarity matrix:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (112,112,108,108,8) (8,108,108,112,112) (112,112,108,108,8) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatclassification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TDT\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m TDT()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, metric=msm)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtest()\n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/SimilarityClassifier.py:108\u001b[0m, in \u001b[0;36mSimilarityClassifier.prepare_input\u001b[0;34m(self, train, test, tid_col, class_col, space_geohash, geo_precision, validate, metric, dataset_descriptor, inverse)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_input\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m                   train, test,\n\u001b[1;32m     98\u001b[0m                   tid_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtid\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Load Data - Tarlis:\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_geohash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeo_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_descriptor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;241m0\u001b[39m] \n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/SimilarityClassifier.py:88\u001b[0m, in \u001b[0;36mSimilarityClassifier.xy\u001b[0;34m(self, train, test, tid_col, class_col, space_geohash, geo_precision, validate, metric, dataset_descriptor, inverse)\u001b[0m\n\u001b[1;32m     86\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inverse: \u001b[38;5;66;03m# Use inverse of similarity (distance metric):\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43msimilarity_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_jobs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m )\n\u001b[1;32m     89\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m similarity_matrix(test, train,  measure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m]) )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/SimilarityClassifier.py:180\u001b[0m, in \u001b[0;36msimilarity_matrix\u001b[0;34m(A, B, measure, n_jobs)\u001b[0m\n\u001b[1;32m    177\u001b[0m similarity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack(similarity)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m upper:\n\u001b[0;32m--> 180\u001b[0m     similarity \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m similarity\u001b[38;5;241m.\u001b[39mtranspose() \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;28mlen\u001b[39m(A))\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m similarity\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (112,112,108,108,8) (8,108,108,112,112) (112,112,108,108,8) "
     ]
    }
   ],
   "source": [
    "from matclassification.methods import TDT\n",
    "\n",
    "model = TDT()\n",
    "model.prepare_input(train, test)#, metric=msm)\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.4.1. Trajectory K-Nearest Neighbors vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-06 02:47:01.469191\n",
      "\n",
      "[TKNN:] Building model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33aeb66bfc9744c98e2b310840b4f858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting Trajectories:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e52280e0ca84485b02991eeaec97bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting Trajectories:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TKNN:] Metric provided - TTP.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04ddb4172fd443ea146bc58c4ac8e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarity matrix:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (108,) into shape (108,108,8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatclassification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TKNN\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m TKNN(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Default metric = MUITAS (default configuration)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtest()\n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/SimilarityClassifier.py:108\u001b[0m, in \u001b[0;36mSimilarityClassifier.prepare_input\u001b[0;34m(self, train, test, tid_col, class_col, space_geohash, geo_precision, validate, metric, dataset_descriptor, inverse)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_input\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m                   train, test,\n\u001b[1;32m     98\u001b[0m                   tid_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtid\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Load Data - Tarlis:\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_geohash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeo_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_descriptor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;241m0\u001b[39m] \n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/SimilarityClassifier.py:88\u001b[0m, in \u001b[0;36mSimilarityClassifier.xy\u001b[0;34m(self, train, test, tid_col, class_col, space_geohash, geo_precision, validate, metric, dataset_descriptor, inverse)\u001b[0m\n\u001b[1;32m     86\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inverse: \u001b[38;5;66;03m# Use inverse of similarity (distance metric):\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43msimilarity_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_jobs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m )\n\u001b[1;32m     89\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m similarity_matrix(test, train,  measure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m]) )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/SimilarityClassifier.py:175\u001b[0m, in \u001b[0;36msimilarity_matrix\u001b[0;34m(A, B, measure, n_jobs)\u001b[0m\n\u001b[1;32m    172\u001b[0m B \u001b[38;5;241m=\u001b[39m A \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m B \u001b[38;5;28;01melse\u001b[39;00m B\n\u001b[1;32m    173\u001b[0m func \u001b[38;5;241m=\u001b[39m delayed(compute_slice)\n\u001b[0;32m--> 175\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m similarity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack(similarity)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m upper:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/workdir/programs/mat-classification-pkg/matclassification/methods/core/SimilarityClassifier.py:168\u001b[0m, in \u001b[0;36msimilarity_matrix.<locals>.compute_slice\u001b[0;34m(A, B, s)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(s\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(A)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputing similarity matrix\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(B), i \u001b[38;5;241m-\u001b[39m s\u001b[38;5;241m.\u001b[39mstart)):\n\u001b[0;32m--> 168\u001b[0m         matrix[i][j] \u001b[38;5;241m=\u001b[39m measure\u001b[38;5;241m.\u001b[39msimilarity(A[i], B[j])\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matrix\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (108,) into shape (108,108,8)"
     ]
    }
   ],
   "source": [
    "from matclassification.methods import TKNN\n",
    "model = TKNN(k=3)\n",
    "\n",
    "X, y = model.prepare_input(train, test, metric=m, inverse=True) # Default metric = MUITAS (default configuration)\n",
    "model.train()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>clstime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.1</td>\n",
       "      <td>23.124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0  0.142857       0.571429                0.1         0.069565      0.453333   \n",
       "\n",
       "   f1_macro  clstime  \n",
       "0       0.1   23.124  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracyTopK5</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  accuracyTopK5  balanced_accuracy  precision_macro  recall_macro  \\\n",
       "0  0.142857       0.571429                0.1         0.069565      0.453333   \n",
       "\n",
       "   f1_macro  p0        p1  \n",
       "0       0.1   3  distance  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2c443aeb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Confusion Matrix:\n",
    "model.cm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 84,  84,  84,  84,  84,  84,  84,  84, 164, 164, 164, 164, 181,\n",
       "        181, 181, 181, 390, 390, 390, 390, 390, 390, 390, 553, 553, 553,\n",
       "        553, 646, 646, 646, 646, 646, 662, 662, 662, 662, 662, 702, 702,\n",
       "        702, 702, 702, 702, 702, 702, 768, 768, 768, 768, 768, 901, 901,\n",
       "        901, 901, 901, 901]),\n",
       " array([ 84,  84,  84,  84,  84,  84,  84,  84,  84,  84,  84,  84,  84,\n",
       "         84,  84, 164, 164, 164, 164, 164, 164, 164, 181, 181, 181, 181,\n",
       "        181, 181, 181, 181, 390, 390, 390, 390, 390, 390, 390, 390, 390,\n",
       "        390, 390, 390, 390, 390, 553, 553, 553, 553, 553, 553, 553, 553,\n",
       "        553, 646, 646, 646]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.y_test_true, model.y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matsimilarity.core import SimilarityMeasure\n",
    "from matmodel.base import *\n",
    "from matmodel.descriptor import *\n",
    "#np.zeros((3, 5, 3, 6))\n",
    "\n",
    "class TTP(SimilarityMeasure):\n",
    "    def __init__(self, dataset_descriptor: DataDescriptor = None, weights = []):\n",
    "        super().__init__(dataset_descriptor)\n",
    "        \n",
    "        if isinstance(weights, np.ndarray):\n",
    "            weights_sum = weights.sum()\n",
    "        else:\n",
    "            weights_sum = sum(weights)\n",
    "        weights = np.array(weights)\n",
    "        self.weights = weights / weights_sum\n",
    "    \n",
    "    def similarity(self, t1: MultipleAspectSequence, t2: MultipleAspectSequence) -> float:\n",
    "        return np.fromiter(map(lambda p1: np.fromiter(map(lambda p2: self._score(p1, p2), t2.points), dtype=np.object_), t1.points), dtype=np.object_)\n",
    "\n",
    "    def _score(self, p1: Point = None, p2: Point = None) -> float:\n",
    "        matches = np.zeros(len(self.attributes))\n",
    "#        for idx, _ in enumerate(self.attributes):\n",
    "        def get_scores(idx):\n",
    "            a1 = p1.aspects[idx]\n",
    "            a2 = p2.aspects[idx]\n",
    "            attr = self._data_descriptor.attributes[idx]\n",
    "#            threshold = self.thresholds.get(idx, 0)\n",
    "            distance = attr.comparator.distance(a1, a2)\n",
    "            return distance\n",
    "        return np.fromiter(map(lambda x: get_scores(x[0]), enumerate(self.attributes)), dtype=np.float) # * self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9577ed56d9114c3793b387372b6235c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting Trajectories:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0e71fc46b14d89b2b23e99db4138ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting Trajectories:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matmodel.util.parsers import df2trajectory\n",
    "\n",
    "X_train, dd = df2trajectory(train.copy())\n",
    "X_test, dd = df2trajectory(test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = TTP(dd)\n",
    "m.similarity(T[0], T[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.utils import gen_even_slices\n",
    "from tqdm.auto import tqdm\n",
    "def similarity_matrix(A, B=None, measure=None, n_jobs=1):\n",
    "    def compute_slice(A, B, s):\n",
    "        matrix = np.zeros(shape=(len(A), len(B)))\n",
    "\n",
    "        for i in tqdm(range(s.start + 1, len(A)), desc='Computing similarity matrix'):\n",
    "            for j in range(0, min(len(B), i - s.start)):\n",
    "                matrix[i][j] = measure.similarity(A[i], B[j])\n",
    "        return matrix\n",
    "\n",
    "    upper = B is not None\n",
    "    B = A if not B else B\n",
    "    func = delayed(compute_slice)\n",
    "\n",
    "    similarity = Parallel(n_jobs=n_jobs, verbose=0)(\n",
    "        func(A, B[s], s) for s in gen_even_slices(len(B), n_jobs))\n",
    "    similarity = np.hstack(similarity)\n",
    "\n",
    "    if not upper:\n",
    "        similarity += similarity.transpose() + np.identity(len(A))\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,3) (2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      2\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3) (2,2) "
     ]
    }
   ],
   "source": [
    "a = np.zeros((3, 3))\n",
    "b = np.identity(2)\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addbc65284384f4999adbd791e14dedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarity matrix:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msimilarity_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36msimilarity_matrix\u001b[0;34m(A, B, measure, n_jobs)\u001b[0m\n\u001b[1;32m     14\u001b[0m B \u001b[38;5;241m=\u001b[39m A \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m B \u001b[38;5;28;01melse\u001b[39;00m B\n\u001b[1;32m     15\u001b[0m func \u001b[38;5;241m=\u001b[39m delayed(compute_slice)\n\u001b[0;32m---> 17\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m similarity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack(similarity)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m upper:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36msimilarity_matrix.<locals>.compute_slice\u001b[0;34m(A, B, s)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(s\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(A)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputing similarity matrix\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(B), i \u001b[38;5;241m-\u001b[39m s\u001b[38;5;241m.\u001b[39mstart)):\n\u001b[0;32m---> 10\u001b[0m         matrix[i][j] \u001b[38;5;241m=\u001b[39m measure\u001b[38;5;241m.\u001b[39msimilarity(A[i], B[j])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matrix\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "similarity_matrix(X_train, measure=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## 3. Scripts\n",
    "\n",
    "The scripts provided with this package for using some of the library functions in command line environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Classifications Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only the MARC classifier:\n",
    "!python3 scripts/cls/MARC.py 'sample/data/FoursquareNYC/train.csv' 'sample/data/FoursquareNYC/test.csv' 'sample/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 scripts/cls/MAT-TC.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Methods for trajectory input (includes MARC):\n",
    "!python3 scripts/cls/MAT-TC.py 'sample/data/FoursquareNYC' 'sample/results' -c 'TRF,TXGB,Tulvae,Bituler,MARC,DeepeST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Methods for movelets input:\n",
    "!python3 scripts/cls/MAT-MC.py 'sample/results/hiper' -c 'MDT,MMLP,MRF,MSVC,MRFHP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For POI-S classification of sequence sizes concatenated (it does feature extraction):\n",
    "!python3 scripts/cls/POIS-TC.py 'sample/data/FoursquareNYC' 'sample/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For POI-S for feature extraction:\n",
    "!python3 scripts/features/POIS.py 'sample/data/FoursquareNYC' 'sample/results/NPOI_poi_1_2_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, for POI-S feature extraction AND classification (it classify each sequence size alone and all sizes concatenated):\n",
    "!python3 scripts/features/POIS.py 'sample/data/FoursquareNYC' 'sample/results/NPOI_poi_1_2_3' --classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3. Helpers for Experimental Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Concatenate the feature files (created by label) containing the movelets matrix into one train.csv and one test.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 scripts/helpers/MAT-MergeDatasets.py /Users/tarlisportela/Downloads/sample/results/HpL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 scripts/helpers/MAT-MergeDatasets.py ./sample/results/hiper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# That's all, thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# By Tarlis Portela (2024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
